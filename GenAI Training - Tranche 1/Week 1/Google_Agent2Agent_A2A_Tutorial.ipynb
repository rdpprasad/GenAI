{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZwUfX1ZUTgk"
      },
      "outputs": [],
      "source": [
        "# Implementation of agent2agent protocol using RAG\n",
        "\n",
        "# Install necessary libraries (if you were using actual LLM APIs and vector databases)\n",
        "# !pip install transformers  # For using transformer models\n",
        "# !pip install faiss-cpu   # For a simple vector database\n",
        "\n",
        "# 1. Simulate a Knowledge Base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Louvre Museum is a famous art museum in Paris.\",\n",
        "    \"Mount Everest is the highest mountain in the world.\",\n",
        "    \"The Amazon River is the largest river by discharge volume.\",\n",
        "    \"Artificial intelligence (AI) is the simulation of human intelligence processes by machines.\",\n",
        "    \"Retrieval Augmented Generation (RAG) is a technique that combines information retrieval with text generation.\"\n",
        "]\n",
        "\n",
        "# 2. Simulate a Retriever (simple keyword search)\n",
        "def simple_retriever(query, kb):\n",
        "    relevant_docs = [doc for doc in kb if any(word.lower() in doc.lower() for word in query.split())]\n",
        "    return relevant_docs\n",
        "\n",
        "# 3. Simulate an LLM (simple text generation based on context)\n",
        "def simple_llm_response(query, context):\n",
        "    if context:\n",
        "        context_str = \"Context: \" + \" \".join(context)\n",
        "    else:\n",
        "        context_str = \"No relevant context found.\"\n",
        "\n",
        "    # Simple rule-based response based on query and context\n",
        "    if \"capital of France\" in query.lower() and \"Paris\" in context_str:\n",
        "        return f\"Based on the information, the capital of France is Paris.\"\n",
        "    elif \"highest mountain\" in query.lower() and \"Mount Everest\" in context_str:\n",
        "        return f\"Based on the information, the highest mountain in the world is Mount Everest.\"\n",
        "    elif \"RAG\" in query.lower() and \"Retrieval Augmented Generation\" in context_str:\n",
        "         return f\"RAG stands for Retrieval Augmented Generation, which is a technique combining information retrieval and text generation.\"\n",
        "    elif context:\n",
        "         return f\"Based on the context: {context_str}. Regarding your query: {query}\"\n",
        "    else:\n",
        "        return f\"I'm not sure how to answer that based on the available information. Query: {query}\"\n",
        "\n",
        "\n",
        "# 4. Implement Agents\n",
        "class Agent:\n",
        "    def __init__(self, name, knowledge_base):\n",
        "        self.name = name\n",
        "        self.knowledge_base = knowledge_base\n",
        "\n",
        "    def process_query(self, query):\n",
        "        print(f\"{self.name} received query: '{query}'\")\n",
        "        # Agent performs RAG\n",
        "        retrieved_docs = simple_retriever(query, self.knowledge_base)\n",
        "        print(f\"{self.name} retrieved documents: {retrieved_docs}\")\n",
        "        response = simple_llm_response(query, retrieved_docs)\n",
        "        return response\n",
        "\n",
        "class AgentA(Agent):\n",
        "    def __init__(self, knowledge_base):\n",
        "        super().__init__(\"Agent A\", knowledge_base)\n",
        "\n",
        "    def initial_processing(self, query):\n",
        "        return self.process_query(query)\n",
        "\n",
        "    def process_agent_b_response(self, query, agent_b_response):\n",
        "        print(f\"{self.name} received response from Agent B: '{agent_b_response}'\")\n",
        "        # Agent A might use Agent B's response to refine its understanding or ask another question\n",
        "        # For this simple example, Agent A will just acknowledge Agent B's response\n",
        "        print(f\"{self.name} acknowledges Agent B's response.\")\n",
        "        return f\"Agent A processed response from Agent B.\"\n",
        "\n",
        "\n",
        "class AgentB(Agent):\n",
        "     def __init__(self, knowledge_base):\n",
        "        super().__init__(\"Agent B\", knowledge_base)\n",
        "\n",
        "     def process_input_from_agent_a(self, input_from_a):\n",
        "         print(f\"{self.name} received input from Agent A: '{input_from_a}'\")\n",
        "         # Agent B processes the input, potentially performs its own RAG search\n",
        "         return self.process_query(input_from_a)\n",
        "\n",
        "\n",
        "# 5. Orchestrate the Interaction\n",
        "\n",
        "# Create the agents\n",
        "agent_a = AgentA(knowledge_base)\n",
        "agent_b = AgentB(knowledge_base)\n",
        "\n",
        "# Initial query to Agent A\n",
        "initial_query = \"What is the capital of France?\"\n",
        "agent_a_initial_response = agent_a.initial_processing(initial_query)\n",
        "print(f\"Agent A's initial response: {agent_a_initial_response}\\n\")\n",
        "\n",
        "# Agent A sends its response/context to Agent B (in a real scenario, this would be a more complex interaction)\n",
        "agent_b_input = f\"Agent A is asking about: {initial_query}. Agent A found this info: {agent_a_initial_response}\"\n",
        "agent_b_response = agent_b.process_input_from_agent_a(agent_b_input)\n",
        "print(f\"Agent B's response: {agent_b_response}\\n\")\n",
        "\n",
        "# Agent A processes Agent B's response\n",
        "agent_a.process_agent_b_response(initial_query, agent_b_response)\n",
        "\n",
        "print(\"\\n--- Another Interaction ---\")\n",
        "\n",
        "# Another query\n",
        "another_query = \"Tell me about RAG.\"\n",
        "agent_a_initial_response_2 = agent_a.initial_processing(another_query)\n",
        "print(f\"Agent A's initial response: {agent_a_initial_response_2}\\n\")\n",
        "\n",
        "agent_b_input_2 = f\"Agent A is asking about: {another_query}. Agent A found this info: {agent_a_initial_response_2}\"\n",
        "agent_b_response_2 = agent_b.process_input_from_agent_a(agent_b_input_2)\n",
        "print(f\"Agent B's response: {agent_b_response_2}\\n\")\n",
        "\n",
        "agent_a.process_agent_b_response(another_query, agent_b_response_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example using SDLC\n",
        "\n",
        "# Simulate another scenario using the defined agents and framework\n",
        "print(\"\\n--- SDLC Example: Handling a more complex query ---\")\n",
        "\n",
        "# This example simulates a query that might require combining information or further processing\n",
        "\n",
        "# Define a new query that might require more than a simple lookup\n",
        "complex_query = \"Where is the Eiffel Tower and what is the highest mountain?\"\n",
        "\n",
        "# Agent A's initial processing\n",
        "agent_a_complex_response = agent_a.initial_processing(complex_query)\n",
        "print(f\"Agent A's initial response: {agent_a_complex_response}\\n\")\n",
        "\n",
        "# Agent A sends its response/context to Agent B\n",
        "agent_b_complex_input = f\"Agent A is asking about: {complex_query}. Agent A found this info: {agent_a_complex_response}\"\n",
        "agent_b_complex_response = agent_b.process_input_from_agent_a(agent_b_complex_input)\n",
        "print(f\"Agent B's response: {agent_b_complex_response}\\n\")\n",
        "\n",
        "# Agent A processes Agent B's response\n",
        "agent_a.process_agent_b_response(complex_query, agent_b_complex_response)\n",
        "\n",
        "# In a more sophisticated SDLC setup, Agent A might further process the combined\n",
        "# information from itself and Agent B to provide a more comprehensive answer to the complex query.\n",
        "# This simple example just demonstrates the information flow and interaction.\n",
        "\n",
        "print(\"\\n--- SDLC Example: Query with no direct answer in KB ---\")\n",
        "\n",
        "# Query that doesn't have a direct answer in the knowledge base\n",
        "unknown_query = \"What is the population of Tokyo?\"\n",
        "\n",
        "# Agent A's initial processing\n",
        "agent_a_unknown_response = agent_a.initial_processing(unknown_query)\n",
        "print(f\"Agent A's initial response: {agent_a_unknown_response}\\n\")\n",
        "\n",
        "# Agent A sends its response/context to Agent B\n",
        "agent_b_unknown_input = f\"Agent A is asking about: {unknown_query}. Agent A found this info: {agent_a_unknown_response}\"\n",
        "agent_b_unknown_response = agent_b.process_input_from_agent_a(agent_b_unknown_input)\n",
        "print(f\"Agent B's response: {agent_b_unknown_response}\\n\")\n",
        "\n",
        "# Agent A processes Agent B's response\n",
        "agent_a.process_agent_b_response(unknown_query, agent_b_unknown_response)\n",
        "\n",
        "# This illustrates how the agents handle queries outside their known domain,\n",
        "# typically resulting in a response indicating lack of relevant information."
      ],
      "metadata": {
        "id": "amg0j29WUnmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# 6. Introduce a Supervisor/Orchestrator for more complex flows\n",
        "\n",
        "class Supervisor:\n",
        "    def __init__(self, agent_a, agent_b):\n",
        "        self.agent_a = agent_a\n",
        "        self.agent_b = agent_b\n",
        "\n",
        "    def handle_interaction(self, initial_query):\n",
        "        print(f\"\\nSupervisor initiating interaction for query: '{initial_query}'\")\n",
        "\n",
        "        # Supervisor directs the initial query to Agent A\n",
        "        agent_a_response_step1 = self.agent_a.initial_processing(initial_query)\n",
        "        print(f\"Supervisor received response from Agent A (Step 1): {agent_a_response_step1}\")\n",
        "\n",
        "        # Supervisor decides the next step - maybe Agent B can add value based on Agent A's response\n",
        "        # This is where more complex logic would reside in a real system (e.g., analyzing Agent A's confidence, identifying missing info)\n",
        "        # For this simple example, the Supervisor always passes Agent A's response to Agent B\n",
        "        agent_b_input = f\"Agent A processed the query '{initial_query}' and provided: {agent_a_response_step1}. Can you add anything?\"\n",
        "        agent_b_response_step2 = self.agent_b.process_input_from_agent_a(agent_b_input)\n",
        "        print(f\"Supervisor received response from Agent B (Step 2): {agent_b_response_step2}\")\n",
        "\n",
        "        # Supervisor receives Agent B's response and might decide to send it back to Agent A for final processing\n",
        "        # Or, the Supervisor might synthesize the final answer itself based on responses from multiple agents\n",
        "        # In this example, the Supervisor passes Agent B's response back to Agent A\n",
        "        final_response = self.agent_a.process_agent_b_response(initial_query, agent_b_response_step2)\n",
        "        print(f\"Supervisor received final processing result from Agent A (Step 3): {final_response}\")\n",
        "\n",
        "        print(f\"Supervisor concludes interaction for query: '{initial_query}'\")\n",
        "        # In a real application, the Supervisor would construct the final user-facing output here.\n",
        "        # For this example, we'll just return a simple acknowledgment.\n",
        "        return f\"Interaction completed by Supervisor for query: {initial_query}\"\n",
        "\n",
        "# Create the Supervisor\n",
        "supervisor = Supervisor(agent_a, agent_b)\n",
        "\n",
        "# Use the Supervisor to handle interactions\n",
        "supervisor.handle_interaction(\"What is the capital of France?\")\n",
        "\n",
        "supervisor.handle_interaction(\"Tell me about AI and RAG.\")\n",
        "\n",
        "supervisor.handle_interaction(\"What is the tallest building in the world?\") # Query outside KB to see interaction flow"
      ],
      "metadata": {
        "id": "kglUsvhlUxjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Simulate a different knowledge base for Agent B\n",
        "knowledge_base_b = [\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"Jupyter notebooks are widely used for data science.\",\n",
        "    \"Google Colaboratory is a cloud-based Jupyter notebook environment.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "# Update Agent B to use its own knowledge base\n",
        "class AgentB(Agent):\n",
        "    def __init__(self, knowledge_base):\n",
        "        super().__init__(\"Agent B\", knowledge_base) # Use the passed knowledge base\n",
        "\n",
        "    def process_input_from_agent_a(self, input_from_a):\n",
        "        print(f\"{self.name} received input from Agent A: '{input_from_a}'\")\n",
        "        # Agent B processes the input, potentially performs its own RAG search using its KB\n",
        "        return self.process_query(input_from_a)\n",
        "\n",
        "\n",
        "# Recreate the agents with potentially different knowledge bases\n",
        "agent_a = AgentA(knowledge_base) # Agent A uses the original KB\n",
        "agent_b = AgentB(knowledge_base_b) # Agent B uses its specific KB\n",
        "\n",
        "# Recreate the Supervisor with the updated agents\n",
        "supervisor = Supervisor(agent_a, agent_b)\n",
        "\n",
        "# Example 1: Query that might be in Agent A's KB but not Agent B's\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query in A) ---\")\n",
        "supervisor.handle_interaction(\"What is the highest mountain?\")\n",
        "\n",
        "# Example 2: Query that might be in Agent B's KB but not Agent A's\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query in B) ---\")\n",
        "supervisor.handle_interaction(\"Tell me about Python.\")\n",
        "\n",
        "# Example 3: Query that might require combining info (though simple RAG won't truly combine)\n",
        "# This demonstrates the flow when both agents might have relevant pieces or one doesn't.\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query potentially in both/neither) ---\")\n",
        "supervisor.handle_interaction(\"What is RAG and are Jupyter notebooks related to AI?\")\n",
        "\n",
        "# Example 4: Query outside both KBs\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query outside both) ---\")\n",
        "supervisor.handle_interaction(\"What is the deepest ocean trench?\")"
      ],
      "metadata": {
        "id": "ZetcMQTwU7WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Including Model context protocol MCP in the above SDLC example\n",
        "\n",
        "# Install necessary libraries (if you were using actual LLM APIs and vector databases)\n",
        "# !pip install transformers  # For using transformer models\n",
        "# !pip install faiss-cpu   # For a simple vector database\n",
        "\n",
        "# 1. Simulate a Knowledge Base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Eiffel Tower is located in Paris.\",\n",
        "    \"The Louvre Museum is a famous art museum in Paris.\",\n",
        "    \"Mount Everest is the highest mountain in the world.\",\n",
        "    \"The Amazon River is the largest river by discharge volume.\",\n",
        "    \"Artificial intelligence (AI) is the simulation of human intelligence processes by machines.\",\n",
        "    \"Retrieval Augmented Generation (RAG) is a technique that combines information retrieval with text generation.\"\n",
        "]\n",
        "\n",
        "# 2. Simulate a Retriever (simple keyword search)\n",
        "def simple_retriever(query, kb):\n",
        "    relevant_docs = [doc for doc in kb if any(word.lower() in doc.lower() for word in query.split())]\n",
        "    return relevant_docs\n",
        "\n",
        "# 3. Simulate an LLM (simple text generation based on context)\n",
        "# This function now incorporates the concept of a \"Model Context\"\n",
        "def simple_llm_response(query, context, model_context=None):\n",
        "    if context:\n",
        "        context_str = \"Context: \" + \" \".join(context)\n",
        "    else:\n",
        "        context_str = \"No relevant context found.\"\n",
        "\n",
        "    # Incorporate Model Context (MCP)\n",
        "    mcp_info = \"\"\n",
        "    if model_context:\n",
        "        mcp_info = f\" (Processed with Model: {model_context.model_name}, Confidence: {model_context.confidence:.2f})\"\n",
        "\n",
        "    # Simple rule-based response based on query, context, and potentially model_context\n",
        "    if \"capital of France\" in query.lower() and \"Paris\" in context_str:\n",
        "        return f\"Based on the information, the capital of France is Paris.{mcp_info}\"\n",
        "    elif \"highest mountain\" in query.lower() and \"Mount Everest\" in context_str:\n",
        "        return f\"Based on the information, the highest mountain in the world is Mount Everest.{mcp_info}\"\n",
        "    elif \"RAG\" in query.lower() and \"Retrieval Augmented Generation\" in context_str:\n",
        "         return f\"RAG stands for Retrieval Augmented Generation, which is a technique combining information retrieval and text generation.{mcp_info}\"\n",
        "    elif context:\n",
        "         return f\"Based on the context: {context_str}. Regarding your query: {query}{mcp_info}\"\n",
        "    else:\n",
        "        return f\"I'm not sure how to answer that based on the available information. Query: {query}{mcp_info}\"\n",
        "\n",
        "# Define a simple Model Context Protocol (MCP) representation\n",
        "class ModelContext:\n",
        "    def __init__(self, model_name, confidence=1.0, processing_steps=None):\n",
        "        self.model_name = model_name  # Identifier of the model used\n",
        "        self.confidence = confidence  # Confidence score (simple placeholder)\n",
        "        self.processing_steps = processing_steps or [] # List of steps/transforms applied\n",
        "\n",
        "    def add_processing_step(self, step):\n",
        "        self.processing_steps.append(step)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"confidence\": self.confidence,\n",
        "            \"processing_steps\": self.processing_steps\n",
        "        }\n",
        "\n",
        "# 4. Implement Agents (updated to handle Model Context)\n",
        "class Agent:\n",
        "    def __init__(self, name, knowledge_base, model_name):\n",
        "        self.name = name\n",
        "        self.knowledge_base = knowledge_base\n",
        "        self.model_name = model_name # Model name for this agent\n",
        "\n",
        "    def process_query(self, query, incoming_model_context=None):\n",
        "        print(f\"{self.name} received query: '{query}'\")\n",
        "\n",
        "        # Create/Update Model Context\n",
        "        current_model_context = incoming_model_context or ModelContext(self.model_name)\n",
        "        current_model_context.add_processing_step(f\"{self.name}_processing_query\") # Log processing step\n",
        "\n",
        "        # Agent performs RAG\n",
        "        retrieved_docs = simple_retriever(query, self.knowledge_base)\n",
        "        print(f\"{self.name} retrieved documents: {retrieved_docs}\")\n",
        "        current_model_context.add_processing_step(f\"{self.name}_retrieval\") # Log retrieval step\n",
        "\n",
        "        # Simulate confidence based on retrieval success (very basic)\n",
        "        current_model_context.confidence = 1.0 if retrieved_docs else 0.5\n",
        "\n",
        "        # Generate response using LLM, passing the model context\n",
        "        response = simple_llm_response(query, retrieved_docs, current_model_context)\n",
        "        current_model_context.add_processing_step(f\"{self.name}_generation\") # Log generation step\n",
        "\n",
        "        # Return the response and the updated model context\n",
        "        return response, current_model_context\n",
        "\n",
        "\n",
        "class AgentA(Agent):\n",
        "    def __init__(self, knowledge_base, model_name=\"ModelA\"):\n",
        "        super().__init__(\"Agent A\", knowledge_base, model_name)\n",
        "\n",
        "    def initial_processing(self, query):\n",
        "        # Start with a new Model Context for the initial query\n",
        "        return self.process_query(query, ModelContext(self.model_name))\n",
        "\n",
        "    def process_agent_b_response(self, query, agent_b_response, agent_b_model_context):\n",
        "        print(f\"{self.name} received response from Agent B: '{agent_b_response}'\")\n",
        "        print(f\"{self.name} received Model Context from Agent B: {agent_b_model_context.to_dict()}\")\n",
        "\n",
        "        # Agent A processes Agent B's response, potentially using its context\n",
        "        # For this simple example, Agent A will just acknowledge and potentially update context\n",
        "        current_model_context = ModelContext(self.model_name) # Start fresh or merge contexts\n",
        "        current_model_context.add_processing_step(f\"{self.name}_processing_agent_b_response\")\n",
        "        # In a real scenario, Agent A might analyze agent_b_model_context,\n",
        "        # potentially adjusting its confidence or adding steps based on B's workflow.\n",
        "        # For now, just logging the receipt.\n",
        "        print(f\"{self.name} acknowledges Agent B's response and context.\")\n",
        "        # Return a simple acknowledgment and its own context\n",
        "        return f\"Agent A processed response from Agent B.\", current_model_context\n",
        "\n",
        "\n",
        "class AgentB(Agent):\n",
        "     def __init__(self, knowledge_base, model_name=\"ModelB\"):\n",
        "        super().__init__(\"Agent B\", knowledge_base, model_name)\n",
        "\n",
        "     def process_input_from_agent_a(self, input_from_a, agent_a_model_context):\n",
        "         print(f\"{self.name} received input from Agent A: '{input_from_a}'\")\n",
        "         print(f\"{self.name} received Model Context from Agent A: {agent_a_model_context.to_dict()}\")\n",
        "         # Agent B processes the input, potentially performs its own RAG search,\n",
        "         # inheriting or updating the model context from Agent A.\n",
        "         # Here we pass Agent A's context along, allowing Agent B to build upon it.\n",
        "         # A real system might merge or analyze contexts.\n",
        "         return self.process_query(input_from_a, incoming_model_context=agent_a_model_context)\n",
        "\n",
        "\n",
        "# 5. Orchestrate the Interaction (updated to pass Model Context)\n",
        "\n",
        "# Create the agents\n",
        "agent_a = AgentA(knowledge_base)\n",
        "agent_b = AgentB(knowledge_base)\n",
        "\n",
        "# Initial query to Agent A\n",
        "initial_query = \"What is the capital of France?\"\n",
        "agent_a_initial_response, agent_a_context_step1 = agent_a.initial_processing(initial_query)\n",
        "print(f\"Agent A's initial response: {agent_a_initial_response}\")\n",
        "print(f\"Agent A's context after step 1: {agent_a_context_step1.to_dict()}\\n\")\n",
        "\n",
        "# Agent A sends its response/context to Agent B\n",
        "# The query for Agent B is based on Agent A's processing, and A's context is passed\n",
        "agent_b_input = f\"Agent A is asking about: {initial_query}. Agent A found this info: {agent_a_initial_response}\"\n",
        "agent_b_response, agent_b_context_step2 = agent_b.process_input_from_agent_a(agent_b_input, agent_a_context_step1)\n",
        "print(f\"Agent B's response: {agent_b_response}\")\n",
        "print(f\"Agent B's context after step 2: {agent_b_context_step2.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# Agent A processes Agent B's response and context\n",
        "agent_a_final_ack, agent_a_context_step3 = agent_a.process_agent_b_response(initial_query, agent_b_response, agent_b_context_step2)\n",
        "print(f\"Agent A's final acknowledgment: {agent_a_final_ack}\")\n",
        "print(f\"Agent A's context after step 3: {agent_a_context_step3.to_dict()}\\n\")\n",
        "\n",
        "print(\"\\n--- Another Interaction (with MCP) ---\")\n",
        "\n",
        "# Another query\n",
        "another_query = \"Tell me about RAG.\"\n",
        "agent_a_initial_response_2, agent_a_context_step1_2 = agent_a.initial_processing(another_query)\n",
        "print(f\"Agent A's initial response: {agent_a_initial_response_2}\")\n",
        "print(f\"Agent A's context after step 1: {agent_a_context_step1_2.to_dict()}\\n\")\n",
        "\n",
        "agent_b_input_2 = f\"Agent A is asking about: {another_query}. Agent A found this info: {agent_a_initial_response_2}\"\n",
        "agent_b_response_2, agent_b_context_step2_2 = agent_b.process_input_from_agent_a(agent_b_input_2, agent_a_context_step1_2)\n",
        "print(f\"Agent B's response: {agent_b_response_2}\")\n",
        "print(f\"Agent B's context after step 2: {agent_b_context_step2_2.to_dict()}\\n\")\n",
        "\n",
        "agent_a_final_ack_2, agent_a_context_step3_2 = agent_a.process_agent_b_response(another_query, agent_b_response_2, agent_b_context_step2_2)\n",
        "print(f\"Agent A's final acknowledgment: {agent_a_final_ack_2}\")\n",
        "print(f\"Agent A's context after step 3: {agent_a_context_step3_2.to_dict()}\\n\")\n",
        "\n",
        "# Simulate another scenario using the defined agents and framework (with MCP)\n",
        "print(\"\\n--- SDLC Example: Handling a more complex query (with MCP) ---\")\n",
        "\n",
        "# Define a new query that might require combining information or further processing\n",
        "complex_query = \"Where is the Eiffel Tower and what is the highest mountain?\"\n",
        "\n",
        "# Agent A's initial processing\n",
        "agent_a_complex_response, agent_a_complex_context_step1 = agent_a.initial_processing(complex_query)\n",
        "print(f\"Agent A's initial response: {agent_a_complex_response}\")\n",
        "print(f\"Agent A's context after step 1: {agent_a_complex_context_step1.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# Agent A sends its response/context to Agent B\n",
        "agent_b_complex_input = f\"Agent A is asking about: {complex_query}. Agent A found this info: {agent_a_complex_response}\"\n",
        "agent_b_complex_response, agent_b_complex_context_step2 = agent_b.process_input_from_agent_a(agent_b_complex_input, agent_a_complex_context_step1)\n",
        "print(f\"Agent B's response: {agent_b_complex_response}\")\n",
        "print(f\"Agent B's context after step 2: {agent_b_complex_context_step2.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# Agent A processes Agent B's response and context\n",
        "agent_a_complex_final_ack, agent_a_complex_context_step3 = agent_a.process_agent_b_response(complex_query, agent_b_complex_response, agent_b_complex_context_step2)\n",
        "print(f\"Agent A's final acknowledgment: {agent_a_complex_final_ack}\")\n",
        "print(f\"Agent A's context after step 3: {agent_a_complex_context_step3.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n--- SDLC Example: Query with no direct answer in KB (with MCP) ---\")\n",
        "\n",
        "# Query that doesn't have a direct answer in the knowledge base\n",
        "unknown_query = \"What is the population of Tokyo?\"\n",
        "\n",
        "# Agent A's initial processing\n",
        "agent_a_unknown_response, agent_a_unknown_context_step1 = agent_a.initial_processing(unknown_query)\n",
        "print(f\"Agent A's initial response: {agent_a_unknown_response}\")\n",
        "print(f\"Agent A's context after step 1: {agent_a_unknown_context_step1.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# Agent A sends its response/context to Agent B\n",
        "agent_b_unknown_input = f\"Agent A is asking about: {unknown_query}. Agent A found this info: {agent_a_unknown_response}\"\n",
        "agent_b_unknown_response, agent_b_unknown_context_step2 = agent_b.process_input_from_agent_a(agent_b_unknown_input, agent_a_unknown_context_step1)\n",
        "print(f\"Agent B's response: {agent_b_unknown_response}\")\n",
        "print(f\"Agent B's context after step 2: {agent_b_unknown_context_step2.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# Agent A processes Agent B's response and context\n",
        "agent_a_unknown_final_ack, agent_a_unknown_context_step3 = agent_a.process_agent_b_response(unknown_query, agent_b_unknown_response, agent_b_unknown_context_step2)\n",
        "print(f\"Agent A's final acknowledgment: {agent_a_unknown_final_ack}\")\n",
        "print(f\"Agent A's context after step 3: {agent_a_unknown_context_step3.to_dict()}\\n\")\n",
        "\n",
        "\n",
        "# 6. Introduce a Supervisor/Orchestrator for more complex flows (updated for MCP)\n",
        "\n",
        "class Supervisor:\n",
        "    def __init__(self, agent_a, agent_b):\n",
        "        self.agent_a = agent_a\n",
        "        self.agent_b = agent_b\n",
        "\n",
        "    def handle_interaction(self, initial_query):\n",
        "        print(f\"\\nSupervisor initiating interaction for query: '{initial_query}'\")\n",
        "\n",
        "        # Supervisor directs the initial query to Agent A\n",
        "        # Agent A returns response and its updated context\n",
        "        agent_a_response_step1, agent_a_context_step1 = self.agent_a.initial_processing(initial_query)\n",
        "        print(f\"Supervisor received response from Agent A (Step 1): {agent_a_response_step1}\")\n",
        "        print(f\"Supervisor received context from Agent A (Step 1): {agent_a_context_step1.to_dict()}\")\n",
        "\n",
        "\n",
        "        # Supervisor decides the next step and passes Agent A's response and context to Agent B\n",
        "        agent_b_input = f\"Agent A processed the query '{initial_query}' and provided: {agent_a_response_step1}. Can you add anything?\"\n",
        "        agent_b_response_step2, agent_b_context_step2 = self.agent_b.process_input_from_agent_a(agent_b_input, agent_a_context_step1)\n",
        "        print(f\"Supervisor received response from Agent B (Step 2): {agent_b_response_step2}\")\n",
        "        print(f\"Supervisor received context from Agent B (Step 2): {agent_b_context_step2.to_dict()}\")\n",
        "\n",
        "\n",
        "        # Supervisor receives Agent B's response and context and passes it back to Agent A\n",
        "        final_response_ack, final_context_step3 = self.agent_a.process_agent_b_response(initial_query, agent_b_response_step2, agent_b_context_step2)\n",
        "        print(f\"Supervisor received final processing result from Agent A (Step 3): {final_response_ack}\")\n",
        "        print(f\"Supervisor received final context after step 3: {final_context_step3.to_dict()}\")\n",
        "\n",
        "\n",
        "        print(f\"Supervisor concludes interaction for query: '{initial_query}'\")\n",
        "        # In a real application, the Supervisor would synthesize the final user-facing output\n",
        "        # based on the responses and potentially the final context.\n",
        "        # For this example, we'll just return a simple acknowledgment and the final context.\n",
        "        return f\"Interaction completed by Supervisor for query: {initial_query}\", final_context_step3.to_dict()\n",
        "\n",
        "# Create the Supervisor\n",
        "supervisor = Supervisor(agent_a, agent_b)\n",
        "\n",
        "# Use the Supervisor to handle interactions\n",
        "final_output, final_mcp = supervisor.handle_interaction(\"What is the capital of France?\")\n",
        "print(f\"Final Supervisor Output: {final_output}\")\n",
        "print(f\"Final Model Context: {final_mcp}\\n\")\n",
        "\n",
        "\n",
        "final_output_2, final_mcp_2 = supervisor.handle_interaction(\"Tell me about AI and RAG.\")\n",
        "print(f\"Final Supervisor Output: {final_output_2}\")\n",
        "print(f\"Final Model Context: {final_mcp_2}\\n\")\n",
        "\n",
        "final_output_3, final_mcp_3 = supervisor.handle_interaction(\"What is the tallest building in the world?\") # Query outside KB to see interaction flow\n",
        "print(f\"Final Supervisor Output: {final_output_3}\")\n",
        "print(f\"Final Model Context: {final_mcp_3}\\n\")\n",
        "\n",
        "\n",
        "# Simulate a different knowledge base for Agent B\n",
        "knowledge_base_b = [\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"Jupyter notebooks are widely used for data science.\",\n",
        "    \"Google Colaboratory is a cloud-based Jupyter notebook environment.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "# Update Agent B to use its own knowledge base\n",
        "class AgentB(Agent): # Redefine AgentB for clarity with new KB\n",
        "    def __init__(self, knowledge_base, model_name=\"ModelB_KB2\"): # Give it a different model name\n",
        "        super().__init__(\"Agent B (KB2)\", knowledge_base, model_name)\n",
        "\n",
        "    def process_input_from_agent_a(self, input_from_a, agent_a_model_context):\n",
        "        print(f\"{self.name} received input from Agent A: '{input_from_a}'\")\n",
        "        print(f\"{self.name} received Model Context from Agent A: {agent_a_model_context.to_dict()}\")\n",
        "        # Agent B processes the input, potentially performs its own RAG search using its KB\n",
        "        return self.process_query(input_from_a, incoming_model_context=agent_a_model_context)\n",
        "\n",
        "\n",
        "# Recreate the agents with potentially different knowledge bases and model names\n",
        "agent_a = AgentA(knowledge_base, model_name=\"ModelA_KB1\") # Agent A uses the original KB\n",
        "agent_b = AgentB(knowledge_base_b, model_name=\"ModelB_KB2\") # Agent B uses its specific KB\n",
        "\n",
        "# Recreate the Supervisor with the updated agents\n",
        "supervisor = Supervisor(agent_a, agent_b)\n",
        "\n",
        "# Example 1: Query that might be in Agent A's KB but not Agent B's\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query in A, with MCP) ---\")\n",
        "final_output_4, final_mcp_4 = supervisor.handle_interaction(\"What is the highest mountain?\")\n",
        "print(f\"Final Supervisor Output: {final_output_4}\")\n",
        "print(f\"Final Model Context: {final_mcp_4}\\n\")\n",
        "\n",
        "\n",
        "# Example 2: Query that might be in Agent B's KB but not Agent A's\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query in B, with MCP) ---\")\n",
        "final_output_5, final_mcp_5 = supervisor.handle_interaction(\"Tell me about Python.\")\n",
        "print(f\"Final Supervisor Output: {final_output_5}\")\n",
        "print(f\"Final Model Context: {final_mcp_5}\\n\")\n",
        "\n",
        "\n",
        "# Example 3: Query that might require combining info (though simple RAG won't truly combine)\n",
        "# This demonstrates the flow when both agents might have relevant pieces or one doesn't.\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query potentially in both/neither, with MCP) ---\")\n",
        "final_output_6, final_mcp_6 = supervisor.handle_interaction(\"What is RAG and are Jupyter notebooks related to AI?\")\n",
        "print(f\"Final Supervisor Output: {final_output_6}\")\n",
        "print(f\"Final Model Context: {final_mcp_6}\\n\")\n",
        "\n",
        "\n",
        "# Example 4: Query outside both KBs\n",
        "print(\"\\n--- SDLC Example with Different KBs (Query outside both, with MCP) ---\")\n",
        "final_output_7, final_mcp_7 = supervisor.handle_interaction(\"What is the deepest ocean trench?\")\n",
        "print(f\"Final Supervisor Output: {final_output_7}\")\n",
        "print(f\"Final Model Context: {final_mcp_7}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rOQ50gP2Vbyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}