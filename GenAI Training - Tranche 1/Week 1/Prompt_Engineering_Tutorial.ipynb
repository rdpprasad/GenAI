{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdpprasad/GenAI/blob/main/Prompt_Engineering_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5r3TjJawZ-eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepared by Tamal Acharya"
      ],
      "metadata": {
        "id": "1L36hMq_sQmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Engineering\n",
        "\n",
        "Prompt engineering is the practice of designing and refining prompts to guide a language model to produce desired outputs. It involves crafting the input text or instructions given to the model to achieve a specific task, style, or response format. Different techniques are used to improve the model's understanding and performance."
      ],
      "metadata": {
        "id": "QNJ5mni3uJ32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here is a brief description of the prompt engineering techniques used in the notebook:\n",
        "\n",
        "\n",
        "1. Zero-shot Prompting: Providing a prompt without any examples, expecting the model to perform the task based on its pre-training.\n",
        "\n",
        "2. One-shot Prompting: Providing a single example in the prompt to guide the model's response.\n",
        "\n",
        "3. Few-shot Prompting: Providing a few examples in the prompt to illustrate the desired output format and style.\n",
        "\n",
        "4. Instruction Prompting: Giving explicit instructions within the prompt to direct the model's behavior.\n",
        "\n",
        "5. Chain-of-Thought Prompting: Asking the model to think step-by-step before providing the final answer. This encourages the model to show its reasoning process.\n",
        "\n",
        "6. Role Prompting: Assigning a specific persona or role to the model within the prompt.\n",
        "\n",
        "7. Contextual Prompting: Providing context about the source or situation of the input to help the model generate a relevant response.\n",
        "\n",
        "8. Prompt Templating with Placeholders: Using a template structure with placeholders to insert the input data into the prompt.\n",
        "\n",
        "9. Few-shot with Explanation: Similar to few-shot, but includes explanations for the provided examples.\n",
        "\n",
        "10. Step-by-step Reasoning: Explicitly asking the model to explain its reasoning process in steps.\n",
        "\n",
        "11. Self-Consistency Prompting: Asking the model to generate multiple possible answers and select the most likely one.\n",
        "\n",
        "12. Interactive Prompting: Designing the prompt to simulate an interactive conversation or question-answer flow.\n",
        "\n",
        "13. Negative Prompting: Specifying what the model should not do in its response.\n",
        "Few-shot with Format Constraints: Providing examples that demonstrate a specific required output format.\n",
        "\n",
        "15. Multi-step Task Decomposition: Breaking down a complex task into smaller, sequential steps within the prompt."
      ],
      "metadata": {
        "id": "Yf87MzhJWgcm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8Jee4NIWewr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Yyz0cQUUSng"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# # Prompt Engineering Techniques with OpenAI API\n",
        "# # =============================================\n",
        "# # This notebook covers various prompting techniques to get the best results from language models.\n",
        "# # Each cell includes explanation, prompt examples, and Python code to call OpenAI API.\n",
        "\n",
        "# import openai\n",
        "\n",
        "# # --- Set your OpenAI API key ---\n",
        "# openai.api_key = \"YOUR_API_KEY\"\n",
        "\n",
        "# def query_openai(prompt, model=\"gpt-4o-mini\", max_tokens=100, temperature=0.7):\n",
        "#     \"\"\"\n",
        "#     Helper function to query OpenAI chat completion endpoint.\n",
        "#     \"\"\"\n",
        "#     response = openai.ChatCompletion.create(\n",
        "#         model=model,\n",
        "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#         max_tokens=max_tokens,\n",
        "#         temperature=temperature,\n",
        "#     )\n",
        "#     return response['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "# Prompt Engineering Techniques with Google Gemini and Cohere API\n",
        "# ===================================================\n",
        "# This notebook covers various prompting techniques to get the best results from language models.\n",
        "# Each cell includes explanation, prompt examples, and Python code to call Google Gemini API.\n",
        "\n",
        "# Install the Google Generative AI library\n",
        "!pip install -q -U google-generativeai==0.3.1\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Set your Google Cloud API key ---\n",
        "# You can get this from the Google Cloud console\n",
        "GOOGLE_API_KEY = \"AIzaSyCE_oRfRmDP5zikCdiZP5LF0ZwZR9wCuhQ\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# You can get your Cohere API key from the Cohere dashboard\n",
        "#COHERE_API_KEY = \"YOUR_COHERE_API_KEY\"\n",
        "#co = cohere.Client(COHERE_API_KEY)\n",
        "\n",
        "def query_gemini(prompt, model=\"gemini-2.0-flash-001\", max_output_tokens=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Helper function to query Google Gemini chat completion endpoint.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(model)\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=genai.types.GenerationConfig(\n",
        "            max_output_tokens=max_output_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "def query_cohere(prompt, model=\"command\", max_tokens=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Helper function to query Cohere chat completion endpoint.\n",
        "    \"\"\"\n",
        "    response = co.generate(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.generations[0].text\n",
        "\n",
        "# Set the preferred model to use\n",
        "PREFERRED_MODEL = \"gemini\" # Change to \"cohere\" to use Cohere\n",
        "\n",
        "def query_language_model(prompt):\n",
        "    if PREFERRED_MODEL == \"gemini\":\n",
        "        return query_gemini(prompt)\n",
        "    elif PREFERRED_MODEL == \"cohere\":\n",
        "        return query_cohere(prompt)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid PREFERRED_MODEL specified.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 1. Zero-shot Prompting --\n",
        "#Zero-shot Prompting: Providing a prompt without any examples, expecting the model to perform the task based on its pre-training.\n",
        "print(\"# 1. Zero-shot Prompting\")\n",
        "zero_shot_prompt = \"Translate the following English sentence to French: 'I love programming.'\"\n",
        "print(\"Prompt:\\n\", zero_shot_prompt)\n",
        "print(\"Response:\\n\", query_language_model(zero_shot_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kB6uc3oCUcVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b74cde31-e0a2-41c0-d773-0ae63b9320cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 1. Zero-shot Prompting\n",
            "Prompt:\n",
            " Translate the following English sentence to French: 'I love programming.'\n",
            "Response:\n",
            " The most common and direct translation is:\n",
            "\n",
            "**J'adore programmer.**\n",
            "\n",
            "Here are a few other options, depending on the nuance you want to convey:\n",
            "\n",
            "*   **J'aime programmer.** (This is a more general \"I like programming.\")\n",
            "*   **J'aime beaucoup programmer.** (This means \"I like programming a lot.\")\n",
            "*   **Je suis passionn√©(e) par la programmation.** (This means \"I am passionate about programming.\" Note the\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 2. One-shot Prompting --\n",
        "#One-shot Prompting: Providing a single example in the prompt to guide the model's response.\n",
        "print(\"# 2. One-shot Prompting\")\n",
        "one_shot_prompt = \"\"\"Translate English to French.\n",
        "Example: \"\"Hello\"\" ‚Üí \"\"Bonjour\"\"\n",
        "Now translate: \"I love programming.\"\"\"\n",
        "print(\"Prompt:\\n\", one_shot_prompt)\n",
        "print(\"Response:\\n\", query_language_model(one_shot_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qlxoEC_0UjbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "25d27f78-8c2c-47f1-e865-a425642a635f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 2. One-shot Prompting\n",
            "Prompt:\n",
            " Translate English to French.\n",
            "Example: \"\"Hello\"\" ‚Üí \"\"Bonjour\"\"\n",
            "Now translate: \"I love programming.\n",
            "Response:\n",
            " J'adore la programmation.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 3. Few-shot Prompting --\n",
        "#Few-shot Prompting: Providing a few examples in the prompt to illustrate the desired output format and style.\n",
        "print(\"# 3. Few-shot Prompting\")\n",
        "few_shot_prompt = \"\"\"Translate English to French.\n",
        "\"\"Hello\"\" ‚Üí \"\"Bonjour\"\"\n",
        "\"\"Goodbye\"\" ‚Üí \"\"Au revoir\"\"\n",
        "Now translate: \"I love programming.\"\"\"\n",
        "print(\"Prompt:\\n\", few_shot_prompt)\n",
        "print(\"Response:\\n\", query_language_model(few_shot_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fmXI3jcOUjUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "cfc63d4f-4ca9-4dd4-b844-0630c60b9ef3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 3. Few-shot Prompting\n",
            "Prompt:\n",
            " Translate English to French.\n",
            "\"\"Hello\"\" ‚Üí \"\"Bonjour\"\"\n",
            "\"\"Goodbye\"\" ‚Üí \"\"Au revoir\"\"\n",
            "Now translate: \"I love programming.\n",
            "Response:\n",
            " J'aime programmer.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 4. Instruction Prompting --\n",
        "#Instruction Prompting: Giving explicit instructions within the prompt to direct the model's behavior.\n",
        "print(\"# 4. Instruction Prompting\")\n",
        "instruction_prompt = \"You are a helpful assistant. Translate the following sentence into French: 'I love programming.'\"\n",
        "print(\"Prompt:\\n\", instruction_prompt)\n",
        "print(\"Response:\\n\", query_language_model(instruction_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "p3IKsfQMUjRV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "059df4b2-80ed-4b05-8e3e-943b5c9eac90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 4. Instruction Prompting\n",
            "Prompt:\n",
            " You are a helpful assistant. Translate the following sentence into French: 'I love programming.'\n",
            "Response:\n",
            " There are a few ways to translate \"I love programming\" into French, depending on the nuance you want to convey:\n",
            "\n",
            "*   **J'adore programmer.** (This is the most common and generally the best translation. It's a strong and enthusiastic \"I love\".)\n",
            "\n",
            "*   **J'aime programmer.** (This is also correct, but \"aime\" is a slightly weaker \"love\" than \"adore\". It's more like \"I like\" but can still be\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 5. Chain-of-Thought Prompting --\n",
        "#Chain-of-Thought Prompting: Asking the model to think step-by-step before providing the final answer. This encourages the model to show its reasoning process.\n",
        "print(\"# 5. Chain-of-Thought Prompting\")\n",
        "cot_prompt = \"\"\"Translate the sentence \"I love programming\" into French.\n",
        "Think step-by-step before giving the final answer.\"\"\"\n",
        "print(\"Prompt:\\n\", cot_prompt)\n",
        "print(\"Response:\\n\", query_language_model(cot_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LJSOrG17UjOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "fba307f6-29a8-4f09-e8a2-9a96c6b54e56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 5. Chain-of-Thought Prompting\n",
            "Prompt:\n",
            " Translate the sentence \"I love programming\" into French.\n",
            "Think step-by-step before giving the final answer.\n",
            "Response:\n",
            " Okay, let's think about the French words for each part of the sentence:\n",
            "\n",
            "*   \"I\" translates to \"Je\"\n",
            "*   \"love\" translates to \"aime\"\n",
            "*   \"programming\" translates to \"programmation\"\n",
            "\n",
            "Now, let's put them together. Since \"Je\" is followed by a verb starting with a vowel, it becomes \"J'\".\n",
            "\n",
            "Therefore, the complete sentence is:\n",
            "\n",
            "\"J'aime la programmation.\"\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 6. Role Prompting --\n",
        "#Role Prompting: Assigning a specific persona or role to the model within the prompt.\n",
        "print(\"# 6. Role Prompting\")\n",
        "role_prompt = \"\"\"You are a professional French translator.\n",
        "Translate: \"\"I love programming.\"\"\"\"\"\n",
        "print(\"Prompt:\\n\", role_prompt)\n",
        "print(\"Response:\\n\", query_language_model(role_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zckKOubGUjL_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "9dbe5767-b603-4748-9de9-18717c8cc60d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 6. Role Prompting\n",
            "Prompt:\n",
            " You are a professional French translator.\n",
            "Translate: \"\"I love programming.\n",
            "Response:\n",
            " There are a few ways to translate \"I love programming\" into French, depending on the nuance you want to convey:\n",
            "\n",
            "*   **J'aime la programmation.** (This is the most common and straightforward translation. It's generally suitable for most contexts.)\n",
            "\n",
            "*   **J'adore la programmation.** (This expresses a stronger feeling of love, similar to \"I adore programming.\")\n",
            "\n",
            "*   **J'aime beaucoup la programmation.** (This means \"I like programming\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 7. Contextual Prompting --\n",
        "#Contextual Prompting: Providing context about the source or situation of the input to help the model generate a relevant response.\n",
        "print(\"# 7. Contextual Prompting\")\n",
        "context_prompt = \"\"\"The following sentence is from a technical programming blog.\n",
        "Translate it into French: \"\"I love programming.\"\"\"\"\"\n",
        "print(\"Prompt:\\n\", context_prompt)\n",
        "print(\"Response:\\n\", query_language_model(context_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RIB-vxW8UjK5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9b2bfdbf-afe9-46f7-dd25-5e84a5285606"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 7. Contextual Prompting\n",
            "Prompt:\n",
            " The following sentence is from a technical programming blog.\n",
            "Translate it into French: \"\"I love programming.\n",
            "Response:\n",
            " Here are a few options for translating \"I love programming\" into French, with slightly different nuances:\n",
            "\n",
            "*   **J'adore programmer.** (This is a very common and natural translation, implying a strong liking.)\n",
            "\n",
            "*   **J'aime programmer.** (This is a more direct and literal translation, meaning \"I like to program.\" While correct, it's generally considered slightly less enthusiastic than \"J'adore.\")\n",
            "\n",
            "*   **J'aime beaucoup programmer.** (This translates\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 8. Prompt Templating with Placeholders --\n",
        "#Prompt Templating with Placeholders: Using a template structure with placeholders to insert the input data into the prompt.\n",
        "print(\"# 8. Prompt Templating with Placeholders\")\n",
        "sentence = \"I love programming.\"\n",
        "template_prompt = f\"Translate the sentence \\\"{sentence}\\\" into French.\"\n",
        "print(\"Prompt:\\n\", template_prompt)\n",
        "print(\"Response:\\n\", query_language_model(template_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uA5K-odRUjJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b018cd24-a65a-48da-edee-68e32566fd5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 8. Prompt Templating with Placeholders\n",
            "Prompt:\n",
            " Translate the sentence \"I love programming.\" into French.\n",
            "Response:\n",
            " The most common and natural translation of \"I love programming\" into French is:\n",
            "\n",
            "**J'adore la programmation.**\n",
            "\n",
            "Here are a few other options, with slightly different nuances:\n",
            "\n",
            "*   **J'aime la programmation.** (This is also correct, but \"adorer\" expresses a stronger liking than \"aimer\")\n",
            "*   **J'aime programmer.** (This focuses on the act of programming rather than programming in general. It translates to \"I like to program\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 9. Few-shot with Explanation --\n",
        "#Few-shot with Explanation: Similar to few-shot, but includes explanations for the provided examples.\n",
        "print(\"# 9. Few-shot with Explanation\")\n",
        "few_shot_explain_prompt = \"\"\"Translate English to French with explanation.\n",
        "Example: \"\"Hello\"\" ‚Üí \"\"Bonjour\"\" because \"\"Bonjour\"\" means \"\"Hello\"\" in French.\n",
        "Now translate: \"\"I love programming.\"\"\"\"\"\n",
        "print(\"Prompt:\\n\", few_shot_explain_prompt)\n",
        "print(\"Response:\\n\", query_language_model(few_shot_explain_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hG5X6lr1UjI0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "76a81707-c59e-4187-91e8-9f9394e7cf21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 9. Few-shot with Explanation\n",
            "Prompt:\n",
            " Translate English to French with explanation.\n",
            "Example: \"\"Hello\"\" ‚Üí \"\"Bonjour\"\" because \"\"Bonjour\"\" means \"\"Hello\"\" in French.\n",
            "Now translate: \"\"I love programming.\n",
            "Response:\n",
            " \"I love programming.\" ‚Üí \"J'adore programmer.\" because \"J'adore\" means \"I love/adore\" and \"programmer\" is the infinitive form of the verb \"to program\". While \"J'aime programmer\" is also correct and means \"I like programming,\" \"J'adore programmer\" expresses a stronger feeling of liking/loving programming.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 10. Step-by-step Reasoning --\n",
        "#Step-by-step Reasoning: Explicitly asking the model to explain its reasoning process in steps.\n",
        "print(\"# 10. Step-by-step Reasoning\")\n",
        "step_by_step_prompt = \"\"\"Translate the sentence \"I love programming\" into French.\n",
        "Explain your reasoning step-by-step before providing the final translation.\"\"\"\n",
        "print(\"Prompt:\\n\", step_by_step_prompt)\n",
        "print(\"Response:\\n\", query_language_model(step_by_step_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "27dLKvt7UjDc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "093e5ebd-cfef-480e-aff2-7647d86fdc64"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 10. Step-by-step Reasoning\n",
            "Prompt:\n",
            " Translate the sentence \"I love programming\" into French.\n",
            "Explain your reasoning step-by-step before providing the final translation.\n",
            "Response:\n",
            " Okay, let's break down how to translate \"I love programming\" into French:\n",
            "\n",
            "1.  **\"I\"**: The French pronoun for \"I\" is \"Je\".\n",
            "\n",
            "2.  **\"Love\"**: There are several ways to express \"love\" in French, depending on the intensity and the object of the love. In this case, programming is an activity, not a person. Therefore, we wouldn't use \"aimer\" in the romantic sense. A\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 11. Self-Consistency Prompting --\n",
        "#Self-Consistency Prompting: Asking the model to generate multiple possible answers and select the most likely one.\n",
        "print(\"# 11. Self-Consistency Prompting\")\n",
        "self_consistency_prompt = \"\"\"Translate 'I love programming' to French.\n",
        "Generate multiple possible translations and pick the most accurate.\"\"\"\n",
        "print(\"Prompt:\\n\", self_consistency_prompt)\n",
        "print(\"Response:\\n\", query_language_model(self_consistency_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9fHFfeLDUjCY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "2eb5c84b-e5c4-4ce2-b44b-08951173e709"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 11. Self-Consistency Prompting\n",
            "Prompt:\n",
            " Translate 'I love programming' to French.\n",
            "Generate multiple possible translations and pick the most accurate.\n",
            "Response:\n",
            " Here are several possible translations of \"I love programming\" into French:\n",
            "\n",
            "*   **J'aime programmer.** (This is a very common and straightforward translation.)\n",
            "*   **J'adore programmer.** (This is a stronger expression of love, using \"adorer\" which means \"to adore.\")\n",
            "*   **J'aime beaucoup la programmation.** (This translates to \"I like programming a lot,\" and uses the noun \"la programmation.\")\n",
            "*   **Je suis\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 12. Interactive Prompting --\n",
        "#Interactive Prompting: Designing the prompt to simulate an interactive conversation or question-answer flow.\n",
        "print(\"# 12. Interactive Prompting\")\n",
        "interactive_prompt = \"\"\"You will ask questions and then translate the answers into French.\n",
        "Q: What do you love?\n",
        "A: Programming.\n",
        "Now translate 'Programming' into French.\"\"\"\n",
        "print(\"Prompt:\\n\", interactive_prompt)\n",
        "print(\"Response:\\n\", query_language_model(interactive_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pEozK8LwVMYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "419f88ca-4772-49b5-dbf9-6d60928b526d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 12. Interactive Prompting\n",
            "Prompt:\n",
            " You will ask questions and then translate the answers into French.\n",
            "Q: What do you love?\n",
            "A: Programming.\n",
            "Now translate 'Programming' into French.\n",
            "Response:\n",
            " La programmation.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 13. Negative Prompting (what NOT to do) --\n",
        "#Negative Prompting: Specifying what the model should not do in its response. Few-shot with Format Constraints: Providing examples that demonstrate a specific required output format.\n",
        "print(\"# 13. Negative Prompting\")\n",
        "negative_prompt = \"\"\"Translate the following sentence into French, but do NOT use formal language: 'I love programming.'\"\"\"\n",
        "print(\"Prompt:\\n\", negative_prompt)\n",
        "print(\"Response:\\n\", query_language_model(negative_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_V1GZwfyVMU1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "485cbb92-0eb9-4d9f-af85-3edf9ff0cfec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 13. Negative Prompting\n",
            "Prompt:\n",
            " Translate the following sentence into French, but do NOT use formal language: 'I love programming.'\n",
            "Response:\n",
            " Here are a few options, from slightly more \"standard\" to more slangy:\n",
            "\n",
            "*   **J'adore programmer.** (This is a very common and perfectly acceptable way to say it, just less formal than \"J'aime programmer.\")\n",
            "*   **Je kiffe programmer.** (Using \"kiffer\" is slang for \"to like/love,\" especially something you enjoy doing.)\n",
            "*   **Je suis dingue de programmation.** (This means \"I'm crazy\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 14. Few-shot with Format Constraints --\n",
        "#Provides examples to demonstrate a specific required output format for the model.\n",
        "print(\"# 14. Few-shot with Format Constraints\")\n",
        "format_constraint_prompt = \"\"\"Translate English to French in the format: English_sentence ‚Üí French_sentence (no explanation).\n",
        "\"Hello\" ‚Üí \"Bonjour\"\n",
        "\"\"Goodbye\"\" ‚Üí \"\"Au revoir\"\"\n",
        "Now translate: \"\"I love programming.\"\"\"\"\"\n",
        "print(\"Prompt:\\n\", format_constraint_prompt)\n",
        "print(\"Response:\\n\", query_language_model(format_constraint_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cSrglUSVVMSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "fd87785d-1d16-4ccc-e41b-2e8e524b5891"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 14. Few-shot with Format Constraints\n",
            "Prompt:\n",
            " Translate English to French in the format: English_sentence ‚Üí French_sentence (no explanation).\n",
            "\"Hello\" ‚Üí \"Bonjour\"\n",
            "\"\"Goodbye\"\" ‚Üí \"\"Au revoir\"\"\n",
            "Now translate: \"\"I love programming.\n",
            "Response:\n",
            " \"\"I love programming.\"\" ‚Üí \"\"J'adore la programmation.\"\"\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- 15. Multi-step Task Decomposition --\n",
        "#Multi-step Task Decomposition: Breaking down a complex task into smaller, sequential steps within the prompt.\n",
        "print(\"# 15. Multi-step Task Decomposition\")\n",
        "multi_step_prompt = \"\"\"First, identify the language of this sentence: 'I love programming.'\n",
        "Then, translate it into French.\"\"\"\n",
        "print(\"Prompt:\\n\", multi_step_prompt)\n",
        "print(\"Response:\\n\", query_language_model(multi_step_prompt))\n",
        "print(\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JN4EiGcdVMO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "4b6a9388-0d95-4487-d115-a85a80eae74b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 15. Multi-step Task Decomposition\n",
            "Prompt:\n",
            " First, identify the language of this sentence: 'I love programming.'\n",
            "Then, translate it into French.\n",
            "Response:\n",
            " The language of the sentence 'I love programming' is **English**.\n",
            "\n",
            "The translation of the sentence into French is:\n",
            "\n",
            "**J'adore programmer.**\n",
            "\n",
            "Alternatively, you could also say:\n",
            "\n",
            "**J'aime programmer.** (This is a more common and general translation).\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis Prompting\n",
        "\n",
        "#This technique involves prompting the model to identify the sentiment\n",
        " #(e.g., positive, negative, neutral) of a given text.\n",
        "\n",
        "# -- Sentiment Analysis Prompting --\n",
        "print(\"# Sentiment Analysis Prompting\")\n",
        "sentiment_prompt = \"Analyze the sentiment of the following review: 'This product is amazing!'\"\n",
        "print(\"Prompt:\\n\", sentiment_prompt)\n",
        "print(\"Response:\\n\", query_language_model(sentiment_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "r401mdgTXN3P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "638be45d-fc55-4367-9d9e-a332b2264c1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Sentiment Analysis Prompting\n",
            "Prompt:\n",
            " Analyze the sentiment of the following review: 'This product is amazing!'\n",
            "Response:\n",
            " The sentiment of the review \"This product is amazing!\" is overwhelmingly **positive**.\n",
            "\n",
            "Here's why:\n",
            "\n",
            "*   **\"Amazing\"** is a strong, positive adjective. It conveys a high degree of satisfaction and enthusiasm.\n",
            "*   The exclamation point (!) further emphasizes the positive sentiment and excitement.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization Prompting\n",
        "\n",
        "#This technique involves asking the model to summarize a given text.\n",
        "\n",
        "# -- Summarization Prompting --\n",
        "print(\"# Summarization Prompting\")\n",
        "text_to_summarize = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. This sentence is often used to test\n",
        "typewriters and keyboards because it contains all of the letters in the English alphabet.\n",
        "It's a pangram.\n",
        "\"\"\"\n",
        "summarization_prompt = f\"Summarize the following text: {text_to_summarize}\"\n",
        "print(\"Prompt:\\n\", summarization_prompt)\n",
        "print(\"Response:\\n\", query_language_model(summarization_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "pF3qJUfWXhzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "5db1fb92-d19c-485d-a422-69c0c72f3b90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Summarization Prompting\n",
            "Prompt:\n",
            " Summarize the following text: \n",
            "The quick brown fox jumps over the lazy dog. This sentence is often used to test\n",
            "typewriters and keyboards because it contains all of the letters in the English alphabet.\n",
            "It's a pangram.\n",
            "\n",
            "Response:\n",
            " The sentence \"The quick brown fox jumps over the lazy dog\" is a pangram, meaning it contains every letter of the English alphabet. This makes it useful for testing typewriters and keyboards.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question Answering Prompting\n",
        "\n",
        "# This technique involves providing context and a question,\n",
        "# and asking the model to answer based on the provided information.\n",
        "\n",
        "# -- Question Answering Prompting --\n",
        "print(\"# Question Answering Prompting\")\n",
        "context = \"\"\"\n",
        "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
        "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
        "Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially\n",
        "criticized by some of France's leading artists and intellectuals for its design,\n",
        "but it has become a global cultural icon of France and one of the most recognisable\n",
        "structures in the world.\n",
        "\"\"\"\n",
        "question = \"Who designed and built the Eiffel Tower?\"\n",
        "qa_prompt = f\"Based on the following text, answer the question.\\n\\nContext: {context}\\n\\nQuestion: {question}\"\n",
        "print(\"Prompt:\\n\", qa_prompt)\n",
        "print(\"Response:\\n\", query_language_model(qa_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "DoCNwgsXXhv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "a5d42aa7-d259-4356-8c60-830296d8b709"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Question Answering Prompting\n",
            "Prompt:\n",
            " Based on the following text, answer the question.\n",
            "\n",
            "Context: \n",
            "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
            "It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\n",
            "Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially\n",
            "criticized by some of France's leading artists and intellectuals for its design,\n",
            "but it has become a global cultural icon of France and one of the most recognisable\n",
            "structures in the world.\n",
            "\n",
            "\n",
            "Question: Who designed and built the Eiffel Tower?\n",
            "Response:\n",
            " Gustave Eiffel's company designed and built the Eiffel Tower.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Generation (Creative Writing) Prompting\n",
        "\n",
        "# This technique involves prompting the model to generate creative content,\n",
        "# like a poem or story, based on a given theme or prompt.\n",
        "\n",
        "# -- Text Generation (Creative Writing) Prompting --\n",
        "print(\"# Text Generation (Creative Writing) Prompting\")\n",
        "creative_prompt = \"Write a short poem about a rainy day.\"\n",
        "print(\"Prompt:\\n\", creative_prompt)\n",
        "print(\"Response:\\n\", query_language_model(creative_prompt))\n",
        "print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "STQp2mjBXhs-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "271ea4e2-ba0d-461b-a108-e60847f4b18d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Text Generation (Creative Writing) Prompting\n",
            "Prompt:\n",
            " Write a short poem about a rainy day.\n",
            "Response:\n",
            " The sky weeps a gentle tear,\n",
            "A silver curtain, soft and clear.\n",
            "The world is hushed, a muted tone,\n",
            "As raindrops patter on the stone.\n",
            "\n",
            "The thirsty earth drinks deep and slow,\n",
            "Where tiny rivulets now flow.\n",
            "A cozy day, a peaceful scene,\n",
            "Washed clean, and fresh, and evergreen.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta Prompting\n",
        "\n",
        "#Meta Prompting involves thinking about how to structure your prompts or using specific syntax to guide the model's output format [1].\n",
        "#It can also refer to a process of generating and refining prompts by evaluating the outputs [2].\n",
        "#This section focuses on the structural and syntax-focused aspects within a single prompt.\n",
        "\n",
        "# -- Meta Prompting --\n",
        "print(\"# Meta Prompting\")\n",
        "meta_prompt = \"\"\"Structure the following information about a book in JSON format:\n",
        "Title: The Hitchhiker's Guide to the Galaxy\n",
        "Author: Douglas Adams\n",
        "Genre: Science Fiction\n",
        "Publication Year: 1979\n",
        "\"\"\"\n",
        "print(\"Prompt:\\n\", meta_prompt)\n",
        "print(\"Response:\\n\", query_language_model(meta_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "0ukC-AM-XhqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "35febb3f-2da5-4054-b188-882efd2d2345"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Meta Prompting\n",
            "Prompt:\n",
            " Structure the following information about a book in JSON format:\n",
            "Title: The Hitchhiker's Guide to the Galaxy\n",
            "Author: Douglas Adams\n",
            "Genre: Science Fiction\n",
            "Publication Year: 1979\n",
            "\n",
            "Response:\n",
            " ```json\n",
            "{\n",
            "  \"title\": \"The Hitchhiker's Guide to the Galaxy\",\n",
            "  \"author\": \"Douglas Adams\",\n",
            "  \"genre\": \"Science Fiction\",\n",
            "  \"publication_year\": 1979\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Prompting (Simulated)\n",
        "\n",
        "#Retrieval Augmented Generation (RAG) involves retrieving relevant information from external sources and using it as context to generate a response [1]. While a full RAG system is more complex, we can simulate this technique by providing the necessary context directly within the prompt.\n",
        "#The model will then use this provided information to answer the question.\n",
        "\n",
        "# -- RAG Prompting (Simulated) --\n",
        "print(\"# RAG Prompting (Simulated)\")\n",
        "\n",
        "# This text represents the \"retrieved\" information that would be used as context in a RAG system.\n",
        "retrieved_context = \"\"\"\n",
        "The capital of France is Paris. It is located on the Seine River and is a major center for art, fashion, and culture.\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = f\"\"\"Based on the following information, what is the capital of France?\n",
        "\n",
        "Information: {retrieved_context}\n",
        "\"\"\"\n",
        "print(\"Prompt:\\n\", rag_prompt)\n",
        "print(\"Response:\\n\", query_language_model(rag_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "R8b1fYlQYJXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "560de337-3fd6-446b-845a-a21863c4b10c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# RAG Prompting (Simulated)\n",
            "Prompt:\n",
            " Based on the following information, what is the capital of France?\n",
            "\n",
            "Information: \n",
            "The capital of France is Paris. It is located on the Seine River and is a major center for art, fashion, and culture.\n",
            "\n",
            "\n",
            "Response:\n",
            " Based on the information provided, the capital of France is **Paris**.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instruction-Based Prompting\n",
        "\n",
        "#Description: Instruction-based prompting is a fundamental technique where you provide clear and explicit instructions to the language model within the prompt itself.\n",
        "#This directs the model's behavior and guides it towards generating the desired output.\n",
        "#It's like giving the model a set of commands to follow.\n",
        "\n",
        "# -- 4. Instruction Prompting --\n",
        "print(\"# 4. Instruction Prompting\")\n",
        "instruction_prompt = \"You are a helpful assistant. Translate the following sentence into French: 'I love programming.'\"\n",
        "print(\"Prompt:\\n\", instruction_prompt)\n",
        "print(\"Response:\\n\", query_language_model(instruction_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "1v7AD6SsY5Zf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "60614afa-5ec6-4821-870d-dbf5ec33def3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 4. Instruction Prompting\n",
            "Prompt:\n",
            " You are a helpful assistant. Translate the following sentence into French: 'I love programming.'\n",
            "Response:\n",
            " Here are a few ways to translate \"I love programming\" into French, with slightly different nuances:\n",
            "\n",
            "*   **J'adore programmer.** (This is a very common and natural translation, using \"adorer\" which means \"to adore\" or \"to love a lot.\")\n",
            "\n",
            "*   **J'aime programmer.** (This is also correct and very common. \"Aimer\" means \"to like\" or \"to love.\" It's a bit less intense than \"adorer\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ry4TBLUKY_sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Delimiters for Clarity\n",
        "\n",
        "#Description: Using delimiters in your prompt helps to clearly separate different parts of the prompt, such as instructions, context, and the input text.\n",
        "#This makes the prompt easier for the model to parse and reduces ambiguity, leading to more accurate and consistent responses.\n",
        "#Common delimiters include triple quotes (\"\"\"), angled brackets (<>), or XML tags (<tag>).\n",
        "\n",
        "# -- Delimiters for Clarity --\n",
        "print(\"# Delimiters for Clarity\")\n",
        "delimiters_prompt = \"\"\"\n",
        "Translate the following text into Spanish.\n",
        "\n",
        "Text to translate:\n",
        "<<<\n",
        "Hello, how are you? I am learning about prompt engineering.\n",
        ">>>\n",
        "\n",
        "Instructions:\n",
        "- Provide only the Spanish translation.\n",
        "- Do not include any additional explanations.\n",
        "\"\"\"\n",
        "print(\"Prompt:\\n\", delimiters_prompt)\n",
        "print(\"Response:\\n\", query_language_model(delimiters_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "9GO1Si5gY5Rk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2538c75a-dcf3-4681-d357-dae4cd1caf6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Delimiters for Clarity\n",
            "Prompt:\n",
            " \n",
            "Translate the following text into Spanish.\n",
            "\n",
            "Text to translate:\n",
            "<<<\n",
            "Hello, how are you? I am learning about prompt engineering.\n",
            ">>>\n",
            "\n",
            "Instructions:\n",
            "- Provide only the Spanish translation.\n",
            "- Do not include any additional explanations.\n",
            "\n",
            "Response:\n",
            " Hola, ¬øc√≥mo est√°s? Estoy aprendiendo sobre la ingenier√≠a de prompts.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Chaining Techniques\n",
        "\n",
        "# Description: Prompt chaining involves breaking down a complex task into a series of smaller, sequential prompts.\n",
        "# The output of one prompt serves as the input for the next prompt.\n",
        "# This allows you to build up to a final desired result by guiding the model through intermediate steps.\n",
        "# While a full implementation often involves application logic to manage the flow, we can illustrate the concept with a multi-step prompt that implies chaining.\n",
        "\n",
        "# -- 15. Multi-step Task Decomposition --\n",
        "print(\"# 15. Multi-step Task Decomposition\")\n",
        "multi_step_prompt = \"\"\"First, identify the language of this sentence: 'I love programming.'\n",
        "Then, translate it into French.\"\"\"\n",
        "print(\"Prompt:\\n\", multi_step_prompt)\n",
        "print(\"Response:\\n\", query_language_model(multi_step_prompt))\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "Zjxhan7vY5Bh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "c2c58bec-4600-44db-af2f-35fc37f0de67"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 15. Multi-step Task Decomposition\n",
            "Prompt:\n",
            " First, identify the language of this sentence: 'I love programming.'\n",
            "Then, translate it into French.\n",
            "Response:\n",
            " The language of the sentence \"I love programming\" is **English**.\n",
            "\n",
            "The French translation is: **J'adore programmer.**\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional techniques can be added similarly...\n",
        "\n",
        "print(\"All techniques demonstrated.\")\n"
      ],
      "metadata": {
        "id": "tZCfdyiTVVo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WvyGAJcRqoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Demonstrating Prompting Techniques in the Context of SDLC\n",
        "\n",
        "Prompt engineering techniques are more related to interacting with pre-trained language models rather than the traditional Software Development Life Cycle (SDLC). However, we can explore how these techniques might be applied at different stages or within processes that are part of the SDLC, particularly in the context of developing applications that utilize language models.\n",
        "\n",
        "In this context, the prompt engineering techniques mentioned in the notebook (like Zero-shot, Few-shot, Instruction, Chain-of-Thought, etc.) are primarily tools used within the Development and Maintenance phases of an SDLC for applications that rely on language models. They are methods for crafting the inputs to the model to achieve the desired outputs for specific tasks within the software."
      ],
      "metadata": {
        "id": "KBlruxBezDvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai==0.3.1\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Set your Google Cloud API key ---\n",
        "# You can get this from the Google Cloud console\n",
        "GOOGLE_API_KEY = \"YOUR_GOOGLE_API_KEY\" # Replace with your actual API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def query_gemini(prompt, model=\"gemini-pro\", max_output_tokens=200, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Helper function to query Google Gemini chat completion endpoint.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(model)\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                max_output_tokens=max_output_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "print(\"Demonstrating Prompting Techniques in the Context of SDLC\")\n",
        "print(\"=======================================================\\n\")\n",
        "\n",
        "# --- 1. Requirement Gathering / Analysis Phase ---\n",
        "# Prompting can be used to help define the scope and desired behavior of the language model component.\n",
        "print(\"# 1. Requirement Gathering / Analysis: Defining Model Behavior\")\n",
        "requirement_prompt = \"\"\"\n",
        "As part of our new application, we need a feature that summarizes customer feedback.\n",
        "Describe the ideal output format and key information that should be included in the summary\n",
        "for a large volume of text reviews. Consider negative, positive, and neutral feedback.\n",
        "\"\"\"\n",
        "print(\"Prompt for requirement analysis:\\n\", requirement_prompt)\n",
        "print(\"Simulated Model Response (for analysis):\\n\", query_gemini(requirement_prompt))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# --- 2. Design Phase ---\n",
        "# Prompting can be used to prototype interactions and design how the language model will fit into the application.\n",
        "print(\"# 2. Design Phase: Prototyping Interactions\")\n",
        "design_prompt = \"\"\"\n",
        "Design a conversation flow where a user asks for product recommendations based on their preferences.\n",
        "Outline the first few turns of the conversation, showing how the AI would respond and ask follow-up questions.\n",
        "User: I'm looking for a new laptop for graphic design.\n",
        "\"\"\"\n",
        "print(\"Prompt for designing interaction flow:\\n\", design_prompt)\n",
        "print(\"Simulated Model Response (for design):\\n\", query_gemini(design_prompt))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# --- 3. Development Phase ---\n",
        "# Prompting is directly used here to build the language model interactions.\n",
        "# Examples of various prompting techniques would fall under this phase.\n",
        "\n",
        "# Example: Instruction Prompting for a specific function within the application\n",
        "print(\"# 3. Development Phase: Instruction Prompting for a Function\")\n",
        "instruction_dev_prompt = \"Create a Python function that takes a product name and generates a short, catchy marketing slogan for it.\"\n",
        "print(\"Prompt for generating code/logic:\\n\", instruction_dev_prompt)\n",
        "print(\"Simulated Model Response (code/logic idea):\\n\", query_gemini(instruction_dev_prompt, max_output_tokens=300))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# Example: Few-shot Prompting for consistent output format in a feature\n",
        "print(\"# 3. Development Phase: Few-shot Prompting for Formatting\")\n",
        "few_shot_dev_prompt = \"\"\"\n",
        "We need to extract key features from product descriptions.\n",
        "Here's an example of the desired format:\n",
        "Description: \"This phone has a 6.5-inch display and a 4000mAh battery.\"\n",
        "Features: {\"display\": \"6.5-inch\", \"battery\": \"4000mAh\"}\n",
        "\n",
        "Now extract features from this description:\n",
        "Description: \"The camera has a 20MP sensor and optical image stabilization.\"\n",
        "Features:\n",
        "\"\"\"\n",
        "print(\"Prompt for ensuring consistent output format:\\n\", few_shot_dev_prompt)\n",
        "print(\"Simulated Model Response (formatted output):\\n\", query_gemini(few_shot_dev_prompt))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# --- 4. Testing Phase ---\n",
        "# Prompting is crucial for generating test cases and evaluating model performance.\n",
        "print(\"# 4. Testing Phase: Generating Test Cases\")\n",
        "testing_prompt = \"\"\"\n",
        "Generate a set of challenging test cases for a sentiment analysis model.\n",
        "Include examples that are ambiguous, use sarcasm, or have mixed sentiments.\n",
        "For each test case, provide the text and the expected sentiment label (Positive, Negative, Neutral).\n",
        "\"\"\"\n",
        "print(\"Prompt for generating test cases:\\n\", testing_prompt)\n",
        "print(\"Simulated Model Response (test case ideas):\\n\", query_gemini(testing_prompt, max_output_tokens=300))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# --- 5. Deployment Phase ---\n",
        "# While less direct prompting of the *model* itself, prompt engineering principles\n",
        "# are applied when configuring the deployed model and its parameters.\n",
        "# For instance, setting appropriate `temperature` and `max_tokens`.\n",
        "\n",
        "# --- 6. Maintenance Phase ---\n",
        "# Prompting is used for monitoring, debugging, and improving the model's responses.\n",
        "print(\"# 6. Maintenance Phase: Debugging and Improvement\")\n",
        "maintenance_prompt = \"\"\"\n",
        "A user reported that our chatbot gave an incorrect answer about the capital of Australia.\n",
        "The chatbot responded with 'Sydney'.\n",
        "How can we modify the prompt or model configuration to prevent this error in the future?\n",
        "Consider adding specific instructions or context.\n",
        "\"\"\"\n",
        "print(\"Prompt for analyzing errors and suggesting fixes:\\n\", maintenance_prompt)\n",
        "print(\"Simulated Model Response (suggestions for improvement):\\n\", query_gemini(maintenance_prompt, max_output_tokens=300))\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "print(\"End of SDLC-related prompting examples.\")"
      ],
      "metadata": {
        "id": "qilJPXkXy7jE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}