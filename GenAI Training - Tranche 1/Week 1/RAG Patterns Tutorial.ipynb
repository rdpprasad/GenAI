{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cq9BgFrqO_LaIMh9Wsx4jVyybet5f6H_","timestamp":1749175108165}],"authorship_tag":"ABX9TyOeo4M50wLkD5MHEFzrRWQc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Prepared by Tamal Acharya"],"metadata":{"id":"0ytuZHSE8K9K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tobh-yt7slS"},"outputs":[],"source":["# This tutorial demonstrates various Retrieval Augmented Generation (RAG) patterns\n","# using Python and open-source libraries.\n","\n","# Ensure necessary libraries are installed\n","!pip install -q langchain-community langchain langchain-core chromadb transformers torch datasets\n","\n","import os\n","from langchain_community.document_loaders import TextLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_community.llms import HuggingFacePipeline\n","from langchain.chains import RetrievalQA\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","\n","# --- Setup ---\n","\n","# 1. Create a dummy text file for demonstration\n","dummy_text = \"\"\"\n","Retrieval Augmented Generation (RAG) is a technique that enhances\n","the ability of language models to generate more accurate and informative\n","responses. It combines the power of pre-trained language models with\n","external knowledge retrieval systems.\n","\n","The core idea is to first retrieve relevant documents or information chunks\n","from a knowledge base based on the user's query. These retrieved pieces of\n","information are then used as context for the language model to generate a\n","response. This approach helps mitigate the limitations of large language models,\n","such as hallucination and outdated information, by grounding the generation in\n","real-world data.\n","\n","RAG has several benefits. It can provide more accurate and factual answers,\n","reduce hallucinations, and allow the model to access and utilize information\n","that was not present in its training data. It is particularly useful for\n","tasks requiring up-to-date information or domain-specific knowledge.\n","\n","Common RAG patterns include Naive RAG, Advanced RAG (like Recursive Retrieval or\n","HyDE), and Agentic RAG. Each pattern offers different strategies for retrieval\n","and integration with the language model.\n","\"\"\"\n","\n","with open(\"rag_info.txt\", \"w\") as f:\n","    f.write(dummy_text)\n","\n","# 2. Load the document\n","loader = TextLoader(\"rag_info.txt\")\n","documents = loader.load()\n","\n","# 3. Split the document into chunks\n","text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n","texts = text_splitter.split_documents(documents)\n","\n","# 4. Initialize the Embedding Model\n","# We'll use a common sentence transformer model\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# 5. Initialize the Vector Store (ChromaDB in this case)\n","# This stores the embedded chunks and allows for efficient similarity search\n","db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n","\n","# 6. Initialize the Language Model (using HuggingFace transformers pipeline)\n","# We'll use a small causal language model for demonstration purposes\n","model_name = \"distilbert/distilgpt2\" # A smaller model for faster execution\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Create a text generation pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    device=0 if torch.cuda.is_available() else -1, # Use GPU if available\n","    max_new_tokens=100, # Limit generated tokens for faster results\n","    do_sample=True,\n","    top_k=50,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","# --- RAG Pattern 1: Naive RAG ---\n","# The most basic pattern: retrieve top-k documents and pass them directly\n","# as context to the language model.\n","\n","print(\"\\n--- Demonstrating Naive RAG ---\")\n","\n","# Create a retriever from the vector store\n","retriever = db.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 chunks\n","\n","# Define the prompt template for RAG\n","# The prompt includes a placeholder for the retrieved context\n","template = \"\"\"Use the following pieces of context to answer the question.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use a maximum of three sentences.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Helpful Answer:\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# Create the RAG chain\n","# This chain combines retrieval, prompting, and language model inference\n","rag_chain = (\n","    {\"context\": retriever, \"question\": lambda x: x[\"question\"]}\n","    | prompt\n","    | llm\n",")\n","\n","# Ask a question\n","question = \"What is Retrieval Augmented Generation?\"\n","print(f\"\\nQuestion: {question}\")\n","\n","# Invoke the RAG chain\n","response = rag_chain.invoke({\"question\": question})\n","\n","# The HuggingFace pipeline often includes the prompt in the output,\n","# we need to extract the generated text.\n","# This part might need adjustment based on the specific LLM and pipeline output format.\n","# A common pattern is to split by the 'Helpful Answer:' part.\n","generated_text = response.split(\"Helpful Answer:\")[-1].strip()\n","print(f\"Generated Answer: {generated_text}\")\n","\n","# --- RAG Pattern 2: Hypothetical Document Embeddings (HyDE) ---\n","# An Advanced RAG pattern. Instead of embedding the query directly,\n","# HyDE first generates a hypothetical answer to the query. Then, it\n","# embeds this hypothetical answer and uses it for retrieval.\n","\n","print(\"\\n--- Demonstrating HyDE RAG ---\")\n","\n","# Create a simpler LLM chain to generate the hypothetical answer\n","hyde_prompt_template = \"\"\"Please write a concise, hypothetical answer to the following question:\n","\n","Question: {question}\n","\n","Hypothetical Answer:\"\"\"\n","hyde_prompt = ChatPromptTemplate.from_template(hyde_prompt_template)\n","hyde_chain = hyde_prompt | llm\n","\n","# Define a custom retriever that uses HyDE\n","class HydeRetriever:\n","    def __init__(self, llm_chain, vectorstore, k=4):\n","        self.llm_chain = llm_chain\n","        self.vectorstore = vectorstore\n","        self.k = k\n","\n","    def get_relevant_documents(self, query):\n","        # Generate hypothetical answer\n","        hypothetical_answer = self.llm_chain.invoke({\"question\": query})\n","        # Extract the generated text (similar to Naive RAG)\n","        # This might need adjustment based on the specific LLM output\n","        hypothetical_answer = hypothetical_answer.split(\"Hypothetical Answer:\")[-1].strip()\n","        print(f\"\\nGenerated Hypothetical Answer for HyDE: {hypothetical_answer}\")\n","\n","        # Retrieve documents based on the hypothetical answer\n","        retrieved_docs = self.vectorstore.similarity_search(hypothetical_answer, k=self.k)\n","        return retrieved_docs\n","\n","# Initialize the HyDE retriever\n","hyde_retriever = HydeRetriever(hyde_chain, db, k=2)\n","\n","# Create the RAG chain using the HyDE retriever\n","hyde_rag_chain = (\n","    {\"context\": hyde_retriever.get_relevant_documents, \"question\": lambda x: x[\"question\"]}\n","    | prompt # Use the same answer generation prompt as Naive RAG\n","    | llm\n",")\n","\n","# Ask the same question\n","question_hyde = \"What are the benefits of RAG?\"\n","print(f\"\\nQuestion: {question_hyde}\")\n","\n","# Invoke the HyDE RAG chain\n","response_hyde = hyde_rag_chain.invoke({\"question\": question_hyde})\n","\n","# Extract the generated text\n","generated_text_hyde = response_hyde.split(\"Helpful Answer:\")[-1].strip()\n","print(f\"Generated Answer (HyDE): {generated_text_hyde}\")\n","\n","# --- RAG Pattern 3: Agentic RAG (Conceptual) ---\n","# Agentic RAG involves an AI agent that dynamically decides whether to retrieve,\n","# which retrieval strategy to use, and how to integrate the retrieved information.\n","# Implementing a full agent is complex and often requires more sophisticated\n","# orchestrator frameworks (like LangGraph). This section provides a conceptual\n","# outline and a simplified example using Langchain's agent capabilities.\n","\n","print(\"\\n--- Conceptual Demonstrating Agentic RAG ---\")\n","\n","# For a full Agentic RAG implementation, you would typically:\n","# 1. Define tools the agent can use (e.g., a retrieval tool, a search engine tool, a calculator).\n","# 2. Define the agent's personality/instructions.\n","# 3. Use an agent executor framework (like Langchain's Agent Executor).\n","\n","# Simplified conceptual example using a Langchain Agent (requires additional tools)\n","# !pip install -q wikipedia # Example tool if you want to try\n","# from langchain.agents import initialize_agent, AgentType, Tool\n","\n","# We won't run this fully without more setup and tools, but here's the idea:\n","# agent_tools = [\n","#     Tool(\n","#         name=\"Vector Store Retriever\",\n","#         func=db.as_retriever().get_relevant_documents,\n","#         description=\"Useful for retrieving information about RAG from the local knowledge base.\"\n","#     ),\n","#     # Add other tools like a general search tool, calculator, etc.\n","# ]\n","\n","# agent = initialize_agent(\n","#     agent_tools,\n","#     llm,\n","#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","#     verbose=True # Set to True to see the agent's thought process\n","# )\n","\n","# print(\"\\nQuestion for Agentic RAG (Conceptual): What are common RAG patterns?\")\n","# try:\n","#     agent_response = agent.run(\"What are common RAG patterns?\")\n","#     print(f\"Agent's Answer: {agent_response}\")\n","# except Exception as e:\n","#     print(f\"Could not run Agentic RAG example without full tool setup: {e}\")\n","#     print(\"Agentic RAG involves a more complex orchestration layer where an agent decides how to answer the query, potentially using multiple tools including retrieval.\")\n","\n","\n","print(\"\\n--- Tutorial Complete ---\")\n","print(\"This tutorial covered Naive RAG and a conceptual HyDE RAG implementation.\")\n","print(\"Agentic RAG is a more advanced pattern often requiring AI agent frameworks.\")\n","\n","# --- Clean up ---\n","# Remove the dummy file and chroma db directory\n","import shutil\n","os.remove(\"rag_info.txt\")\n","if os.path.exists(\"./chroma_db\"):\n","    shutil.rmtree(\"./chroma_db\")\n","print(\"\\nCleaned up generated files and directories.\")\n","\n"]},{"cell_type":"code","source":["# --- RAG Pattern 4: Parent Document Retriever ---\n","# An Advanced RAG pattern. Instead of retrieving small chunks directly,\n","# this retriever retrieves larger \"parent\" documents based on the query's\n","# similarity to smaller \"child\" chunks. The language model then receives\n","# the full parent document as context, which can provide better context\n","# and coherence.\n","\n","print(\"\\n--- Demonstrating Parent Document Retriever RAG ---\")\n","\n","from langchain.storage import InMemoryStore\n","from langchain.retrievers import ParentDocumentRetriever\n","\n","# We need to create smaller \"child\" chunks for embedding and retrieval\n","child_text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n","child_texts = child_text_splitter.split_documents(documents)\n","\n","# We need a document store to hold the parent documents\n","store = InMemoryStore()\n","\n","# Initialize the Parent Document Retriever\n","parent_document_retriever = ParentDocumentRetriever(\n","    vectorstore=db,       # The vector store for searching child chunks\n","    docstore=store,       # The store for retrieving parent documents\n","    child_splitter=child_text_splitter, # The splitter used for creating child chunks\n","    parent_splitter=text_splitter, # The splitter used for creating parent documents (same as initial splitter)\n",")\n","\n","# Add the documents to the retriever's stores\n","# This processes the documents, splits them into children, embeds children,\n","# stores children in the vectorstore, and stores parents in the docstore.\n","parent_document_retriever.add_documents(documents)\n","\n","# Create the RAG chain using the Parent Document Retriever\n","parent_rag_chain = (\n","    {\"context\": parent_document_retriever.get_relevant_documents, \"question\": lambda x: x[\"question\"]}\n","    | prompt # Use the same answer generation prompt\n","    | llm\n",")\n","\n","# Ask a question\n","question_parent = \"What is the core idea behind RAG?\"\n","print(f\"\\nQuestion: {question_parent}\")\n","\n","# Invoke the Parent Document Retriever RAG chain\n","response_parent = parent_rag_chain.invoke({\"question\": question_parent})\n","\n","# Extract the generated text\n","generated_text_parent = response_parent.split(\"Helpful Answer:\")[-1].strip()\n","print(f\"Generated Answer (Parent Document Retriever): {generated_text_parent}\")\n","\n","# --- RAG Pattern 5: Ensemble Retriever (Conceptual) ---\n","# An Advanced RAG pattern that combines the results of multiple different\n","# retrieval methods (e.g., vector search, keyword search, graph search).\n","# The results are then re-ranked or merged to get the final set of documents.\n","# This is typically done using a re-ranker or a fusion algorithm (like Reciprocal Rank Fusion).\n","\n","print(\"\\n--- Conceptual Demonstrating Ensemble Retriever RAG ---\")\n","\n","# Implementing a full Ensemble Retriever requires setting up multiple retrievers\n","# and a re-ranker or fusion mechanism.\n","\n","# Conceptual Example:\n","# from langchain.retrievers import EnsembleRetriever\n","# from langchain.retrievers import BM25Retriever # Example of a keyword retriever\n","\n","# # Initialize a keyword retriever\n","# bm25_retriever = BM25Retriever.from_documents(texts) # Uses the same text chunks as vectorstore\n","# bm25_retriever.k = 2 # Retrieve top 2\n","\n","# # Initialize the vector store retriever\n","# vector_retriever = db.as_retriever(search_kwargs={\"k\": 2})\n","\n","# # Initialize the Ensemble Retriever (without re-ranking for simplicity)\n","# ensemble_retriever = EnsembleRetriever(retrievers=[bm22_retriever, vector_retriever], weights=[0.5, 0.5])\n","\n","# # Create the RAG chain using the Ensemble Retriever\n","# ensemble_rag_chain = (\n","#     {\"context\": ensemble_retriever.get_relevant_documents, \"question\": lambda x: x[\"question\"]}\n","#     | prompt\n","#     | llm\n","# )\n","\n","# print(\"\\nQuestion for Ensemble RAG (Conceptual): What are the benefits of RAG?\")\n","# try:\n","#     ensemble_response = ensemble_rag_chain.invoke({\"question\": \"What are the benefits of RAG?\"})\n","#     generated_text_ensemble = ensemble_response.split(\"Helpful Answer:\")[-1].strip()\n","#     print(f\"Generated Answer (Ensemble): {generated_text_ensemble}\")\n","# except Exception as e:\n","#     print(f\"Could not run Ensemble RAG example without full setup: {e}\")\n","#     print(\"Ensemble RAG combines different retrieval methods to potentially improve recall.\")\n","\n","\n","print(\"\\n--- Tutorial Complete ---\")\n","print(\"This tutorial covered Naive RAG, a conceptual HyDE RAG, Parent Document Retriever RAG, and conceptual Ensemble RAG.\")\n","print(\"Agentic RAG and full Ensemble RAG implementations are more advanced patterns often requiring additional components or frameworks.\")\n","\n","# --- Clean up ---\n","# Remove the dummy file and chroma db directory\n","os.remove(\"rag_info.txt\")\n","if os.path.exists(\"./chroma_db\"):\n","    shutil.rmtree(\"./chroma_db\")\n","print(\"\\nCleaned up generated files and directories.\")\n"],"metadata":{"id":"gop-vgT38M6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the context of RAG (Retrieval-Augmented Generation) systems, “RAG patterns” typically refer to the architectural or design patterns used to implement RAG pipelines. These patterns govern how the retrieval and generation steps are organized and interact with each other.\n","\n","Here are the most common RAG patterns:\n","\n","1. Retrieve-then-Read (Standard RAG)\n","Pattern:\n","\n","Retrieve documents first based on the user query.\n","\n","Feed them to a generative model (like GPT or LLaMA) to produce an answer.\n","\n","Use Case: Most common and scalable RAG implementation.\n","\n","2. Read-Retrieve-Read\n","Pattern:\n","\n","First “read” the query to rewrite or enrich it (e.g., query expansion or rephrasing).\n","\n","Then retrieve documents using the refined query.\n","\n","Finally generate an answer based on retrieved content.\n","\n","Use Case: Helps improve retrieval quality when original queries are ambiguous or under-specified.\n","\n","3. Retrieve-Rank-Read\n","Pattern:\n","\n","Retrieve a large set of candidate documents.\n","\n","Rank them using a more sophisticated scorer (e.g., BERT-based cross-encoder).\n","\n","Select top-k documents and pass to the generator.\n","\n","Use Case: When retrieval quality is critical. Adds a second step to boost precision.\n","\n","4. Multi-step RAG (Iterative Retrieval)\n","Pattern:\n","\n","Run multiple rounds of retrieval and generation.\n","\n","Each round refines the query based on the previous response.\n","\n","Use Case: Complex tasks like multi-hop QA, research agents, or chain-of-thought reasoning.\n","\n","5. Fusion-in-Decoder (FiD)\n","Pattern:\n","\n","All retrieved documents are encoded independently, then fused at the decoder level (e.g., T5-based models).\n","\n","Use Case: Used in models like FiD where the decoder integrates evidence from multiple sources simultaneously.\n","\n","6. Hybrid RAG (Dense + Sparse Retrieval)\n","Pattern:\n","\n","Combines both dense retrieval (e.g., using vector similarity) and sparse retrieval (e.g., BM25).\n","\n","Merges or re-ranks results before feeding to the generation model.\n","\n","Use Case: Improves coverage and recall of relevant documents.\n","\n","7. Agentic RAG / Tool-augmented RAG\n","Pattern:\n","\n","Uses an agent that can retrieve, reason, and plan.\n","\n","The agent might invoke multiple retrievals, tools, or even external APIs before generating a final response.\n","\n","Use Case: Complex task-solving, especially in enterprise AI assistants or copilots.\n","\n","8. Retriever-Augmented Planning\n","Pattern:\n","\n","Planner decomposes tasks into sub-tasks.\n","\n","Each sub-task may involve retrieval + generation.\n","\n","Use Case: Long-document QA, multi-turn conversations, or coding agents."],"metadata":{"id":"Pq3b-Mcf9AFo"}},{"cell_type":"code","source":["# Retrieve-then-Read (Standard RAG), Read-Retrieve-Read, Retrieve-Rank-Read,\n","# Multi-step RAG (Iterative Retrieval), Fusion-in-Decoder (FiD), Hybrid RAG (Dense + Sparse Retrieval)\n","\n","import os\n","from langchain_community.document_loaders import TextLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_community.llms import HuggingFacePipeline\n","from langchain.chains import RetrievalQA\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","import shutil\n","from langchain.storage import InMemoryStore\n","from langchain.retrievers import ParentDocumentRetriever\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.retrievers import BM25Retriever, EnsembleRetriever\n","from langchain_core.output_parsers import StrOutputParser\n","from typing import List\n","from langchain_core.documents import Document\n","\n","# This tutorial demonstrates various Retrieval Augmented Generation (RAG) patterns\n","# using Python and open-source libraries.\n","\n","# Ensure necessary libraries are installed\n","!pip install -q langchain-community langchain langchain-core chromadb transformers torch datasets rank_bm25\n","\n","# --- Setup ---\n","\n","# 1. Create a dummy text file for demonstration\n","dummy_text = \"\"\"\n","Retrieval Augmented Generation (RAG) is a technique that enhances\n","the ability of language models to generate more accurate and informative\n","responses. It combines the power of pre-trained language models with\n","external knowledge retrieval systems.\n","\n","The core idea is to first retrieve relevant documents or information chunks\n","from a knowledge base based on the user's query. These retrieved pieces of\n","information are then used as context for the language model to generate a\n","response. This approach helps mitigate the limitations of large language models,\n","such as hallucination and outdated information, by grounding the generation in\n","real-world data.\n","\n","RAG has several benefits. It can provide more accurate and factual answers,\n","reduce hallucinations, and allow the model to access and utilize information\n","that was not present in its training data. It is particularly useful for\n","tasks requiring up-to-date information or domain-specific knowledge.\n","\n","Common RAG patterns include Naive RAG, Advanced RAG (like Recursive Retrieval or\n","HyDE), and Agentic RAG. Other patterns include Retrieve-Rank-Read, Multi-step\n","or Iterative Retrieval, Fusion-in-Decoder (FiD), and Hybrid RAG combining\n","dense and sparse retrieval. Each pattern offers different strategies for retrieval\n","and integration with the language model.\n","\"\"\"\n","\n","with open(\"rag_info.txt\", \"w\") as f:\n","    f.write(dummy_text)\n","\n","# 2. Load the document\n","loader = TextLoader(\"rag_info.txt\")\n","documents = loader.load()\n","\n","# 3. Split the document into chunks\n","text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n","texts = text_splitter.split_documents(documents)\n","\n","# 4. Initialize the Embedding Model\n","# We'll use a common sentence transformer model\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# 5. Initialize the Vector Store (ChromaDB in this case)\n","# This stores the embedded chunks and allows for efficient similarity search\n","db = Chroma.from_documents(texts, embeddings, persist_directory=\"./chroma_db\")\n","\n","# 6. Initialize the Language Model (using HuggingFace transformers pipeline)\n","# We'll use a small causal language model for demonstration purposes\n","model_name = \"distilbert/distilgpt2\" # A smaller model for faster execution\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token # Needed for some models with pipeline\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Create a text generation pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    device=0 if torch.cuda.is_available() else -1, # Use GPU if available\n","    max_new_tokens=100, # Limit generated tokens for faster results\n","    do_sample=True,\n","    top_k=50,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.eos_token_id # Set pad token id for pipeline\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","# Define the standard RAG prompt template\n","template = \"\"\"Use the following pieces of context to answer the question.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use a maximum of three sentences.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\n","Helpful Answer:\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","# Helper function to extract text from pipeline response\n","def extract_generated_text(response):\n","    # This part might need adjustment based on the specific LLM and pipeline output format.\n","    # A common pattern is to split by the 'Helpful Answer:' part.\n","    # Sometimes the LLM generates extra text before the \"Helpful Answer:\",\n","    # or the prompt is included. We try to handle common cases.\n","    if \"Helpful Answer:\" in response:\n","        return response.split(\"Helpful Answer:\")[-1].strip()\n","    # If the prompt structure isn't perfectly followed, try a simpler split or just return response\n","    lines = response.strip().split('\\n')\n","    # Find the last non-empty line that doesn't look like the question/context part\n","    answer_lines = []\n","    found_answer_start = False\n","    for line in reversed(lines):\n","        line = line.strip()\n","        if line and not line.startswith(\"Context:\") and not line.startswith(\"Question:\"):\n","            answer_lines.append(line)\n","            found_answer_start = True\n","        elif found_answer_start:\n","            break # Stop if we find empty lines after the potential answer\n","    return \"\\n\".join(reversed(answer_lines)).strip()\n","\n","\n","# --- RAG Pattern 1: Retrieve-then-Read (Standard RAG) ---\n","# The most basic pattern: retrieve top-k documents and pass them directly\n","# as context to the language model.\n","\n","print(\"\\n--- Demonstrating Retrieve-then-Read (Standard RAG) ---\")\n","\n","# Create a retriever from the vector store\n","retriever = db.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 chunks\n","\n","# Create the RAG chain\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()} # Use RunnablePassthrough for question\n","    | prompt\n","    | llm\n","    | StrOutputParser() # Parse the output to string\n",")\n","\n","# Ask a question\n","question_std = \"What is Retrieval Augmented Generation?\"\n","print(f\"\\nQuestion: {question_std}\")\n","\n","# Invoke the RAG chain\n","response_std = rag_chain.invoke(question_std)\n","print(f\"Generated Answer (Standard RAG): {response_std}\")\n","\n","\n","# --- RAG Pattern 2: Read-Retrieve-Read (Query Expansion/Rewriting) ---\n","# First \"read\" the query to potentially rewrite or expand it using an LLM,\n","# then use the expanded query for retrieval. The final \"read\" is the generation step.\n","\n","print(\"\\n--- Demonstrating Read-Retrieve-Read (Query Rewriting) ---\")\n","\n","# LLM Chain to rewrite or expand the query\n","query_rewrite_prompt_template = \"\"\"You are a helpful assistant that rewrites user questions to improve information retrieval.\n","Rewrite the following question to make it more effective for searching a document database:\n","\n","Question: {question}\n","\n","Rewritten Question:\"\"\"\n","query_rewrite_prompt = ChatPromptTemplate.from_template(query_rewrite_prompt_template)\n","query_rewrite_chain = query_rewrite_prompt | llm | StrOutputParser()\n","\n","# Custom Retriever that uses the rewritten query\n","class RewritingRetriever:\n","    def __init__(self, rewrite_chain, vectorstore, k=4):\n","        self.rewrite_chain = rewrite_chain\n","        self.vectorstore = vectorstore\n","        self.k = k\n","\n","    def get_relevant_documents(self, query):\n","        # Rewrite the query\n","        rewritten_query = self.rewrite_chain.invoke({\"question\": query})\n","        # Clean up potential extra text from the LLM\n","        rewritten_query = rewritten_query.split(\"Rewritten Question:\")[-1].strip()\n","        print(f\"\\nOriginal Question: {query}\")\n","        print(f\"Rewritten Question for Retrieval: {rewritten_query}\")\n","        # Retrieve documents using the rewritten query\n","        retrieved_docs = self.vectorstore.similarity_search(rewritten_query, k=self.k)\n","        return retrieved_docs\n","\n","# Initialize the Rewriting Retriever\n","rewriting_retriever = RewritingRetriever(query_rewrite_chain, db, k=2)\n","\n","# Create the RAG chain using the Rewriting Retriever\n","read_retrieve_read_chain = (\n","    {\"context\": rewriting_retriever.get_relevant_documents, \"question\": RunnablePassthrough()}\n","    | prompt # Use the standard answer generation prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Ask a question (maybe one that could benefit from rewriting)\n","question_rewrite = \"Tell me about RAG's good points.\"\n","print(f\"\\nQuestion: {question_rewrite}\")\n","\n","# Invoke the Read-Retrieve-Read RAG chain\n","response_rewrite = read_retrieve_read_chain.invoke(question_rewrite)\n","print(f\"Generated Answer (Read-Retrieve-Read): {response_rewrite}\")\n","\n","\n","# --- RAG Pattern 3: Retrieve-Rank-Read ---\n","# Retrieve a larger set of documents, then re-rank them to select the most relevant\n","# before passing to the language model. This requires a separate re-ranking step.\n","# For simplicity, we'll simulate ranking by just using a combination of retrievers.\n","# A proper implementation would use a cross-encoder re-ranker.\n","\n","print(\"\\n--- Demonstrating Retrieve-Rank-Read (Conceptual) ---\")\n","\n","# This requires a re-ranker model, which we won't set up fully here.\n","# A conceptual idea often involves retrieving more documents initially\n","# and potentially using a different method or a dedicated re-ranking model\n","# to select the final set.\n","\n","# Conceptual simulation: Retrieve more docs initially\n","retriever_wide = db.as_retriever(search_kwargs={\"k\": 4}) # Retrieve top 4 chunks\n","\n","# In a real Retrieve-Rank-Read, you'd insert a re-ranking step here\n","# For example:\n","# from langchain.retrievers import ContextualCompressionRetriever\n","# from langchain.retrievers.document_compressors import CrossEncoderReranker\n","# from transformers import CrossEncoder\n","#\n","# reranker_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-2-v2\") # Example re-ranker\n","# compressor = CrossEncoderReranker(model=reranker_model, top_n=2) # Rank and select top 2\n","# compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever_wide)\n","\n","# For this demo, we'll just use the wider retrieval as a proxy.\n","retrieve_rank_read_retriever = retriever_wide # Using the wider retriever\n","\n","# Create the RAG chain using the (conceptually ranked) wider retrieval\n","retrieve_rank_read_chain = (\n","    {\"context\": retrieve_rank_read_retriever, \"question\": RunnablePassthrough()}\n","    | prompt # Use the standard answer generation prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Ask a question\n","question_rank = \"What are the various RAG patterns mentioned?\"\n","print(f\"\\nQuestion: {question_rank}\")\n","\n","# Invoke the Retrieve-Rank-Read RAG chain (conceptual)\n","response_rank = retrieve_rank_read_chain.invoke(question_rank)\n","print(f\"Generated Answer (Retrieve-Rank-Read Conceptual): {response_rank}\")\n","\n","\n","# --- RAG Pattern 4: Multi-step RAG (Iterative Retrieval) ---\n","# An Advanced RAG pattern where the model can perform multiple retrieval steps\n","# to refine its understanding or gather more information needed to answer a query.\n","# This often involves an agent or a carefully designed chain that uses the output\n","# of a previous step to inform the next retrieval.\n","\n","print(\"\\n--- Demonstrating Multi-step RAG (Iterative Retrieval - Conceptual) ---\")\n","\n","# Implementing true multi-step RAG is complex and typically involves agents or\n","# custom looping logic where the LLM decides if it needs more information\n","# and formulates a new query based on the current context and the previous output.\n","\n","# Conceptual idea: A simple chain that first gets initial docs, then\n","# potentially asks a clarifying question or performs a second retrieval.\n","# This is difficult to implement generatively with a basic pipeline setup.\n","\n","# For demonstration, we can simulate a simple two-step process:\n","# 1. Initial Retrieval and Generation attempt.\n","# 2. (Conceptual): If the first answer is insufficient (e.g., short, says it doesn't know),\n","#    reformulate the question or retrieve differently.\n","\n","# A simplified iterative approach might look like this (requires more LLM guidance):\n","# Initial response chain (same as standard RAG)\n","# first_step_chain = (\n","#     {\"context\": db.as_retriever(search_kwargs={\"k\": 1}), \"question\": RunnablePassthrough()}\n","#     | prompt\n","#     | llm\n","#     | StrOutputParser()\n","# )\n","#\n","# print(\"\\nInitial attempt at Multi-step RAG:\")\n","# initial_response = first_step_chain.invoke(\"Describe the limitations of RAG.\")\n","# print(f\"First step response: {initial_response}\")\n","#\n","# # Now, conceptually, you'd analyze `initial_response`. If it's not good,\n","# # you might use an LLM to decide to retrieve more or ask a follow-up.\n","# # Example: If initial_response is \"I don't know\", trigger a different retrieval or query.\n","#\n","# # This loop/conditional logic is the core of iterative RAG.\n","# # It requires a more sophisticated control flow than simple chain sequencing.\n","# # LangGraph or similar frameworks are well-suited for this.\n","\n","print(\"Multi-step RAG typically requires agents or complex state management.\")\n","print(\"Demonstration here is conceptual and does not fully implement iterative retrieval.\")\n","\n","# --- RAG Pattern 5: Fusion-in-Decoder (FiD) ---\n","# This is an architecture pattern rather than a pipeline pattern. It uses models\n","# specifically designed to handle multiple input documents by encoding them\n","# independently and then fusing this information in the decoder.\n","# This requires a FiD-architecture model (like T5-based FiD).\n","\n","print(\"\\n--- Demonstrating Fusion-in-Decoder (FiD) ---\")\n","\n","# Implementing FiD requires using a model with the FiD architecture,\n","# and preparing the input data by concatenating the prompt and multiple\n","# document contexts in a specific format the model expects.\n","# We are using a simple causal LLM (distilgpt2), which is not a FiD model.\n","\n","# To implement FiD, you would need to:\n","# 1. Load a FiD model (e.g., using `transformers.AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-fid\")`).\n","# 2. Prepare the input by formatting the query and each retrieved document.\n","# 3. Pass this specially formatted input to the FiD model.\n","\n","print(\"Demonstration requires a FiD model, which is not used in this setup (using distilgpt2).\")\n","print(\"FiD integrates multiple documents at the model's decoder level.\")\n","\n","\n","# --- RAG Pattern 6: Hybrid RAG (Dense + Sparse Retrieval) ---\n","# Combines different retrieval methods, typically dense (vector search) and\n","# sparse (keyword search like BM25). The results are often merged or re-ranked.\n","\n","print(\"\\n--- Demonstrating Hybrid RAG (Dense + Sparse Retrieval) ---\")\n","\n","# Initialize a dense retriever (from our vector store)\n","dense_retriever = db.as_retriever(search_kwargs={\"k\": 2})\n","\n","# Initialize a sparse retriever (BM25)\n","bm25_retriever = BM25Retriever.from_documents(texts)\n","bm25_retriever.k = 2\n","\n","# Initialize an Ensemble Retriever to combine the results\n","# Weights determine the contribution of each retriever. Reciprocal Rank Fusion\n","# is a common method for merging results from different retrievers.\n","ensemble_retriever = EnsembleRetriever(retrievers=[dense_retriever, bm25_retriever], weights=[0.5, 0.5])\n","\n","# Create the RAG chain using the Ensemble Retriever\n","hybrid_rag_chain = (\n","    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n","    | prompt # Use the standard answer generation prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Ask a question\n","question_hybrid = \"What is RAG and why is it beneficial?\"\n","print(f\"\\nQuestion: {question_hybrid}\")\n","\n","# Invoke the Hybrid RAG chain\n","response_hybrid = hybrid_rag_chain.invoke(question_hybrid)\n","print(f\"Generated Answer (Hybrid RAG): {response_hybrid}\")\n","\n","\n","# --- RAG Pattern 4 (Revisited): Parent Document Retriever ---\n","# (Moved down to group with Advanced RAG patterns)\n","# An Advanced RAG pattern. Instead of retrieving small chunks directly,\n","# this retriever retrieves larger \"parent\" documents based on the query's\n","# similarity to smaller \"child\" chunks. The language model then receives\n","# the full parent document as context, which can provide better context\n","# and coherence.\n","\n","print(\"\\n--- Demonstrating Parent Document Retriever RAG ---\")\n","\n","# We need to create smaller \"child\" chunks for embedding and retrieval\n","child_text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n","child_texts = child_text_splitter.split_documents(documents) # Split the *original* documents\n","\n","# We need a document store to hold the parent documents\n","# Clear previous store if it exists\n","store = InMemoryStore()\n","\n","# Initialize the Parent Document Retriever\n","parent_document_retriever = ParentDocumentRetriever(\n","    vectorstore=db,       # The vector store for searching child chunks (uses `db` which contains embeddings of `texts`)\n","    docstore=store,       # The store for retrieving parent documents\n","    child_splitter=child_text_splitter, # The splitter used for creating child chunks\n","    parent_splitter=text_splitter, # The splitter used for creating parent documents (same as initial splitter)\n",")\n","\n","# Add the documents to the retriever's stores\n","# This processes the *original* documents, splits them into children, embeds children,\n","# stores children in the vectorstore (implicitly adds via `vectorstore=db`),\n","# and stores parents in the docstore.\n","parent_document_retriever.add_documents(documents) # Add original documents\n","\n","# Create the RAG chain using the Parent Document Retriever\n","parent_rag_chain = (\n","    {\"context\": parent_document_retriever, \"question\": RunnablePassthrough()}\n","    | prompt # Use the same answer generation prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Ask a question\n","question_parent = \"What is the core idea behind RAG?\"\n","print(f\"\\nQuestion: {question_parent}\")\n","\n","# Invoke the Parent Document Retriever RAG chain\n","response_parent = parent_rag_chain.invoke(question_parent)\n","print(f\"Generated Answer (Parent Document Retriever): {response_parent}\")\n","\n","# --- RAG Pattern 7 & 8: Agentic RAG / Retriever-Augmented Planning ---\n","# These patterns involve more complex orchestration, often requiring AI agents\n","# that can decide when and how to use retrieval, potentially in multiple steps\n","# or as part of a planning process. This is beyond the scope of a basic sequential\n","# chain setup.\n","\n","print(\"\\n--- Conceptual Demonstrating Agentic RAG / Retriever-Augmented Planning ---\")\n","print(\"These patterns involve complex AI agents and planning frameworks (e.g., LangGraph).\")\n","print(\"Implementing them requires setting up tools for the agent and defining its decision-making process.\")\n","print(\"They are used for complex tasks, multi-turn interactions, and dynamic use of tools including retrieval.\")\n","\n","print(\"\\n--- Tutorial Complete ---\")\n","print(\"This tutorial demonstrated implementations of Retrieve-then-Read, Read-Retrieve-Read (Query Rewriting),\")\n","print(\"Hybrid RAG, and Parent Document Retriever RAG.\")\n","print(\"Conceptual outlines were provided for Retrieve-Rank-Read, Multi-step/Iterative RAG, Fusion-in-Decoder,\")\n","print(\"and Agentic/Planning RAG due to their increased complexity or specific model requirements.\")\n","\n","\n","# --- Clean up ---\n","# Remove the dummy file and chroma db directory\n","os.remove(\"rag_info.txt\")\n","if os.path.exists(\"./chroma_db\"):\n","    shutil.rmtree(\"./chroma_db\")\n","print(\"\\nCleaned up generated files and directories.\")"],"metadata":{"id":"2NlFSZyG8uyk"},"execution_count":null,"outputs":[]}]}