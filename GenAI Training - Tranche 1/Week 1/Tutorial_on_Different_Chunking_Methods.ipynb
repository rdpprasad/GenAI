{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Prepared by Tamal Acharya"
      ],
      "metadata": {
        "id": "_R3pmIVs2Hdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l5eBUrtV9VO"
      },
      "outputs": [],
      "source": [
        "# Implementation on different chunking methods like naive chunking, recursive character text splitter,\n",
        "# embedding chunking, agentic chunking, overlap chunking.\n",
        "\n",
        "!pip install langchain-text-splitters langchain-community langchain langchain-chroma\n",
        "\n",
        "import os\n",
        "from langchain.text_splitter import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TextSplitter,\n",
        ")\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Create a dummy text file\n",
        "with open(\"example_text.txt\", \"w\") as f:\n",
        "    f.write(\"This is the first sentence of a longer document. \"\n",
        "            \"It covers various topics including technology and science. \"\n",
        "            \"Here is the third sentence, introducing a new idea. \"\n",
        "            \"Followed by a fourth sentence that expands on the previous one. \"\n",
        "            \"Sentence five continues the discussion. \"\n",
        "            \"And finally, the sixth and last sentence concludes this paragraph.\")\n",
        "\n",
        "# --- Naive Chunking ---\n",
        "print(\"--- Naive Chunking ---\")\n",
        "# This is a very basic approach, splitting by a simple character or fixed length\n",
        "# without much intelligence. Using CharacterTextSplitter for this.\n",
        "with open(\"example_text.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "naive_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\", # Split by newlines (if any), or just one big chunk\n",
        "    chunk_size=500, # Define a large chunk size to effectively get one chunk\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "naive_chunks = naive_splitter.create_documents([text])\n",
        "print(f\"Number of naive chunks: {len(naive_chunks)}\")\n",
        "for i, chunk in enumerate(naive_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk.page_content[:100]}...\") # Print first 100 chars\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Recursive Character Text Splitter ---\n",
        "print(\"--- Recursive Character Text Splitter ---\")\n",
        "# Splits recursively based on a list of characters. It tries to split by the\n",
        "# first character, then the second if the first doesn't work, and so on.\n",
        "# Useful for preserving structural integrity.\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Load the document using a loader (more standard practice)\n",
        "loader = TextLoader(\"example_text.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "recursive_chunks = recursive_splitter.split_documents(documents)\n",
        "print(f\"Number of recursive chunks: {len(recursive_chunks)}\")\n",
        "for i, chunk in enumerate(recursive_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk.page_content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Overlap Chunking ---\n",
        "print(\"--- Overlap Chunking ---\")\n",
        "# Overlap chunking is not a separate *method* but a *parameter* used in most\n",
        "# text splitters (like RecursiveCharacterTextSplitter). It ensures that\n",
        "# chunks share some common text at the boundaries. This helps in RAG by\n",
        "# providing context around the split points. The recursive_splitter already\n",
        "# demonstrates overlap (chunk_overlap=20).\n",
        "print(\"Overlap is demonstrated in the Recursive Character Text Splitter example above.\")\n",
        "print(\"The 'chunk_overlap' parameter specifies the number of characters to overlap.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Embedding Chunking (Conceptual / RAG Integration) ---\n",
        "print(\"--- Embedding Chunking (Conceptual / RAG Integration) ---\")\n",
        "# Embedding chunking isn't a direct text splitting algorithm itself.\n",
        "# It's the *process* of taking text chunks (generated by a splitter) and\n",
        "# converting them into numerical vector embeddings. These embeddings are then\n",
        "# used for semantic search in a Vector Store as part of a RAG pipeline.\n",
        "# We will demonstrate this by creating embeddings and a vector store from our chunks.\n",
        "\n",
        "# Ensure Ollama is running locally or accessible\n",
        "# You might need to install Ollama and pull a model like 'nomic-embed-text'\n",
        "# !ollama pull nomic-embed-text # Example command if using local Ollama\n",
        "\n",
        "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\") # Use an appropriate embedding model\n",
        "\n",
        "# We'll use the chunks generated by the recursive splitter\n",
        "vectorstore = Chroma.from_documents(recursive_chunks, embedding_model)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(\"Embeddings created and stored in Chroma vectorstore.\")\n",
        "print(\"This vectorstore can now be used for semantic search.\")\n",
        "\n",
        "# Example semantic search (simulating the RAG retrieval step)\n",
        "query = \"What is the first sentence about?\"\n",
        "print(f\"\\nSearching for documents relevant to: '{query}'\")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "print(\"Retrieved documents:\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i+1}: {doc.page_content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Agentic Chunking (Conceptual) ---\n",
        "print(\"--- Agentic Chunking (Conceptual) ---\")\n",
        "# Agentic chunking is a more advanced and evolving concept. It involves using\n",
        "# an AI agent or a set of rules to decide *how* to split text, potentially\n",
        "# considering the semantic meaning, topic changes, or future use cases.\n",
        "# This is not a standard, readily available text splitter in libraries like\n",
        "# LangChain yet, but rather an architectural pattern where an agent\n",
        "# intelligently processes and segments data.\n",
        "\n",
        "print(\"Agentic chunking is an advanced concept where an AI agent dynamically\")\n",
        "print(\"determines how to split text based on semantic understanding or task.\")\n",
        "print(\"This is typically implemented through more complex processing pipelines\")\n",
        "print(\"rather than a single text splitter class.\")\n",
        "print(\"Example scenario: An agent identifies key sections in a document and splits\")\n",
        "print(\"accordingly, or it uses context to decide where to split for optimal retrieval.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Putting it together in a RAG Pipeline ---\n",
        "print(\"--- RAG Pipeline Example ---\")\n",
        "# A simple RAG chain using the vector store created from recursive chunks.\n",
        "\n",
        "# Ensure Ollama is running locally or accessible\n",
        "# You might need to install Ollama and pull a model like 'llama2' or 'mistral'\n",
        "# !ollama pull mistral # Example command if using local Ollama\n",
        "\n",
        "llm = ChatOllama(model=\"mistral\") # Use an appropriate LLM\n",
        "\n",
        "prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use a maximum of three sentences and keep the answer concise.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Ask a question using the RAG pipeline\n",
        "question_rag = \"What topics are covered in the document?\"\n",
        "print(f\"Asking RAG pipeline: '{question_rag}'\")\n",
        "response = rag_chain.invoke(question_rag)\n",
        "print(\"RAG Answer:\")\n",
        "print(response)\n",
        "\n",
        "# Clean up the dummy file\n",
        "os.remove(\"example_text.txt\")\n",
        "print(\"\\nCleaned up example_text.txt\")\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "1.  **Installation:** We install the necessary LangChain libraries.\n",
        "2.  **Dummy Data:** A simple text file `example_text.txt` is created to serve as our input document.\n",
        "3.  **Naive Chunking:** We use `CharacterTextSplitter` with a large `chunk_size` and simple separator (`\\n`) to simulate a naive split. This often results in large chunks or chunks broken at arbitrary points if no specific separator exists.\n",
        "4.  **Recursive Character Text Splitter:** This is a more robust method. It attempts to split by a list of separators (`\\n\\n`, `\\n`, ` `, `\"\"`) in order. `chunk_size` defines the target size, and `chunk_overlap` specifies how many characters should overlap between consecutive chunks. This is crucial for RAG to maintain context.\n",
        "5.  **Overlap Chunking:** Explained as a *parameter* (`chunk_overlap`) used within splitters like the recursive one, rather than a separate method.\n",
        "6.  **Embedding Chunking (Conceptual/RAG Integration):** This section explains that 'embedding chunking' refers to the step of taking text chunks and converting them into numerical vectors (embeddings) for use in a vector store. We demonstrate this by creating `OllamaEmbeddings` and using them to build a `Chroma` vector store from the recursive chunks. This vector store is the heart of the RAG retrieval mechanism.\n",
        "7.  **Agentic Chunking (Conceptual):** Describes this as an advanced pattern where an AI agent intelligently decides the splitting strategy, contrasting it with standard fixed-rule splitters. It's presented as a concept rather than a specific code implementation within standard libraries.\n",
        "8.  **RAG Pipeline Example:** We build a simple RAG chain:\n",
        "    *   We use the `retriever` created from our `Chroma` vector store.\n",
        "    *   `RunnablePassthrough()` passes the user's question to both the retriever and the prompt.\n",
        "    *   A `ChatPromptTemplate` is defined to instruct the LLM.\n",
        "    *   An `Ollama` LLM is used (you need to have Ollama running with a model like `mistral`).\n",
        "    *   `StrOutputParser()` converts the LLM's output to a string.\n",
        "    *   The chain is invoked with a question, demonstrating how retrieval (finding relevant chunks via embeddings) and generation (LLM answering based on retrieved context) work together.\n",
        "\n",
        "This tutorial provides a practical demonstration of common chunking techniques and conceptual explanations for more advanced ones, integrating them into a basic RAG pipeline using LangChain. Remember to replace placeholder values like `[your Cloud Platform project ID]` if you are integrating with Google Cloud Storage, although that part of the previous conversation was not directly used in the core chunking/RAG example."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed window chunking, fixed window with overlap chunking, Semantic chunking, Embedding chunking, Agentic chunking\n",
        "\n",
        "# --- Fixed Window Chunking ---\n",
        "print(\"--- Fixed Window Chunking ---\")\n",
        "# Simple fixed-size chunking without much regard for content boundaries.\n",
        "fixed_window_splitter = CharacterTextSplitter(\n",
        "    separator=\"\", # No specific separator, just split by size\n",
        "    chunk_size=50, # Define a fixed chunk size\n",
        "    chunk_overlap=0, # No overlap\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "fixed_window_chunks = fixed_window_splitter.create_documents([text])\n",
        "print(f\"Number of fixed window chunks: {len(fixed_window_chunks)}\")\n",
        "for i, chunk in enumerate(fixed_window_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk.page_content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Fixed Window with Overlap Chunking ---\n",
        "print(\"--- Fixed Window with Overlap Chunking ---\")\n",
        "# Fixed-size chunking with a specified overlap between chunks.\n",
        "fixed_overlap_splitter = CharacterTextSplitter(\n",
        "    separator=\"\", # No specific separator, just split by size\n",
        "    chunk_size=50, # Define a fixed chunk size\n",
        "    chunk_overlap=10, # Define overlap size\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "fixed_overlap_chunks = fixed_overlap_splitter.create_documents([text])\n",
        "print(f\"Number of fixed window with overlap chunks: {len(fixed_overlap_chunks)}\")\n",
        "for i, chunk in enumerate(fixed_overlap_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk.page_content}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# --- Semantic Chunking ---\n",
        "print(\"--- Semantic Chunking ---\")\n",
        "# Semantic chunking aims to split text based on semantic meaning or topic changes.\n",
        "# This is often achieved by analyzing embeddings or using models to identify\n",
        "# boundaries. LangChain's built-in splitters primarily focus on structural/character\n",
        "# rules. A common approach for semantic chunking involves:\n",
        "# 1. Splitting into smaller, overlap-heavy chunks (e.g., sentences or small paragraphs).\n",
        "# 2. Embedding these small chunks.\n",
        "# 3. Analyzing similarity between consecutive chunk embeddings.\n",
        "# 4. Identifying \"breaks\" where similarity drops significantly.\n",
        "# 5. Merging small chunks between breaks into larger \"semantic\" chunks.\n",
        "# This requires more complex logic than a simple splitter class. We'll outline the process.\n",
        "\n",
        "print(\"Semantic chunking involves splitting based on meaning/topic changes.\")\n",
        "print(\"A common approach:\")\n",
        "print(\"1. Split text into small units (e.g., sentences) with overlap.\")\n",
        "print(\"2. Embed these units.\")\n",
        "print(\"3. Analyze embedding similarity between consecutive units.\")\n",
        "print(\"4. Identify low-similarity points as potential chunk boundaries.\")\n",
        "print(\"5. Merge units between boundaries to form semantic chunks.\")\n",
        "print(\"This requires custom implementation or specialized libraries/pipelines.\")\n",
        "\n",
        "# Example (Conceptual): Split by sentence first\n",
        "sentence_splitter = CharacterTextSplitter(\n",
        "    separator=\". \", # Simple sentence split (may not be perfect)\n",
        "    chunk_size=None, # Get each sentence as a chunk\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "sentences = sentence_splitter.split_text(text)\n",
        "print(f\"Split into {len(sentences)} potential semantic units (sentences).\")\n",
        "# Further steps (embedding, similarity analysis, merging) would follow here conceptually.\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "mZRi54Z81Ukk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}