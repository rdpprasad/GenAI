{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n7RhZhKBIqA"
      },
      "outputs": [],
      "source": [
        "# Productionizing LLM applications\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers flask flask-ngrok\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- Step 1: Loading a Pre-trained LLM ---\n",
        "\n",
        "# Choose a small model for demonstration\n",
        "MODEL_NAME = \"gpt2\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "    print(f\"Successfully loaded model and tokenizer for {MODEL_NAME}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Exit or handle the error appropriately in a real application\n",
        "    exit()\n",
        "\n",
        "# Set padding token for GPT2, as it doesn't have one by default\n",
        "# This is useful for batching requests later, though not strictly needed for this simple example\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 2 & 3: Serving the Model with Flask and Creating an API Endpoint ---\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Use ngrok to expose the Flask app running in Colab to the internet\n",
        "# Note: This is for demonstration. In production, you would use a proper web server like Gunicorn/uWSGI\n",
        "# behind a reverse proxy like Nginx/Apache.\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate_text():\n",
        "    \"\"\"\n",
        "    API endpoint to generate text based on a prompt.\n",
        "    Expects a JSON payload with a 'prompt' key.\n",
        "    \"\"\"\n",
        "    # --- Step 4: Adding Basic Error Handling ---\n",
        "    if not request.json or 'prompt' not in request.json:\n",
        "        return jsonify({\"error\": \"Invalid request. Please provide a JSON payload with a 'prompt' key.\"}), 400\n",
        "\n",
        "    prompt = request.json['prompt']\n",
        "    max_length = request.json.get('max_length', 50) # Default max_length to 50\n",
        "\n",
        "    if not isinstance(prompt, str) or not isinstance(max_length, int) or max_length <= 0:\n",
        "         return jsonify({\"error\": \"Invalid prompt (must be string) or max_length (must be a positive integer).\"}), 400\n",
        "\n",
        "\n",
        "    print(f\"Received prompt: '{prompt}' with max_length: {max_length}\")\n",
        "\n",
        "    try:\n",
        "        # Tokenize the prompt\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate text\n",
        "        # Added max_length and num_return_sequences for basic control\n",
        "        # You might want to add other generation parameters like do_sample, temperature, top_k, top_p etc.\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1, # Generate only one sequence\n",
        "            no_repeat_ngram_size=2, # Avoid repeating n-grams\n",
        "            pad_token_id=tokenizer.eos_token_id, # Important for padding\n",
        "            attention_mask=None # Let transformers handle attention mask\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Remove the original prompt from the generated text if it's at the beginning\n",
        "        if generated_text.startswith(prompt):\n",
        "            generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "        print(f\"Generated text: '{generated_text}'\")\n",
        "\n",
        "        return jsonify({\"generated_text\": generated_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")\n",
        "        return jsonify({\"error\": \"An internal error occurred during text generation.\"}), 500\n",
        "\n",
        "# Function to run the Flask app in a separate thread\n",
        "# This is needed in Colab/Jupyter to keep the notebook interactive\n",
        "def run_flask():\n",
        "    # Flask's run() is blocking, so we run it in a separate thread\n",
        "    # Use debug=False in production\n",
        "    app.run()\n",
        "\n",
        "# Start the Flask app in a new thread\n",
        "thread = threading.Thread(target=run_flask)\n",
        "thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "thread.start()\n",
        "\n",
        "# Give Flask/ngrok some time to start\n",
        "print(\"Starting Flask server...\")\n",
        "time.sleep(5) # Wait a bit for ngrok to establish tunnel\n",
        "\n",
        "# You can test the API by sending a POST request to the ngrok URL.\n",
        "# The ngrok URL will be printed by the `run_with_ngrok` function output.\n",
        "# Example using `requests` (run this in a separate cell after the server starts):\n",
        "#\n",
        "# import requests\n",
        "#\n",
        "# # Replace with the actual ngrok URL printed above\n",
        "# ngrok_url = \"YOUR_NGROK_URL\"\n",
        "#\n",
        "# data = {\"prompt\": \"Tell me a short story about a cat and a dog.\", \"max_length\": 100}\n",
        "#\n",
        "# try:\n",
        "#     response = requests.post(f\"{ngrok_url}/generate\", json=data)\n",
        "#     response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "#     result = response.json()\n",
        "#     print(\"API Response:\", result)\n",
        "# except requests.exceptions.RequestException as e:\n",
        "#     print(f\"Error calling API: {e}\")\n",
        "#\n",
        "\n",
        "# --- Step 5: Containerization (Conceptual) ---\n",
        "\n",
        "# While we can't demonstrate full containerization in Colab,\n",
        "# the typical next step for production is to package this application\n",
        "# into a Docker container.\n",
        "\n",
        "# A basic Dockerfile would look something like this:\n",
        "#\n",
        "# FROM python:3.9-slim\n",
        "#\n",
        "# WORKDIR /app\n",
        "#\n",
        "# COPY requirements.txt .\n",
        "# RUN pip install --no-cache-dir -r requirements.txt\n",
        "#\n",
        "# COPY . . # Copy your Python scripts (e.g., app.py)\n",
        "#\n",
        "# # Expose the port your Flask app listens on\n",
        "# EXPOSE 5000 # Default Flask port\n",
        "#\n",
        "# # Command to run the application using a production-ready WSGI server\n",
        "# CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n",
        "#\n",
        "# You would need a `requirements.txt` containing:\n",
        "# transformers\n",
        "# torch\n",
        "# flask\n",
        "# gunicorn # for production server\n",
        "\n",
        "# To build and run the Docker container (outside of Colab):\n",
        "# docker build -t llm-api .\n",
        "# docker run -p 5000:5000 llm-api\n",
        "#\n",
        "\n",
        "# For production deployment, you would then push this image to a container registry\n",
        "# (like Docker Hub, Google Container Registry, AWS ECR) and deploy it to:\n",
        "# - Kubernetes (GKE, EKS, AKS)\n",
        "# - Serverless platforms (Cloud Run, AWS Lambda behind API Gateway)\n",
        "# - Virtual Machines\n",
        "\n",
        "# Considerations for Production LLMs:\n",
        "# - Model Size: Large models require significant memory and compute. Consider smaller models, quantization, or model serving solutions optimized for large models (like NVIDIA Triton, or cloud-specific AI platforms).\n",
        "# - Latency and Throughput: Optimizing model inference speed is crucial. Techniques include batching requests, using optimized runtimes (TensorRT, OpenVINO), and choosing appropriate hardware (GPUs, TPUs).\n",
        "# - Scalability: Your serving infrastructure must be able to handle varying load. This is where Kubernetes or serverless platforms shine.\n",
        "# - Cost: Running powerful GPUs for LLMs can be expensive. Choose cost-effective instances and scale down when not needed.\n",
        "# - Monitoring: Implement logging and metrics to track API usage, errors, and model performance.\n",
        "# - Security: Secure your API endpoints.\n",
        "# - Model Updates: Plan how you will update the model without downtime.\n",
        "\n",
        "print(\"\\nFlask server is running. Use the ngrok URL printed above to send POST requests to the /generate endpoint.\")\n",
        "print(\"Remember to replace 'YOUR_NGROK_URL' in the example testing code.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 6: Basic Load Testing and Performance Considerations ---\n",
        "\n",
        "# In a real production scenario, you'd want to perform load testing\n",
        "# to understand how your API behaves under stress and identify bottlenecks.\n",
        "# Tools like `locust`, `Apache JMeter`, or `bombardier` can be used.\n",
        "\n",
        "# Example (Conceptual):\n",
        "# Using `requests` to send multiple requests:\n",
        "#\n",
        "# import requests\n",
        "# import time\n",
        "# import concurrent.futures\n",
        "#\n",
        "# ngrok_url = \"YOUR_NGROK_URL\" # Replace with the actual ngrok URL\n",
        "# endpoint = f\"{ngrok_url}/generate\"\n",
        "# num_requests = 10 # Number of concurrent requests\n",
        "#\n",
        "# def send_request(prompt, max_length=50):\n",
        "#     try:\n",
        "#         data = {\"prompt\": prompt, \"max_length\": max_length}\n",
        "#         response = requests.post(endpoint, json=data, timeout=60) # Add timeout\n",
        "#         response.raise_for_status()\n",
        "#         return response.json().get(\"generated_text\", \"Error\")\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         return f\"Request Error: {e}\"\n",
        "#\n",
        "# # Example prompts\n",
        "# prompts = [\n",
        "#     \"The quick brown fox jumps over the lazy dog.\",\n",
        "#     \"Once upon a time in a land far, far away,\",\n",
        "#     \"Explain the concept of machine learning.\",\n",
        "#     \"Write a short poem about stars.\",\n",
        "# ]\n",
        "#\n",
        "# print(f\"\\nStarting basic load test with {num_requests} concurrent requests...\")\n",
        "# start_time = time.time()\n",
        "#\n",
        "# # Use ThreadPoolExecutor for concurrent requests\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
        "#     # Map prompts to the send_request function\n",
        "#     # Cycle through prompts if num_requests is greater than the number of prompts\n",
        "#     results = list(executor.map(send_request, [prompts[i % len(prompts)] for i in range(num_requests)]))\n",
        "#\n",
        "# end_time = time.time()\n",
        "# print(\"Load test finished.\")\n",
        "#\n",
        "# # Print results (optional, can be verbose)\n",
        "# # for i, result in enumerate(results):\n",
        "# #     print(f\"Request {i+1}: {result[:100]}...\") # Print first 100 chars\n",
        "#\n",
        "# print(f\"Total time taken for {num_requests} requests: {end_time - start_time:.2f} seconds\")\n",
        "# print(f\"Average time per request: {(end_time - start_time) / num_requests:.2f} seconds\")\n",
        "#\n",
        "# # In a real test, you would also measure:\n",
        "# # - Success rate (how many requests returned 2xx status)\n",
        "# # - Latency distribution (min, max, average, percentiles)\n",
        "# # - Error rate\n",
        "# # - Server-side metrics (CPU/GPU usage, memory, request queue length)\n",
        "\n",
        "\n",
        "# --- Step 7: Adding Logging and Monitoring ---\n",
        "\n",
        "# Implement robust logging to understand what's happening in your application\n",
        "# and for debugging. In production, you'd integrate with a centralized logging\n",
        "# system (like Stackdriver, ELK stack, Splunk).\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Replace print statements with logging calls\n",
        "# Example in the generate_text function:\n",
        "# logging.info(f\"Received prompt: '{prompt}' with max_length: {max_length}\")\n",
        "# logging.info(f\"Generated text: '{generated_text}'\")\n",
        "# logging.error(f\"Error during text generation: {e}\", exc_info=True) # Log exception traceback\n",
        "\n",
        "# For monitoring, you would expose metrics about your application (e.g., number of requests,\n",
        "# latency, error counts) using a library like `prometheus_client` and integrate with\n",
        "# a monitoring system (Prometheus, Datadog, Cloud Monitoring).\n",
        "\n",
        "# --- Step 8: Caching (Consideration) ---\n",
        "\n",
        "# For frequently requested prompts, consider implementing a caching layer\n",
        "# (e.g., using Redis or a simple in-memory cache) to return pre-computed\n",
        "# responses and reduce inference time and cost.\n",
        "\n",
        "# Example (Conceptual simple in-memory cache):\n",
        "#\n",
        "# cache = {}\n",
        "# CACHE_TTL = 3600 # Cache time-to-live in seconds\n",
        "#\n",
        "# @app.route(\"/generate\", methods=[\"POST\"])\n",
        "# def generate_text_with_cache():\n",
        "#     # ... (input validation as before)\n",
        "#\n",
        "#     cache_key = f\"{prompt}_{max_length}\"\n",
        "#\n",
        "#     # Check cache\n",
        "#     if cache_key in cache and (time.time() - cache[cache_key]['timestamp']) < CACHE_TTL:\n",
        "#         logging.info(f\"Cache hit for key: {cache_key}\")\n",
        "#         return jsonify({\"generated_text\": cache[cache_key]['text']})\n",
        "#\n",
        "#     logging.info(f\"Cache miss for key: {cache_key}. Generating text...\")\n",
        "#\n",
        "#     try:\n",
        "#         # ... (Text generation logic as before)\n",
        "#\n",
        "#         # Store in cache\n",
        "#         cache[cache_key] = {'text': generated_text, 'timestamp': time.time()}\n",
        "#         logging.info(f\"Cached result for key: {cache_key}\")\n",
        "#\n",
        "#         return jsonify({\"generated_text\": generated_text})\n",
        "#\n",
        "#     except Exception as e:\n",
        "#         # ... (Error handling as before)\n",
        "#         pass\n",
        "#\n",
        "# # Note: A simple dictionary cache like this is not thread-safe and\n",
        "# # doesn't handle cache eviction well. Use dedicated caching libraries\n",
        "# # or services in production.\n",
        "\n",
        "# --- Step 9: Asynchronous Processing / Task Queues (Consideration) ---\n",
        "\n",
        "# For requests that might take a long time (e.g., generating very long texts),\n",
        "# processing them synchronously in the web server can block the server and\n",
        "# lead to timeouts. Consider using a task queue (like Celery with Redis/RabbitMQ)\n",
        "# to process these requests asynchronously.\n",
        "\n",
        "# The web server would accept the request, queue the task, and immediately\n",
        "# return a response (e.g., a job ID). A separate worker process would pick\n",
        "# up the task from the queue, perform the generation, and store the result.\n",
        "# The client could then poll an endpoint with the job ID to retrieve the result\n",
        "# or receive it via a webhook.\n",
        "\n",
        "# --- Step 10: Model Quantization and Optimization ---\n",
        "\n",
        "# For models that are too large or slow, techniques like quantization,\n",
        "# pruning, or using optimized model formats (like ONNX) can significantly\n",
        "# reduce model size and improve inference speed on various hardware.\n",
        "# Libraries like `Hugging Face Optimum`, `PyTorch Mobile`, or cloud AI platforms\n",
        "# offer tools for this.\n",
        "\n",
        "# Example (Conceptual with basic quantization - requires specific libraries):\n",
        "#\n",
        "# from torch.quantization import quantize_dynamic\n",
        "#\n",
        "# # Apply dynamic quantization to the model (post-training)\n",
        "# # This is a simple example, more advanced techniques exist.\n",
        "# quantized_model = quantize_dynamic(model, {torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv1d}, dtype=torch.qint8)\n",
        "#\n",
        "# # Use quantized_model for inference\n",
        "# # This might reduce model size and potentially improve CPU inference speed.\n",
        "# # model = quantized_model # Replace the original model with the quantized one if suitable\n",
        "\n",
        "# --- Step 11: A/B Testing and Model Versioning ---\n",
        "\n",
        "# In production, you'll often have different versions of your model or\n",
        "# generation parameters. You'll want to deploy these side-by-side and\n",
        "# direct a portion of traffic to each version to compare performance\n",
        "# (e.g., response quality, latency, cost) through A/B testing.\n",
        "# Deployment platforms and API gateways often provide routing capabilities\n",
        "# for this.\n",
        "\n",
        "# --- Step 12: Edge Deployment (Consideration) ---\n",
        "\n",
        "# For applications requiring very low latency or offline capabilities,\n",
        "# deploying a smaller, optimized version of the model directly to\n",
        "# user devices (mobile phones, browsers via WebAssembly/WebGPU) might\n",
        "# be an option. This requires model conversion and integration with\n",
        "# mobile/web frameworks.\n",
        "\n",
        "# --- Step 13: Cost Management ---\n",
        "\n",
        "# Running LLMs, especially large ones on GPUs, can be expensive.\n",
        "# Monitor your infrastructure costs closely. Optimize model size and inference\n",
        "# speed. Consider using spot instances or reserved instances for predictable\n",
        "# workloads. Scale down infrastructure during low-traffic periods.\n",
        "\n"
      ],
      "metadata": {
        "id": "FHSpWHNQBgt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 14: Security Considerations ---\n",
        "\n",
        "# Protecting your API and your LLM model from malicious use is crucial.\n",
        "\n",
        "# - API Authentication and Authorization: Use API keys, OAuth, or other\n",
        "#   authentication mechanisms to ensure only authorized clients can access\n",
        "#   your /generate endpoint.\n",
        "# - Input Sanitization: While LLMs are generally robust, consider sanitizing\n",
        "#   user inputs to prevent potential injection attacks (though less common\n",
        "#   in text generation APIs compared to database queries).\n",
        "# - Output Moderation: LLMs can sometimes generate inappropriate, biased,\n",
        "#   or toxic content. Implement post-processing filters or integrate with\n",
        "#   content moderation services to filter harmful outputs before returning\n",
        "#   them to the user.\n",
        "# - Rate Limiting: Protect your API from abuse and denial-of-service attacks\n",
        "#   by implementing rate limiting based on IP address, API key, or user ID.\n",
        "#   Flask-Limiter is a library that can help with this.\n",
        "\n",
        "# Example (Conceptual rate limiting with Flask-Limiter):\n",
        "#\n",
        "# !pip install Flask-Limiter\n",
        "#\n",
        "# from flask_limiter import Limiter\n",
        "# from flask_limiter.util import get_remote_address\n",
        "#\n",
        "# # Initialize Limiter\n",
        "# # Apply a global limit of 10 requests per minute per IP address\n",
        "# # In a real app, use a more robust storage like Redis instead of in-memory\n",
        "# limiter = Limiter(\n",
        "#     get_remote_address,\n",
        "#     app=app,\n",
        "#     default_limits=[\"10 per minute\"],\n",
        "#     storage_uri=\"memory://\", # Use \"redis://localhost:6379\" for Redis\n",
        "# )\n",
        "#\n",
        "# # Apply the limit to the generate_text endpoint\n",
        "# @app.route(\"/generate\", methods=[\"POST\"])\n",
        "# @limiter.limit(\"2 per second\", override_defaults=False) # Add a stricter limit for this endpoint\n",
        "# def generate_text_with_security():\n",
        "#     # ... (existing generate_text logic)\n",
        "#     pass\n",
        "#\n",
        "# # Note: Remember to apply rate limiting appropriate for your expected traffic.\n",
        "\n",
        "\n",
        "# --- Step 15: Integrating with Data Pipelines and Feedback Loops ---\n",
        "\n",
        "# In production, your LLM might be part of a larger data pipeline.\n",
        "# - Data Ingestion: Where does the input prompt come from?\n",
        "# - Output Storage: Where do you store the generated text? For analysis,\n",
        "#   auditing, or further processing?\n",
        "# - Feedback Loops: How do you collect user feedback on the quality of\n",
        "#   generated text? This feedback is crucial for monitoring model performance\n",
        "#   over time and for fine-tuning/improving the model.\n",
        "\n",
        "# Example (Conceptual - logging feedback):\n",
        "#\n",
        "# @app.route(\"/feedback\", methods=[\"POST\"])\n",
        "# def receive_feedback():\n",
        "#     \"\"\"\n",
        "#     API endpoint to receive user feedback on generated text.\n",
        "#     Expects a JSON payload with 'generated_text_id', 'rating', 'comment'.\n",
        "#     \"\"\"\n",
        "#     if not request.json or 'generated_text_id' not in request.json or 'rating' not in request.json:\n",
        "#         return jsonify({\"error\": \"Invalid request.\"}), 400\n",
        "#\n",
        "#     feedback_data = request.json\n",
        "#     logging.info(f\"Received feedback: {feedback_data}\")\n",
        "#\n",
        "#     # In a real application, you would store this feedback in a database\n",
        "#     # or a data warehouse for analysis.\n",
        "#\n",
        "#     return jsonify({\"message\": \"Feedback received.\"}), 200\n",
        "\n",
        "# --- Step 16: Handling Compliance and Regulatory Requirements ---\n",
        "\n",
        "# Depending on your industry and region, you might have compliance requirements\n",
        "# regarding data privacy (e.g., GDPR, CCPA), content restrictions, or\n",
        "# explainability/fairness of AI models. Ensure your deployment strategy and\n",
        "# data handling practices comply with relevant regulations.\n",
        "# - Data Storage: Anonymize or pseudonymize sensitive data if prompts or\n",
        "#   generated text contain it.\n",
        "# - Model Explainability: While LLMs are often black boxes, some techniques\n",
        "#   (like attention visualization, LIME, SHAP - though harder for generation)\n",
        "#   can provide insights if explainability is required.\n",
        "# - Bias Mitigation: Continuously evaluate your model for biases in its\n",
        "#   output and implement strategies to mitigate them.\n",
        "\n",
        "# --- Step 17: Disaster Recovery and Business Continuity ---\n",
        "\n",
        "# What happens if your primary deployment region goes down?\n",
        "# Implement a disaster recovery plan:\n",
        "# - Redundancy: Deploy your application across multiple availability zones\n",
        "#   or regions.\n",
        "# - Backups: Regularly back up your model weights and application configuration.\n",
        "# - Failover: Have a mechanism to automatically or manually failover to a\n",
        "#   secondary deployment if the primary fails.\n",
        "# - Monitoring: Set up alerts to notify you of outages.\n",
        "\n",
        "# --- Step 18: Cost Optimization Strategies ---\n",
        "\n",
        "# Revisited Cost: Beyond instance types, consider:\n",
        "# - Spot Instances/Preemptible VMs: Use cheaper, interruptible instances for\n",
        "#   batch processing or less critical workloads.\n",
        "# - Auto-scaling: Configure your infrastructure to automatically scale up\n",
        "#   during peak load and scale down during off-peak times.\n",
        "# - Model Serving Platforms: Cloud providers' managed AI platforms often\n",
        "#   offer optimized serving and pricing models (e.g., pay-per-inference).\n",
        "# - Model Choice: Re-evaluate if a smaller, less expensive model can meet\n",
        "#   your requirements.\n",
        "\n",
        "# --- Step 19: Documentation and API Management ---\n",
        "\n",
        "# Provide clear documentation for your API. Use tools like OpenAPI (Swagger)\n",
        "# to define your API contract. Consider using an API Gateway for managing\n",
        "# multiple endpoints, handling authentication, rate limiting, and monitoring\n",
        "# in a centralized way.\n",
        "\n",
        "# --- Step 20: Continuous Integration/Continuous Deployment (CI/CD) ---\n",
        "\n",
        "# Automate the process of building, testing, and deploying your LLM application.\n",
        "# - Version Control: Store your code and model files in a version control system (Git).\n",
        "# - Automated Testing: Implement unit tests, integration tests, and potentially\n",
        "#   model performance tests.\n",
        "# - CI Pipeline: Automatically build and test your application on every code change.\n",
        "# - CD Pipeline: Automate deployment to staging and production environments\n",
        "#   after successful testing.\n",
        "\n",
        "# This is a more comprehensive overview of considerations for productionizing LLM applications.\n",
        "# Each step can be a complex topic in itself, and the specific implementation\n",
        "# details will depend heavily on your chosen cloud provider, infrastructure,\n",
        "# and specific application requirements.\n"
      ],
      "metadata": {
        "id": "S3urLy_rB6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 21: Leveraging Cloud-Specific AI Platforms ---\n",
        "\n",
        "# Instead of building everything from scratch using VMs and Flask,\n",
        "# consider using managed AI/ML platforms provided by cloud providers.\n",
        "# These platforms offer features optimized for model deployment and serving,\n",
        "# often including:\n",
        "# - Easy model deployment from various frameworks (TensorFlow, PyTorch, Hugging Face).\n",
        "# - Auto-scaling based on traffic.\n",
        "# - Integrated monitoring and logging.\n",
        "# - Optimized inference hardware (GPUs, TPUs, custom chips).\n",
        "# - A/B testing and model versioning capabilities.\n",
        "# - Serverless inference options (pay-per-prediction).\n",
        "\n",
        "# Examples:\n",
        "# - Google Cloud AI Platform Prediction / Vertex AI\n",
        "# - AWS SageMaker Endpoints\n",
        "# - Azure Machine Learning Endpoints\n",
        "\n",
        "# Using these platforms can significantly reduce the operational overhead\n",
        "# compared to managing your own infrastructure. You would typically upload\n",
        "# your trained model artifacts and define the serving container/environment.\n",
        "\n",
        "# --- Step 22: Using Specialized LLM Serving Frameworks ---\n",
        "\n",
        "# For serving large and complex LLMs specifically, there are specialized\n",
        "# open-source and commercial serving frameworks designed for high throughput\n",
        "# and low latency. These frameworks often implement advanced techniques like:\n",
        "# - Continuous Batching: Efficiently processes multiple requests simultaneously.\n",
        "# - Quantization and Sparsity optimizations.\n",
        "# - Efficient attention mechanisms.\n",
        "# - Speculative Decoding.\n",
        "\n",
        "# Examples:\n",
        "# - vLLM: Highly optimized for serving large LLMs.\n",
        "# - NVIDIA Triton Inference Server: Supports various models and frameworks with optimizations.\n",
        "# - Text Generation Inference (TGI) by Hugging Face.\n",
        "\n",
        "# These frameworks often require more complex setup but can yield significant\n",
        "# performance improvements for demanding LLM workloads. You would typically\n",
        "# deploy these frameworks on powerful GPU instances or clusters.\n",
        "\n",
        "# --- Step 23: Fine-tuning and Adaptation ---\n",
        "\n",
        "# While loading a pre-trained model is a good starting point, for many\n",
        "# applications, you'll need to fine-tune the LLM on a domain-specific dataset\n",
        "# or adapt it to a specific task (e.g., summarization, translation).\n",
        "# This involves training the model further on your own data.\n",
        "\n",
        "# Considerations for Fine-tuning:\n",
        "# - Data Preparation: Curate and format your training data correctly.\n",
        "# - Compute Resources: Fine-tuning often requires significant compute (GPUs).\n",
        "# - Hyperparameter Tuning: Experiment with learning rates, epochs, etc.\n",
        "# - Model Size: Fine-tuning a large model is resource-intensive. Consider\n",
        "#   Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA.\n",
        "# - Deployment: Deploy the fine-tuned model as a new version.\n",
        "\n",
        "# --- Step 24: Natural Language Understanding (NLU) Integration ---\n",
        "\n",
        "# Before passing user input to the LLM, you might need to perform NLU tasks:\n",
        "# - Intent Recognition: Understand the user's goal.\n",
        "# - Entity Extraction: Identify key information in the prompt.\n",
        "# - Input Validation: Check if the prompt is relevant or appropriate.\n",
        "\n",
        "# This pre-processing step can help route requests, provide better context\n",
        "# to the LLM, or filter out irrelevant inputs.\n",
        "\n",
        "# --- Step 25: Post-processing the Output ---\n",
        "\n",
        "# The raw output from the LLM might need post-processing:\n",
        "# - Formatting: Ensure the text is formatted correctly (paragraphs, bullet points).\n",
        "# - Length Truncation: Limit the output length.\n",
        "# - Content Moderation: As mentioned before, filter inappropriate content.\n",
        "# - Adding Structure: If the expected output is structured (e.g., JSON),\n",
        "#   parse the LLM's text output into the desired format.\n",
        "\n",
        "# --- Step 26: Ethical Considerations and Responsible AI ---\n",
        "\n",
        "# Deploying LLMs comes with significant ethical responsibilities:\n",
        "# - Bias: LLMs can inherit biases from their training data, leading to unfair\n",
        "#   or discriminatory outputs. Continuously evaluate and mitigate bias.\n",
        "# - Fairness: Ensure the model performs equally well for different demographic groups.\n",
        "# - Transparency: While not always possible for LLMs, aim for explainability where\n",
        "#   needed.\n",
        "# - Safety: Prevent the model from generating harmful, illegal, or dangerous content.\n",
        "#   Implement robust safety filters and monitoring.\n",
        "# - Privacy: Handle user data and prompts responsibly, especially if they contain\n",
        "#   sensitive information.\n",
        "\n",
        "# --- Step 27: Team and Process ---\n",
        "\n",
        "# Productionizing and maintaining an LLM application requires a multidisciplinary team:\n",
        "# - ML Engineers: For model development, training, and optimization.\n",
        "# - Software Engineers: For building the serving infrastructure, API, and integrations.\n",
        "# - DevOps/SRE: For deployment, monitoring, scaling, and reliability.\n",
        "# - Data Engineers: For data pipelines and feedback loops.\n",
        "# - Product Managers/Domain Experts: To define requirements and evaluate output quality.\n",
        "\n",
        "# Establish clear processes for model development lifecycle, deployment, monitoring,\n",
        "# and incident response.\n",
        "\n",
        "# --- Step 28: Cost Monitoring and Optimization ---\n",
        "\n",
        "# Implement detailed cost monitoring to understand where resources are being spent.\n",
        "# Use cloud provider cost management tools. Identify and optimize expensive\n",
        "# operations (e.g., large GPU instances running idle, inefficient inference).\n",
        "\n",
        "# --- Step 29: Data Governance and Compliance ---\n",
        "\n",
        "# Establish clear policies for handling the data used for training, fine-tuning,\n",
        "# and inference. Ensure compliance with data privacy regulations and industry-specific\n",
        "# requirements. Track data lineage and model versions.\n",
        "\n",
        "# --- Step 30: Continuous Improvement and Model Retraining ---\n",
        "\n",
        "# LLMs and their performance can degrade over time due to shifting user behavior\n",
        "# or changes in the underlying data distribution (data drift).\n",
        "# - Monitor Model Performance: Track metrics like output quality (can be subjective\n",
        "#   and require human evaluation), user satisfaction, and relevance.\n",
        "# - Collect Feedback: Use feedback loops (Step 15) to identify areas for improvement.\n",
        "# - Retraining Strategy: Plan for periodically retraining your model on new data\n",
        "#   or fine-tuning it to address performance degradation or incorporate new requirements.\n",
        "# - A/B Testing Retrained Models: Always test new model versions against the current\n",
        "#   production version before fully rolling them out.\n",
        "\n",
        "# This extended list covers more advanced and operational aspects of taking\n",
        "# an LLM application from development to a robust, scalable, and maintainable\n",
        "# production service."
      ],
      "metadata": {
        "id": "U6szECJNB_Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 31: Configuration Management ---\n",
        "\n",
        "# Avoid hardcoding configuration values (model names, API keys, database\n",
        "# connections, etc.) directly in your code. Use configuration files (e.g.,\n",
        "# YAML, JSON) or environment variables.\n",
        "\n",
        "# Example using environment variables (Conceptual):\n",
        "#\n",
        "# import os\n",
        "#\n",
        "# MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"gpt2\") # Default to gpt2\n",
        "# API_KEY = os.environ.get(\"API_KEY\") # For external service integration\n",
        "#\n",
        "# # Access configuration values later in the code\n",
        "# print(f\"Using model: {MODEL_NAME}\")\n",
        "#\n",
        "# # To set environment variables in Colab for demonstration:\n",
        "# # os.environ['MODEL_NAME'] = 'gpt2-medium'\n",
        "#\n",
        "# # In production, you would set environment variables in your deployment environment\n",
        "# # (e.g., Dockerfile, Kubernetes deployment YAML, Cloud Run settings).\n",
        "\n",
        "\n",
        "# --- Step 32: API Versioning ---\n",
        "\n",
        "# As your API evolves, you may need to introduce changes that are not\n",
        "# backward-compatible. Implement API versioning to allow clients to continue\n",
        "# using older versions while new versions are being rolled out.\n",
        "\n",
        "# Common versioning strategies:\n",
        "# - URL Path Versioning: e.g., `/v1/generate`, `/v2/generate`\n",
        "# - Header Versioning: e.g., `Accept: application/json; version=1.0`\n",
        "# - Query Parameter Versioning: e.g., `/generate?api-version=1`\n",
        "\n",
        "# Example (Conceptual URL versioning):\n",
        "#\n",
        "# @app.route(\"/v1/generate\", methods=[\"POST\"])\n",
        "# def generate_text_v1():\n",
        "#     # ... (logic for version 1)\n",
        "#     pass\n",
        "#\n",
        "# @app.route(\"/v2/generate\", methods=[\"POST\"])\n",
        "# def generate_text_v2():\n",
        "#     # ... (updated logic for version 2)\n",
        "#     pass\n",
        "\n",
        "# --- Step 33: Documentation and SDKs ---\n",
        "\n",
        "# Good documentation is essential for developers using your API.\n",
        "# - API Reference: Detail endpoints, parameters, request/response formats,\n",
        "#   and error codes (e.g., using OpenAPI/Swagger).\n",
        "# - Guides and Examples: Provide tutorials and code snippets in different\n",
        "#   programming languages.\n",
        "# - Client Libraries (SDKs): Consider generating or manually creating client\n",
        "#   libraries to simplify integration for users.\n",
        "\n",
        "# --- Step 34: Observability (Metrics, Logging, Tracing) ---\n",
        "\n",
        "# Beyond basic logging and monitoring, aim for observability:\n",
        "# - Metrics: Collect detailed metrics (request count, latency percentiles,\n",
        "#   error rates, GPU utilization, model inference time) using tools like\n",
        "#   Prometheus and visualize them in dashboards (Grafana).\n",
        "# - Distributed Tracing: If your application involves multiple services or\n",
        "#   components (e.g., API gateway, your service, caching layer, database),\n",
        "#   use distributed tracing (e.g., OpenTelemetry, Jaeger) to track requests\n",
        "#   end-to-end and identify bottlenecks.\n",
        "# - Structured Logging: Use structured logging (JSON format) for easier\n",
        "#   parsing and analysis in centralized logging systems.\n",
        "\n",
        "# --- Step 35: Disaster Recovery Plan Testing ---\n",
        "\n",
        "# It's not enough to *have* a disaster recovery plan; you need to *test* it\n",
        "# regularly. Simulate failures (e.g., bringing down a database, a server,\n",
        "# or a whole region) to ensure your failover mechanisms work as expected\n",
        "# and your recovery time objectives (RTO) and recovery point objectives (RPO)\n",
        "# are met.\n",
        "\n",
        "# --- Step 36: Capacity Planning ---\n",
        "\n",
        "# Understand the capacity limits of your deployed infrastructure and model.\n",
        "# - Determine how many requests per second your current setup can handle\n",
        "#   while meeting latency requirements.\n",
        "# - Plan for scaling based on anticipated future traffic growth.\n",
        "# - Consider different instance types and their performance characteristics\n",
        "#   for your specific model.\n",
        "\n",
        "# --- Step 37: Model Explainability and Interpretability ---\n",
        "\n",
        "# Depending on the application domain (e.g., healthcare, finance), explainability\n",
        "# might be a legal or ethical requirement. While complex for generative models,\n",
        "# research in this area is ongoing. Techniques might include:\n",
        "# - Attention visualization.\n",
        "# - Analyzing model activations.\n",
        "# - Using simpler surrogate models.\n",
        "\n",
        "# --- Step 38: User Authentication and Authorization ---\n",
        "\n",
        "# For applications where users interact directly or where requests are associated\n",
        "# with specific users, implement robust user authentication (verifying user identity)\n",
        "# and authorization (controlling what actions a user can perform).\n",
        "\n",
        "# Example (Conceptual with basic token auth):\n",
        "#\n",
        "# import uuid\n",
        "#\n",
        "# # Simple in-memory \"database\" of valid API keys\n",
        "# valid_api_keys = {\n",
        "#     str(uuid.uuid4()): \"user1\",\n",
        "#     str(uuid.uuid4()): \"user2\",\n",
        "# }\n",
        "#\n",
        "# @app.route(\"/generate\", methods=[\"POST\"])\n",
        "# def generate_text_with_auth():\n",
        "#     api_key = request.headers.get(\"X-API-Key\") # Get API key from header\n",
        "#\n",
        "#     if api_key not in valid_api_keys:\n",
        "#         return jsonify({\"error\": \"Unauthorized. Invalid API Key.\"}), 401\n",
        "#\n",
        "#     # ... (rest of the generate_text logic)\n",
        "#\n",
        "#     # You can potentially use the user ID (e.g., valid_api_keys[api_key])\n",
        "#     # for logging, rate limiting, or tracking usage per user.\n",
        "#\n",
        "#     return jsonify({\"generated_text\": generated_text})\n",
        "#\n",
        "# # Note: Use a secure method for managing API keys in production, like\n",
        "# # environment variables or a secret management system.\n",
        "\n",
        "\n",
        "# --- Step 39: Securing Model Artifacts ---\n",
        "\n",
        "# Your trained model weights are valuable intellectual property and could\n",
        "# potentially be misused if accessed by unauthorized parties.\n",
        "# - Store model files securely (e.g., in private cloud storage buckets with\n",
        "#   strict access controls).\n",
        "# - Use encrypted storage.\n",
        "# - Control access to your deployment environment where the model is loaded.\n",
        "\n",
        "# --- Step 40: Legal and Compliance Review ---\n",
        "\n",
        "# Before deploying to production, especially in regulated industries, have a\n",
        "# legal and compliance review of your application, data handling practices,\n",
        "# and model usage. Ensure you understand and comply with all relevant laws\n",
        "# and regulations (e.g., data privacy, intellectual property, content moderation,\n",
        "# AI ethics guidelines).\n",
        "\n",
        "# This extended list provides a more comprehensive view of the complexities\n",
        "# involved in successfully productionizing LLM applications, moving beyond\n",
        "# just serving the model to building a reliable, scalable, secure, and\n",
        "# maintainable service."
      ],
      "metadata": {
        "id": "x8BVsQvpCG-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 41: Implementing Health Checks ---\n",
        "\n",
        "# Essential for monitoring and orchestration systems (like Kubernetes)\n",
        "# to determine if your application instance is healthy and can serve traffic.\n",
        "\n",
        "@app.route(\"/healthz\", methods=[\"GET\"])\n",
        "def health_check():\n",
        "    \"\"\"\n",
        "    Basic health check endpoint. Returns 200 OK if the server is running.\n",
        "    Could be extended to check model loading or connectivity to external services.\n",
        "    \"\"\"\n",
        "    # In a more advanced health check, you might:\n",
        "    # - Try to load a small input through the model.\n",
        "    # - Check connectivity to any backend databases or caches.\n",
        "    # - Verify required dependencies are available.\n",
        "\n",
        "    # Simple check: just return OK if the app is running\n",
        "    return jsonify({\"status\": \"ok\"}), 200\n",
        "\n",
        "# Monitoring systems or orchestration platforms will periodically hit this endpoint.\n",
        "# If it returns a non-200 status or times out, the system knows the instance\n",
        "# is unhealthy and can take action (e.g., restart the instance, route traffic away).\n",
        "\n",
        "# --- Step 42: Graceful Shutdown ---\n",
        "\n",
        "# Ensure your application can shut down cleanly without dropping active requests.\n",
        "# This is important during deployments or scaling events.\n",
        "# - Flask's development server doesn't handle this well, but production WSGI\n",
        "#   servers like Gunicorn or uWSGI have mechanisms for graceful shutdown.\n",
        "# - They typically stop accepting new connections but continue processing\n",
        "#   existing ones for a configurable timeout period.\n",
        "\n",
        "# Example (Conceptual - relies on the WSGI server):\n",
        "#\n",
        "# # In a production WSGI server command (like Gunicorn):\n",
        "# # gunicorn -w 4 -b 0.0.0.0:5000 --timeout 30 --graceful-timeout 30 app:app\n",
        "# # '--timeout' is for request duration, '--graceful-timeout' is for shutdown.\n",
        "\n",
        "# --- Step 43: Handling Out-of-Memory Errors ---\n",
        "\n",
        "# LLMs, especially large ones, consume significant memory (GPU and CPU).\n",
        "# - Monitor memory usage closely.\n",
        "# - Choose instances with sufficient RAM and GPU memory.\n",
        "# - Implement techniques like quantization, model parallelism, or offloading\n",
        "#   to reduce memory footprint if needed.\n",
        "# - Be prepared for OOM errors; logs should capture these, and monitoring\n",
        "#   should alert you. Orchestration systems can automatically restart instances.\n",
        "\n",
        "# --- Step 44: Input and Output Token Limits ---\n",
        "\n",
        "# LLMs have context windows (maximum number of input tokens they can process)\n",
        "# and practical limits on the number of output tokens they can generate due to\n",
        "# computational cost and latency.\n",
        "# - Define and enforce maximum limits for both input prompt size and generated\n",
        "#   text length in your API.\n",
        "# - Handle cases where the user prompt exceeds the context window (e.g., truncate\n",
        "#   the prompt, return an error).\n",
        "# - Consider adding input validation for token count, not just character count.\n",
        "\n",
        "# Example modification in generate_text (Conceptual token count check):\n",
        "#\n",
        "# @app.route(\"/generate\", methods=[\"POST\"])\n",
        "# def generate_text_with_limits():\n",
        "#     # ... (initial validation and prompt/max_length extraction)\n",
        "#\n",
        "#     try:\n",
        "#         input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "#         input_token_count = input_ids.shape[-1]\n",
        "#\n",
        "#         MAX_INPUT_TOKENS = 1024 # Define a reasonable max input length for your model\n",
        "#\n",
        "#         if input_token_count > MAX_INPUT_TOKENS:\n",
        "#              return jsonify({\"error\": f\"Prompt exceeds maximum allowed input tokens ({MAX_INPUT_TOKENS}).\"}), 400\n",
        "#\n",
        "#         # ... (rest of generation logic)\n",
        "#\n",
        "#         # Ensure max_length passed to model.generate doesn't exceed a global max output tokens\n",
        "#         MAX_GENERATION_TOKENS = 512 # Define a reasonable max generation length\n",
        "#         effective_max_length = min(max_length, MAX_GENERATION_TOKENS)\n",
        "#\n",
        "#         output = model.generate(\n",
        "#             input_ids,\n",
        "#             max_length=input_token_count + effective_max_length, # Total sequence length\n",
        "#             # ... other parameters\n",
        "#         )\n",
        "#         # ... (decoding and return)\n",
        "#\n",
        "#     except Exception as e:\n",
        "#          # ... (error handling)\n",
        "#          pass\n",
        "\n",
        "# --- Step 45: Handling Bias and Fairness ---\n",
        "\n",
        "# Explicitly address potential biases in LLM outputs.\n",
        "# - Bias Detection: Use libraries or human evaluation to identify if the model\n",
        "#   generates biased or unfair content based on sensitive attributes.\n",
        "# - Bias Mitigation: Techniques include:\n",
        "#     - Data Augmentation/Filtering during fine-tuning.\n",
        "#     - Prompt Engineering to guide the model away from biased responses.\n",
        "#     - Post-processing filters to detect and modify/remove biased outputs.\n",
        "# - Documentation: Be transparent with users about the potential for bias\n",
        "#   in the model's output.\n",
        "\n",
        "# --- Step 46: Handling PII (Personally Identifiable Information) ---\n",
        "\n",
        "# If user prompts might contain PII:\n",
        "# - Anonymization/Pseudonymization: Implement techniques to detect and remove or\n",
        "#   mask PII from prompts before sending them to the model.\n",
        "# - Data Retention Policies: Define how long prompts and generated outputs\n",
        "#   are stored and when they are deleted.\n",
        "# - Compliance: Ensure your handling of PII complies with regulations like GDPR, CCPA.\n",
        "\n",
        "# --- Step 47: Experiment Tracking and Model Registry ---\n",
        "\n",
        "# For managing multiple model versions, fine-tuning experiments, and hyperparameters:\n",
        "# - Use MLflow, Comet ML, Weights & Biases, or cloud-specific services (Vertex AI\n",
        "#   Experiments/Model Registry, SageMaker MLflow integration).\n",
        "# - Track hyperparameters, metrics (perplexity, generation quality scores),\n",
        "#   datasets used, and model artifacts for each experiment.\n",
        "# - Maintain a model registry to store and manage registered model versions\n",
        "#   ready for deployment.\n",
        "\n",
        "# --- Step 48: Prompt Engineering Best Practices ---\n",
        "\n",
        "# How users phrase their prompts significantly impacts LLM output.\n",
        "# - Provide clear guidelines or templates for users on how to write effective prompts.\n",
        "# - Consider adding an internal \"prompt augmentation\" step in your API to\n",
        "#   automatically add context or instructions to the user's raw prompt before\n",
        "#   passing it to the model.\n",
        "\n",
        "# --- Step 49: Integrating with Other Services ---\n",
        "\n",
        "# LLM applications often need to interact with other parts of your system:\n",
        "# - Databases: To store prompts, outputs, feedback, or user data.\n",
        "# - Search Engines/Knowledge Bases: To retrieve relevant information to augment\n",
        "#   the prompt (Retrieval Augmented Generation - RAG).\n",
        "# - Downstream applications: That consume the generated text.\n",
        "# - Monitoring and Alerting Systems.\n",
        "# - Logging Aggregation Systems.\n",
        "\n",
        "# Design your application and infrastructure to facilitate these integrations.\n",
        "\n",
        "# --- Step 50: User Experience (UX) Considerations ---\n",
        "\n",
        "# The quality and latency of the generated text directly impact the user experience.\n",
        "# - Manage user expectations regarding generation time.\n",
        "# - Implement streaming responses if possible (sending back text as it's generated)\n",
        "#   to improve perceived latency.\n",
        "# - Handle errors gracefully and provide informative messages to the user.\n",
        "# - Design the user interface (if any) to effectively capture prompts and display outputs.\n"
      ],
      "metadata": {
        "id": "FiIGJu_7CL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Example of using Flask-Limiter (requires installation: !pip install Flask-Limiter)\n",
        "from flask_limiter import Limiter\n",
        "from flask_limiter.util import get_remote_address\n",
        "\n",
        "# Initialize Limiter\n",
        "# Apply a global limit of 60 requests per minute per IP address by default\n",
        "# In a real app, use a more robust storage like Redis instead of in-memory\n",
        "limiter = Limiter(\n",
        "    get_remote_address,\n",
        "    app=app,\n",
        "    default_limits=[\"60 per minute\"], # Example: 60 requests per minute globally\n",
        "    storage_uri=\"memory://\", # Use \"redis://localhost:6379\" for Redis in production\n",
        ")\n",
        "\n",
        "# Apply the limit to the generate_text endpoint\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "# Apply a stricter limit specifically for the generate endpoint, e.g., 10 requests per minute\n",
        "# Use `override_defaults=False` to keep the global limit if needed, or `True` to replace it\n",
        "@limiter.limit(\"10 per minute\", override_defaults=False)\n",
        "def generate_text_with_security():\n",
        "    \"\"\"\n",
        "    API endpoint to generate text with rate limiting.\n",
        "    Expects a JSON payload with a 'prompt' key.\n",
        "    \"\"\"\n",
        "    if not request.json or 'prompt' not in request.json:\n",
        "        # Note: Rate limiting should ideally happen before this validation for efficiency,\n",
        "        # but for demonstration, it's applied via the decorator.\n",
        "        return jsonify({\"error\": \"Invalid request. Please provide a JSON payload with a 'prompt' key.\"}), 400\n",
        "\n",
        "    prompt = request.json['prompt']\n",
        "    max_length = request.json.get('max_length', 50)\n",
        "\n",
        "    if not isinstance(prompt, str) or not isinstance(max_length, int) or max_length <= 0:\n",
        "         return jsonify({\"error\": \"Invalid prompt (must be string) or max_length (must be a positive integer).\"}), 400\n",
        "\n",
        "    logging.info(f\"Received prompt: '{prompt[:50]}...' with max_length: {max_length}\") # Log truncated prompt\n",
        "\n",
        "    try:\n",
        "        # --- Add Input Token Limit Check (Conceptual) ---\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "        input_token_count = input_ids.shape[-1]\n",
        "\n",
        "        MAX_INPUT_TOKENS = 512  # Define a reasonable max input length for gpt2\n",
        "        if input_token_count > MAX_INPUT_TOKENS:\n",
        "            logging.warning(f\"Prompt exceeds max input tokens: {input_token_count} > {MAX_INPUT_TOKENS}\")\n",
        "            return jsonify({\"error\": f\"Prompt exceeds maximum allowed input tokens ({MAX_INPUT_TOKENS}).\"}), 400\n",
        "\n",
        "        input_ids = input_ids.to(device) # Move to device after token check\n",
        "\n",
        "        # --- Add Global Max Generation Token Limit ---\n",
        "        MAX_GENERATION_TOKENS = 200 # Define a reasonable max generation length to prevent excessive generation\n",
        "        effective_max_length = min(max_length, MAX_GENERATION_TOKENS)\n",
        "\n",
        "        logging.info(f\"Generating with effective max_length: {effective_max_length} tokens\")\n",
        "\n",
        "        # Generate text\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            # Total sequence length = input + generated\n",
        "            max_length=input_token_count + effective_max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            attention_mask=None\n",
        "        )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Remove the original prompt from the generated text if it's at the beginning\n",
        "        if generated_text.startswith(prompt):\n",
        "            generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "        logging.info(f\"Generated text: '{generated_text[:100]}...'\") # Log truncated output\n",
        "\n",
        "        return jsonify({\"generated_text\": generated_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during text generation: {e}\", exc_info=True) # Log traceback for errors\n",
        "        return jsonify({\"error\": \"An internal error occurred during text generation.\"}), 500\n",
        "\n",
        "# --- Add a Health Check Endpoint ---\n",
        "@app.route(\"/healthz\", methods=[\"GET\"])\n",
        "def health_check():\n",
        "    \"\"\"\n",
        "    Basic health check endpoint. Returns 200 OK if the server is running.\n",
        "    Can be extended to check model loading or connectivity.\n",
        "    \"\"\"\n",
        "    logging.info(\"Health check received.\")\n",
        "    # Add checks here if needed, e.g., try a quick model inference or check device status\n",
        "    try:\n",
        "        # Simple model check: encode/decode a token\n",
        "        test_input = tokenizer.encode(\"hello\", return_tensors=\"pt\").to(device)\n",
        "        test_output = model.generate(test_input, max_length=test_input.shape[-1] + 1, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "        tokenizer.decode(test_output[0], skip_special_tokens=True)\n",
        "        logging.info(\"Health check: Model is responsive.\")\n",
        "        return jsonify({\"status\": \"ok\", \"model_status\": \"responsive\"}), 200\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Health check failed: {e}\", exc_info=True)\n",
        "        return jsonify({\"status\": \"error\", \"model_status\": \"unresponsive\", \"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# --- Add a Feedback Endpoint (Conceptual) ---\n",
        "@app.route(\"/feedback\", methods=[\"POST\"])\n",
        "def receive_feedback():\n",
        "    \"\"\"\n",
        "    API endpoint to receive user feedback on generated text.\n",
        "    Expects a JSON payload with 'prompt', 'generated_text', 'rating', 'comment'.\n",
        "    \"\"\"\n",
        "    if not request.json or 'prompt' not in request.json or 'generated_text' not in request.json or 'rating' not in request.json:\n",
        "        return jsonify({\"error\": \"Invalid request. Missing required fields.\"}), 400\n",
        "\n",
        "    feedback_data = request.json\n",
        "    # Validate feedback data types if necessary\n",
        "    if not isinstance(feedback_data.get('rating'), (int, float)):\n",
        "         return jsonify({\"error\": \"Invalid rating format.\"}), 400\n",
        "\n",
        "\n",
        "    logging.info(f\"Received feedback: Prompt='{feedback_data['prompt'][:50]}...', Generated='{feedback_data['generated_text'][:50]}...', Rating={feedback_data['rating']}, Comment='{feedback_data.get('comment', '')[:50]}...'\")\n",
        "\n",
        "    # In a real application, store this feedback in a database or data lake\n",
        "    # for model monitoring and potential retraining data.\n",
        "    # Example: store in a file for this demo (not production-ready)\n",
        "    try:\n",
        "        with open(\"feedback.log\", \"a\") as f:\n",
        "            import json\n",
        "            f.write(json.dumps(feedback_data) + \"\\n\")\n",
        "        logging.info(\"Feedback logged to feedback.log\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to write feedback to file: {e}\")\n",
        "        # Decide if feedback failure should result in an error response\n",
        "\n",
        "    return jsonify({\"message\": \"Feedback received. Thank you!\"}), 200\n",
        "\n",
        "\n",
        "# Re-starting Flask app thread with updated routes\n",
        "# Stop the existing thread if it's running (optional, but good practice)\n",
        "# if 'thread' in locals() and thread.is_alive():\n",
        "#     print(\"Attempting to stop existing Flask thread...\")\n",
        "#     # There's no clean way to stop a Python thread from outside,\n",
        "#     # In a real app, you'd manage the server process differently.\n",
        "#     # For Colab, often just running the cell again is sufficient\n",
        "#     # but might leave the old server running in the background.\n",
        "#     # A more robust approach for development might involve signaling\n",
        "#     # or checking a flag in the request handler (not shown here).\n",
        "#     # For this example, we'll just start a new one.\n",
        "\n",
        "print(\"Starting Flask server with updated endpoints...\")\n",
        "# Start the Flask app in a new thread\n",
        "# Ensure Flask runs on 0.0.0.0 to be accessible by ngrok\n",
        "# Use_reloader=False is important for Colab, otherwise it might start multiple processes\n",
        "thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False))\n",
        "thread.daemon = True # Allow the main program to exit even if the thread is running\n",
        "thread.start()\n",
        "\n",
        "# Give Flask/ngrok some time to start\n",
        "print(\"Giving Flask server time to start...\")\n",
        "time.sleep(10) # Increased sleep slightly\n",
        "\n",
        "print(\"\\nFlask server is running. Use the ngrok URL printed above to send POST requests to the /generate endpoint.\")\n",
        "print(\"Also available: /healthz (GET) and /feedback (POST).\")\n",
        "print(\"Remember to replace 'YOUR_NGROK_URL' in the example testing code.\")\n"
      ],
      "metadata": {
        "id": "YZlHDNokCR7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# --- Step 51: Environment-Specific Configuration ---\n",
        "\n",
        "# Production deployments require different configurations than development\n",
        "# (e.g., database URLs, API keys, logging levels). Use a library like\n",
        "# `python-dotenv` or `Dynaconf` to manage environment-specific settings.\n",
        "\n",
        "# Example using a simple config dictionary (Conceptual):\n",
        "#\n",
        "# import os\n",
        "#\n",
        "# # Define different configurations\n",
        "# CONFIG = {\n",
        "#     \"development\": {\n",
        "#         \"LOG_LEVEL\": logging.DEBUG,\n",
        "#         \"STORAGE_URI\": \"memory://\",\n",
        "#         \"MAX_INPUT_TOKENS\": 1024,\n",
        "#     },\n",
        "#     \"production\": {\n",
        "#         \"LOG_LEVEL\": logging.INFO,\n",
        "#         \"STORAGE_URI\": \"redis://redis:6379\", # Use a real Redis instance\n",
        "#         \"MAX_INPUT_TOKENS\": 512, # Potentially stricter in production\n",
        "#         \"API_KEY_HEADER\": \"X-API-Key\",\n",
        "#     }\n",
        "# }\n",
        "#\n",
        "# # Determine the current environment\n",
        "# ENV = os.environ.get(\"FLASK_ENV\", \"development\") # Default to development\n",
        "# current_config = CONFIG.get(ENV, CONFIG[\"development\"])\n",
        "#\n",
        "# # Apply configuration\n",
        "# logging.basicConfig(level=current_config[\"LOG_LEVEL\"], format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "# limiter = Limiter(\n",
        "#     get_remote_address,\n",
        "#     app=app,\n",
        "#     default_limits=[\"60 per minute\"],\n",
        "#     storage_uri=current_config[\"STORAGE_URI\"],\n",
        "# )\n",
        "# MAX_INPUT_TOKENS = current_config[\"MAX_INPUT_TOKENS\"]\n",
        "#\n",
        "# # Access other config values as needed, e.g.:\n",
        "# # api_key = request.headers.get(current_config.get(\"API_KEY_HEADER\", \"Authorization\"))\n",
        "\n",
        "# In a real application, you would load these configurations from files\n",
        "# or environment variables using a proper configuration management library.\n",
        "\n",
        "# --- Step 52: Infrastructure as Code (IaC) ---\n",
        "\n",
        "# Manage your cloud infrastructure (VMs, containers, load balancers, databases)\n",
        "# using code. This ensures consistency, repeatability, and makes it easier to\n",
        "# manage different environments (dev, staging, prod).\n",
        "\n",
        "# Tools for IaC:\n",
        "# - Terraform\n",
        "# - CloudFormation (AWS)\n",
        "# - Deployment Manager / Cloud Deployment Manager (GCP)\n",
        "# - Azure Resource Manager (ARM) templates\n",
        "# - Kubernetes YAML manifests\n",
        "\n",
        "# Example (Conceptual Terraform structure):\n",
        "#\n",
        "# /terraform\n",
        "#   /modules\n",
        "#     /llm-service\n",
        "#       main.tf # Defines container deployment, scaling, load balancer\n",
        "#     /redis\n",
        "#       main.tf # Defines Redis instance\n",
        "#   /environments\n",
        "#     /dev\n",
        "#       main.tf # Uses llm-service and redis modules, defines dev-specific vars\n",
        "#       variables.tf\n",
        "#     /prod\n",
        "#       main.tf # Uses llm-service and redis modules, defines prod-specific vars\n",
        "#       variables.tf\n",
        "\n",
        "# --- Step 53: Secrets Management ---\n",
        "\n",
        "# Don't hardcode sensitive information (database passwords, API keys for\n",
        "# external services, private keys) in your code or configuration files.\n",
        "# Use a dedicated secrets management system.\n",
        "\n",
        "# Tools:\n",
        "# - HashiCorp Vault\n",
        "# - AWS Secrets Manager\n",
        "# - Google Cloud Secret Manager\n",
        "# - Azure Key Vault\n",
        "# - Kubernetes Secrets (with caution and potentially encryption)\n",
        "\n",
        "# Example (Conceptual - fetching a secret in code):\n",
        "#\n",
        "# import os\n",
        "# # Assuming a library to fetch from a secret manager based on environment variables\n",
        "# # For example, using Google Cloud Secret Manager client library\n",
        "# # from google.cloud import secretmanager\n",
        "#\n",
        "# # def access_secret_version(project_id, secret_id, version_id=\"latest\"):\n",
        "# #     client = secretmanager.SecretManagerServiceClient()\n",
        "# #     name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
        "# #     response = client.access_secret_version(request={\"name\": name})\n",
        "# #     return response.payload.data.decode(\"UTF-8\")\n",
        "#\n",
        "# # REDIS_PASSWORD = access_secret_version(\"my-gcp-project\", \"redis-password\")\n",
        "# # DB_PASSWORD = access_secret_version(\"my-gcp-project\", \"database-password\")\n",
        "\n",
        "# You would configure your deployment environment to provide necessary\n",
        "# credentials or roles for your application to access the secrets manager.\n",
        "\n",
        "# --- Step 54: Centralized Log Management and Analysis ---\n",
        "\n",
        "# As your application scales, logs from different instances and services need\n",
        "# to be collected, aggregated, and analyzed in a centralized system.\n",
        "# - Logging Agents: Use agents (e.g., Filebeat, Fluentd, Cloud Logging agents)\n",
        "#   to collect logs from application instances and send them to a central system.\n",
        "# - Centralized Systems:\n",
        "#   - ELK stack (Elasticsearch, Logstash, Kibana)\n",
        "#   - Splunk\n",
        "#   - Cloud Logging (GCP)\n",
        "#   - CloudWatch Logs (AWS)\n",
        "#   - Azure Monitor Logs\n",
        "\n",
        "# This allows for searching, filtering, analyzing, and visualizing logs from\n",
        "# your entire application stack, crucial for debugging and monitoring.\n",
        "\n",
        "# --- Step 55: Performance Monitoring and Profiling ---\n",
        "\n",
        "# Beyond just latency metrics, understand where time is spent within your\n",
        "# application and model inference.\n",
        "# - Application Performance Monitoring (APM): Use tools (e.g., Datadog, New Relic,\n",
        "#   Prometheus + Grafana) to monitor request traces, function call times, database\n",
        "#   queries, and external service calls.\n",
        "# - Model Profiling: Use profiling tools specific to your ML framework (e.g.,\n",
        "#   PyTorch profiler, TensorFlow profiler) to analyze the performance of\n",
        "#   model layers and operations. Identify bottlenecks (e.g., CPU-bound pre-processing,\n",
        "#   GPU underutilization).\n",
        "\n",
        "# --- Step 56: Data Versioning and Provenance ---\n",
        "\n",
        "# If you are fine-tuning your LLM, managing datasets is crucial.\n",
        "# - Data Versioning: Use tools like DVC (Data Version Control) to version\n",
        "#   datasets alongside your code.\n",
        "# - Data Provenance: Track which dataset version was used to train or fine-tune\n",
        "#   each model version. This is essential for reproducibility and debugging.\n",
        "\n",
        "# --- Step 57: Cost Optimization Strategies (Advanced) ---\n",
        "\n",
        "# - Right-sizing Instances: Continuously evaluate if your instances are\n",
        "#   appropriately sized for the workload. Avoid over-provisioning.\n",
        "# - Autoscaling Optimization: Fine-tune autoscaling policies to respond\n",
        "#   quickly to load changes while avoiding excessive costs from frequent scaling.\n",
        "# - Reserved Instances/Savings Plans: For predictable base loads, purchase\n",
        "#   reserved instances or savings plans from cloud providers for discounted rates.\n",
        "# - Spot Instances: Use spot instances for fault-tolerant workloads like batch\n",
        "#   inference or model training/fine-tuning if cost is a major driver and\n",
        "#   interruptions are acceptable.\n",
        "# - Model Optimization: As mentioned in Step 10, optimize the model itself\n",
        "#   (quantization, pruning) to potentially run on cheaper or fewer resources.\n",
        "\n",
        "# --- Step 58: Handling Dependency Management ---\n",
        "\n",
        "# Use a dependency manager (`pipenv`, `poetry`, `conda`) and a `requirements.txt`\n",
        "# (or similar lock file) to specify exact dependencies. This ensures consistency\n",
        "# across development, testing, and production environments and avoids \"it works\n",
        "# on my machine\" issues.\n",
        "\n",
        "# Example requirements.txt:\n",
        "# transformers==4.30.2\n",
        "# torch==2.0.1\n",
        "# flask==2.3.2\n",
        "# flask-ngrok==0.0.25\n",
        "# Flask-Limiter==3.5.1\n",
        "# gunicorn==21.2.0 # for production server\n",
        "\n",
        "# Ensure your Dockerfile or deployment process installs these dependencies correctly.\n",
        "\n",
        "# --- Step 59: Integration Testing ---\n",
        "\n",
        "# Write automated integration tests that verify the interaction between different\n",
        "# components of your system (e.g., testing the API endpoint, ensuring it\n",
        "# successfully loads the model, performs inference, and returns a valid response).\n",
        "# These tests are crucial for catching issues that unit tests miss.\n",
        "\n",
        "# --- Step 60: Chaos Engineering (Advanced) ---\n",
        "\n",
        "# Intentionally inject failures into your system (e.g., delay requests, shut down\n",
        "# instances, introduce network latency) in a controlled environment to test the\n",
        "# resilience of your application and infrastructure. Tools like \"Chaos Monkey\"\n",
        "# or \"Gremlin\" can be used. This helps identify weak points before they cause\n",
        "# production outages.\n",
        "\n",
        "# These additional steps cover more advanced topics in building robust,\n",
        "# scalable, and maintainable production systems for machine learning models,\n",
        "# including LLMs. The specific steps you implement will depend on the\n",
        "# complexity and criticality of your application.\n"
      ],
      "metadata": {
        "id": "jrqTBVa1CYTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}