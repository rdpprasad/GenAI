{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here's a brief write-up of the LLM evaluation libraries and metrics you mentioned, based on their common use cases:\n",
        "\n",
        "1. deepeval: A Python library for evaluating LLM outputs, particularly useful for RAG (Retrieval Augmented Generation) systems. It provides various metrics like Faithfulness, Answer Relevance, Toxicity, Bias, and more, often allowing for programmatic testing of LLM pipelines.\n",
        "\n",
        "2. lettucedetect: This tool is designed to detect \"hallucinations\" or factual inconsistencies in generated text. It aims to identify sentences that are likely to be false or unfounded.\n",
        "\n",
        "3. RAGAS: Specifically focused on evaluating RAG pipelines. RAGAS offers metrics tailored to assessing the quality of retrieved context and the generated answer's faithfulness to that context, as well as relevance and recall.\n",
        "\n",
        "4. ROUGE (Recall-Oriented Understudy for Gisting Evaluation): A set of metrics commonly used for evaluating summarization and machine translation. It measures the overlap of n-grams and longest common subsequences between the generated text and reference texts, focusing on recall.\n",
        "\n",
        "5. rouge_score: A Python library that provides an implementation of the ROUGE metrics, allowing you to easily compute ROUGE scores between candidate and reference texts.\n",
        "\n",
        "6. giskard: An open-source platform and library for testing and evaluating AI models, including LLMs. It helps identify vulnerabilities, performance issues, and biases through various testing capabilities.\n",
        "\n",
        "7. SECTOR: While less commonly known compared to others, SECTOR is a framework focused on evaluating the semantic coherence and textual similarity of generated text. It aims to measure how well the generated text flows and maintains meaning.\n",
        "\n",
        "8. BLEU (Bilingual Evaluation Understudy): Primarily used for evaluating machine translation, but also applicable to other text generation tasks with reference texts. It measures the precision of n-grams in the generated text compared to reference texts, with a penalty for brevity.\n",
        "\n",
        "These tools and metrics offer different perspectives on LLM performance, covering aspects from factual correctness and relevance to fluency and overall quality. The choice of which to use depends on the specific task and evaluation goals."
      ],
      "metadata": {
        "id": "Z18JVu4dLCuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5XKadi5JLUn"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LLM and Statistical Evaluation Tutorial.ipynb\n",
        "\n",
        "# LLM and Statistical Evaluation Techniques Tutorial\n",
        "\n",
        "This notebook provides a hands-on introduction to using Large Language Models (LLMs) and evaluating their performance using standard statistical metrics in Python. We'll use the Hugging Face `transformers` library for the LLM and the `evaluate` library for metrics.\n",
        "\n",
        "## 1. Setup and Installations\n",
        "\n",
        "First, let's install the necessary libraries.\n",
        "\"\"\"\n",
        "\n",
        "# Install required libraries\n",
        "!pip install transformers datasets evaluate nltk rouge_score\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Download necessary NLTK data for some metrics\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "\"\"\"## 2. Loading a Pre-trained Language Model\n",
        "\n",
        "We'll use a smaller version of GPT-2 for demonstration purposes. Loading the model involves downloading its weights and configuration, and loading its associated tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token for GPT-2, which doesn't have one by default\n",
        "# This is important for batch processing later, though not strictly needed for single-sequence generation\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Use the end-of-sequence token as pad token\n",
        "\n",
        "print(f\"Model '{model_name}' loaded successfully.\")\n",
        "\n",
        "\"\"\"## 3. Text Generation\n",
        "\n",
        "Let's generate some text using the loaded model. We can use the `pipeline` for convenience.\n",
        "\"\"\"\n",
        "\n",
        "# Create a text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1) # Use GPU if available\n",
        "\n",
        "# Define a prompt\n",
        "prompt = \"The quick brown fox jumps over the lazy\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generated_text}\")\n",
        "\n",
        "\"\"\"## 4. Evaluating Text Generation\n",
        "\n",
        "Evaluating generated text is crucial. We can use various metrics to assess the quality, fluency, and relevance of the output compared to reference text (if available).\n",
        "\n",
        "### 4.1 Perplexity\n",
        "\n",
        "Perplexity is a measure of how well a probability model predicts a sample. In language modeling, it measures how well the model predicts a sequence of words. A lower perplexity generally indicates a better model.\n",
        "\n",
        "Calculating perplexity usually requires evaluating the model's likelihood on a held-out dataset. For a single generated text snippet, we can conceptualize it as the inverse probability of the sequence, normalized by length. The `evaluate` library provides a convenient way to calculate it on a dataset.\n",
        "\n",
        "Let's demonstrate calculating perplexity on a simple text string.\n",
        "\"\"\"\n",
        "\n",
        "# Load the perplexity metric\n",
        "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "\n",
        "# We need to calculate perplexity on some text using the model's likelihood\n",
        "# Let's calculate the perplexity of the generated text itself under the model.\n",
        "# Note: A more standard evaluation is on a separate held-out corpus.\n",
        "# This calculation shows the model's confidence in the generated sequence.\n",
        "\n",
        "text_to_evaluate = [generated_text] # Perplexity metric expects a list of strings\n",
        "\n",
        "# Calculate perplexity\n",
        "# This step involves tokenizing the text and computing the model's negative log-likelihood\n",
        "results = perplexity_metric.compute(model_id=model_name,\n",
        "                                    predictions=text_to_evaluate,\n",
        "                                    tokenizer=model_name) # Use model_name to load the corresponding tokenizer for perplexity calculation\n",
        "\n",
        "print(\"\\n--- Perplexity Evaluation ---\")\n",
        "print(f\"Text: {text_to_evaluate[0]}\")\n",
        "print(f\"Perplexity: {results['perplexity']:.2f}\")\n",
        "\n",
        "\"\"\"### 4.2 BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "BLEU is a metric for evaluating the quality of text which has been machine-translated from one natural language to another. It's also widely used for other text generation tasks like summarization or image captioning where reference texts are available.\n",
        "\n",
        "BLEU compares the generated text (candidate) to one or more high-quality reference texts. It measures the precision of n-grams (sequences of n words) in the candidate text relative to the reference texts, with a penalty for short sentences.\n",
        "\"\"\"\n",
        "\n",
        "# Load the BLEU metric\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "\n",
        "# Example: Evaluate a candidate sentence against reference sentences\n",
        "candidate = \"The quick brown fox jumped over the lazy dog.\"\n",
        "references = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A quick brown fox jumps over the lazy dog.\",\n",
        "    \"Quick brown fox jumps over lazy dog.\"\n",
        "]\n",
        "\n",
        "# The metric expects lists of lists for references\n",
        "references_formatted = [[ref] for ref in references]\n",
        "\n",
        "# Compute BLEU score\n",
        "results = bleu_metric.compute(predictions=[candidate], references=references_formatted)\n",
        "\n",
        "print(\"\\n--- BLEU Evaluation ---\")\n",
        "print(f\"Candidate: {candidate}\")\n",
        "print(\"References:\")\n",
        "for ref in references:\n",
        "    print(f\"- {ref}\")\n",
        "print(f\"BLEU score: {results['bleu']:.4f}\")\n",
        "\n",
        "\"\"\"A higher BLEU score indicates better overlap with the reference translations.\n",
        "\n",
        "### 4.3 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "ROUGE is commonly used for evaluating automatic summarization and machine translation. Unlike BLEU which is precision-oriented, ROUGE is recall-oriented, measuring how much of the reference text is covered by the generated text.\n",
        "\n",
        "Common variants:\n",
        "- ROUGE-N: Compares n-grams. ROUGE-1 uses unigrams, ROUGE-2 uses bigrams.\n",
        "- ROUGE-L: Compares based on the Longest Common Subsequence (LCS). This captures sentence-level structure similarity.\n",
        "- ROUGE-W: Weighted LCS.\n",
        "- ROUGE-S: Skip-bigram co-occurrence statistics.\n",
        "\n",
        "Let's compute ROUGE scores.\n",
        "\"\"\"\n",
        "\n",
        "# Load the ROUGE metric\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "# Example: Evaluate a generated summary against a reference summary\n",
        "candidate_summary = \"The fox jumped over the dog.\"\n",
        "reference_summary = \"A quick brown fox jumps over the lazy dog in the forest.\"\n",
        "\n",
        "# Compute ROUGE score\n",
        "# The metric expects lists for predictions and references\n",
        "results = rouge_metric.compute(predictions=[candidate_summary], references=[reference_summary])\n",
        "\n",
        "print(\"\\n--- ROUGE Evaluation ---\")\n",
        "print(f\"Candidate Summary: {candidate_summary}\")\n",
        "print(f\"Reference Summary: {reference_summary}\")\n",
        "print(f\"ROUGE-1 F1: {results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2 F1: {results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L F1: {results['rougeL']:.4f}\") # F1-score is often reported\n",
        "\n",
        "\"\"\"Higher ROUGE scores indicate better overlap with the reference summary.\n",
        "\n",
        "## 5. Evaluating LLMs for Classification Tasks\n",
        "\n",
        "LLMs can also be fine-tuned or used via prompting for classification tasks (e.g., sentiment analysis, topic classification, intent recognition). In such cases, standard classification metrics are used.\n",
        "\n",
        "Let's simulate a classification scenario where an LLM predicts sentiment (positive/negative) and evaluate using common metrics from `sklearn`.\n",
        "\"\"\"\n",
        "\n",
        "# Simulate true labels and LLM predictions\n",
        "# 0: Negative, 1: Positive\n",
        "true_labels = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
        "predicted_labels = [1, 1, 1, 0, 0, 1, 0, 1, 1, 0] # Some correct, some incorrect\n",
        "\n",
        "print(\"\\n--- Classification Evaluation Simulation ---\")\n",
        "print(f\"True Labels:      {true_labels}\")\n",
        "print(f\"Predicted Labels: {predicted_labels}\")\n",
        "\n",
        "# Calculate standard classification metrics\n",
        "\n",
        "# Accuracy: Proportion of correct predictions\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision: Of all instances predicted as positive, what proportion were actually positive?\n",
        "# Useful when the cost of False Positives is high.\n",
        "# average='binary' is for binary classification\n",
        "precision = precision_score(true_labels, predicted_labels, average='binary')\n",
        "print(f\"Precision (Positive Class): {precision:.4f}\")\n",
        "\n",
        "# Recall (Sensitivity): Of all actual positive instances, what proportion were correctly predicted as positive?\n",
        "# Useful when the cost of False Negatives is high.\n",
        "recall = recall_score(true_labels, predicted_labels, average='binary')\n",
        "print(f\"Recall (Positive Class): {recall:.4f}\")\n",
        "\n",
        "# F1-score: The harmonic mean of Precision and Recall. Balances both metrics.\n",
        "f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
        "print(f\"F1-score (Positive Class): {f1:.4f}\")\n",
        "\n",
        "\"\"\"These metrics provide different perspectives on the performance of the classification model.\n",
        "\n",
        "## 6. Conclusion\n",
        "\n",
        "This tutorial demonstrated how to load a basic LLM and perform text generation. Crucially, it showed how to use standard statistical metrics (Perplexity, BLEU, ROUGE for generation; Accuracy, Precision, Recall, F1 for classification) to quantitatively evaluate the performance of LLMs on different tasks.\n",
        "\n",
        "Choosing the right evaluation metric depends heavily on the specific task and the aspects of performance you care most about. Automated metrics are valuable but should often be complemented with human evaluation for a complete picture of an LLM's capabilities.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deepeval, lettucedetect, RAGAS and ROUGE, giskard, SECTOR\n",
        "\n",
        "import pandas as pd\n",
        "!pip install deepeval lettucedetect ragas rouge_score giskard sector\n",
        "import deepeval\n",
        "from deepeval import evaluate as deepeval_evaluate\n",
        "from deepeval.metrics import (\n",
        "    AssertMetric,\n",
        "    BiasMetric,\n",
        "    ToxicityMetric,\n",
        "    NERMetric,\n",
        "    SummarizationMetric,\n",
        "    FaithfulnessMetric,\n",
        "    AnswerRelevanceMetric,\n",
        "    ContextualRelevanceMetric,\n",
        "    CostMetric,\n",
        "    Alerter,\n",
        "    GEval,\n",
        ")\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval import assert_test\n",
        "from lettucedetect import LettuceDetect\n",
        "from ragas import evaluate as ragas_evaluate\n",
        "from ragas.metrics import (\n",
        "    answer_relevance,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    Sari,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from giskard.testing.tests.llm import (\n",
        "    LLMRelevantTest,\n",
        "    LLMHarmfulTest,\n",
        "    LLMQAValidTest,\n",
        ")\n",
        "from giskard.datasets.base import Dataset as GiskardDataset\n",
        "from giskard.models.base.model import LLMModel\n",
        "import sector\n",
        "\n",
        "print(\"\\n--- Additional LLM Evaluation Libraries Installed and Imported ---\")\n",
        "print(\"deepeval, lettucedetect, ragas, rouge_score, giskard, SECTOR\")\n",
        "\n",
        "# --- Examples of using the new libraries (simplified) ---\n",
        "\n",
        "# Example using deepeval (requires a test case)\n",
        "# This is a very basic example. Real usage involves defining test cases with inputs, expected outputs, context, etc.\n",
        "# try:\n",
        "#     test_case = LLMTestCase(\n",
        "#         input=\"What is the capital of France?\",\n",
        "#         actual_output=\"Paris is the capital of France.\",\n",
        "#         expected_output=\"Paris is the capital of France.\",\n",
        "#         context=[\"Paris is the capital and most populous city of France.\"],\n",
        "#         retrieval_context=[\"Paris is the capital and most populous city of France.\"]\n",
        "#     )\n",
        "#     # You would typically run evaluate with a list of test cases\n",
        "#     # deepeval_evaluate([test_case], metrics=[FaithfulnessMetric(), AnswerRelevanceMetric()])\n",
        "#     print(\"\\nDeepeval imported successfully. Evaluation requires specific test cases.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nCould not run deepeval example (requires more setup): {e}\")\n",
        "\n",
        "\n",
        "# Example using lettucedetect\n",
        "ld = LettuceDetect()\n",
        "text_to_check = \"This is a test sentence.\"\n",
        "result = ld.detect(text_to_check)\n",
        "print(f\"\\nLettuceDetect check on '{text_to_check}': {result}\")\n",
        "\n",
        "\n",
        "# Example using ragas (requires a dataset)\n",
        "# This is a placeholder. Ragas requires a Dataset object with 'question', 'answer', 'ground_truth', 'contexts'.\n",
        "# data = {\n",
        "#     'question': [\"What is the capital of France?\"],\n",
        "#     'answer': [\"Paris is the capital of France.\"],\n",
        "#     'ground_truth': [\"The capital of France is Paris.\"],\n",
        "#     'contexts': [[\"Paris is the capital and most populous city of France.\"]]\n",
        "# }\n",
        "# ragas_dataset = Dataset.from_dict(data)\n",
        "# try:\n",
        "#     # ragas_results = ragas_evaluate(ragas_dataset, metrics=[answer_relevance, faithfulness])\n",
        "#     # print(\"\\nRagas evaluation (requires a proper dataset).\")\n",
        "#      print(\"\\nRagas imported successfully. Evaluation requires a Dataset object.\")\n",
        "# except Exception as e:\n",
        "#      print(f\"\\nCould not run ragas example (requires a dataset): {e}\")\n",
        "\n",
        "\n",
        "# Example using rouge_score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score('The quick brown fox jumps over the lazy dog.',\n",
        "                      'The quick brown fox jumped over the lazy dog.')\n",
        "print(f\"\\nRouge_score comparison:\\nReference: 'The quick brown fox jumps over the lazy dog.'\\nCandidate: 'The quick brown fox jumped over the lazy dog.'\")\n",
        "print(f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}\")\n",
        "print(f\"ROUGE-L: {scores['rougeL'].fmeasure:.4f}\")\n",
        "\n",
        "\n",
        "# Example using giskard (requires a GiskardDataset and LLMModel)\n",
        "# This is a placeholder. Giskard requires wrapping your model and data.\n",
        "# try:\n",
        "#     # Define a dummy predict function for the wrapped model\n",
        "#     class DummyLLM(LLMModel):\n",
        "#         def predict(self, df):\n",
        "#             # Simulate LLM output based on input text column\n",
        "#             return [f\"Generated text based on: {txt}\" for txt in df[self.feature_names[0]]]\n",
        "\n",
        "#     # Create a dummy dataset\n",
        "#     dummy_data = pd.DataFrame({'input_text': ['Hello model', 'Another input']})\n",
        "#     giskard_dataset = GiskardDataset(dummy_data, target=None) # No target for text generation\n",
        "\n",
        "#     # Instantiate the dummy model\n",
        "#     dummy_model = DummyLLM(model=\"dummy-model\", name=\"Dummy Model\", feature_names=['input_text'])\n",
        "\n",
        "#     # Example test (requires a lot more context and configuration for real use)\n",
        "#     # test_result = LLMRelevantTest(dataset=giskard_dataset, model=dummy_model).execute()\n",
        "#     # print(f\"\\nGiskard test result (dummy): {test_result}\")\n",
        "#     print(\"\\nGiskard imported successfully. Testing requires wrapping your model and dataset.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nCould not run giskard example (requires wrapping model/data): {e}\")\n",
        "\n",
        "\n",
        "# Example using SECTOR\n",
        "try:\n",
        "    # SECTOR usage typically involves training/loading a model for semantic textual similarity/coherence.\n",
        "    # This is just to show the import works.\n",
        "    # from sector.models import MyModel # Example import from sector, depends on its structure\n",
        "    print(\"\\nSECTOR imported successfully. Usage involves building/loading SECTOR models.\")\n",
        "except ImportError:\n",
        "     print(\"\\nSECTOR imported successfully, but specific modules might need further steps.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not demonstrate SECTOR usage (requires model setup): {e}\")\n",
        "```"
      ],
      "metadata": {
        "id": "Mc-sJWI5Jp_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}