{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy5ZpMj1-pQX"
      },
      "outputs": [],
      "source": [
        "# LLM Observability using langsmith, langfuse and opentelemetry\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install langchain-core langchain-openai langsmith langfuse openinference-instrumentation[openai] opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-grpc opentelemetry-exporter-otlp-proto-http opentelemetry-sdk-extension-aws # Install necessary libraries\n",
        "!pip install --upgrade openai # Ensure you have a recent openai library\n",
        "\n",
        "import os\n",
        "\n",
        "# --- Set up API Keys ---\n",
        "# Replace with your actual API keys or set them as environment variables\n",
        "# It's recommended to use environment variables in a real application\n",
        "# (e.g., via Colab secrets or os.environ['VAR_NAME'] = 'YOUR_KEY')\n",
        "\n",
        "# For LangSmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Or your custom endpoint\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"colab-llm-observability-demo-langsmith\" # Replace with your project name\n",
        "\n",
        "# For LangFuse\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # Or your custom endpoint\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"YOUR_LANGFUSE_PUBLIC_KEY\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"YOUR_LANGFUSE_SECRET_KEY\"\n",
        "# os.environ[\"LANGFUSE_DEBUG\"] = \"true\" # Uncomment for verbose logging\n",
        "\n",
        "# For OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# --- Imports ---\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith import traceable\n",
        "from langfuse import Langfuse\n",
        "from openai import OpenAI\n",
        "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.sdk.trace import TracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
        "\n",
        "# --- Initialize the OpenAI LLM ---\n",
        "# We'll use ChatOpenAI from langchain for LangSmith/LangFuse examples as they integrate well.\n",
        "# We'll use the raw openai client for the OpenTelemetry example to show general instrumentation.\n",
        "llm_langchain = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "llm_openai = OpenAI()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example 1: LangSmith ---\n",
        "\n",
        "# LangSmith automatically traces calls when LANGCHAIN_TRACING_V2 is true\n",
        "# and LANGCHAIN_ENDPOINT/LANGCHAIN_API_KEY are set.\n",
        "# The traceable decorator is optional but can add structure and metadata.\n",
        "@traceable\n",
        "def call_openai_langsmith(prompt: str):\n",
        "  \"\"\"Calls OpenAI using LangChain and is traced by LangSmith.\"\"\"\n",
        "  response = llm_langchain.invoke(prompt)\n",
        "  return response\n",
        "\n",
        "print(\"--- Calling OpenAI via LangSmith ---\")\n",
        "langsmith_response = call_openai_langsmith(\"Write a short poem about a cat.\")\n",
        "print(langsmith_response.content)\n",
        "print(\"\\nCheck your LangSmith project for traces.\")\n",
        "\n",
        "\n",
        "# --- Example 2: LangFuse ---\n",
        "\n",
        "# Initialize LangFuse client\n",
        "langfuse = Langfuse()\n",
        "\n",
        "def call_openai_langfuse(prompt: str):\n",
        "  \"\"\"Calls OpenAI using LangChain and is traced by LangFuse.\"\"\"\n",
        "  # LangFuse integration with LangChain typically happens via callbacks\n",
        "  # when the environment variables are set.\n",
        "  # You can manually create spans if needed for more granular control.\n",
        "  trace = langfuse.trace(\n",
        "      name=\"openai-call-langfuse\",\n",
        "      input=prompt,\n",
        "  )\n",
        "  # Use the trace context if needed, but for simple LLM calls,\n",
        "  # the environment variables handle basic tracing with LangChain.\n",
        "  response = llm_langchain.invoke(prompt)\n",
        "  trace.update(output=response.content)\n",
        "  trace.end()\n",
        "  return response\n",
        "\n",
        "print(\"\\n--- Calling OpenAI via LangFuse ---\")\n",
        "langfuse_response = call_openai_langfuse(\"Write a short story about a dog.\")\n",
        "print(langfuse_response.content)\n",
        "print(\"\\nCheck your LangFuse project for traces.\")\n",
        "\n",
        "# Flush Langfuse data to ensure it's sent\n",
        "langfuse.flush()\n",
        "\n",
        "\n",
        "# --- Example 3: OpenTelemetry (OpenInference) ---\n",
        "\n",
        "# Configure OpenTelemetry TracerProvider\n",
        "# For a real application, you would likely export traces to an OTLP collector\n",
        "# (e.g., Jaeger, Honeycomb, Datadog) instead of the console.\n",
        "# Install required exporter: !pip install opentelemetry-exporter-otlp-proto-grpc or opentelemetry-exporter-otlp-proto-http\n",
        "\n",
        "# Example using a simple ConsoleSpanExporter for demonstration\n",
        "provider = TracerProvider(\n",
        "    resource=Resource.create({\"service.name\": \"colab-openai-demo\"})\n",
        ")\n",
        "# To export to a collector via gRPC:\n",
        "# from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
        "# processor = BatchSpanProcessor(OTLPSpanExporter())\n",
        "\n",
        "# Example using ConsoleSpanExporter\n",
        "processor = BatchSpanProcessor(ConsoleSpanExporter())\n",
        "\n",
        "provider.add_span_processor(processor)\n",
        "trace.set_tracer_provider(provider)\n",
        "\n",
        "# Instrument the OpenAI client\n",
        "# This will automatically create spans for OpenAI API calls.\n",
        "OpenAIInstrumentor().instrument()\n",
        "\n",
        "# Use the raw OpenAI client\n",
        "print(\"\\n--- Calling OpenAI via OpenTelemetry Instrumented Client ---\")\n",
        "otel_response = llm_openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Write a short haiku about a bird.\"}]\n",
        ")\n",
        "print(otel_response.choices[0].message.content)\n",
        "print(\"\\nCheck the console output or your OpenTelemetry collector for spans.\")\n",
        "\n",
        "# Force export of remaining spans (useful for ConsoleExporter or interactive sessions)\n",
        "provider.force_flush()\n"
      ],
      "metadata": {
        "id": "aBgSyPS5_Kwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Extensive Examples ---\n",
        "\n",
        "print(\"\\n--- Extensive Examples ---\")\n",
        "\n",
        "# Example with LangSmith and a more complex prompt\n",
        "print(\"\\n--- LangSmith Example with Complex Prompt ---\")\n",
        "@traceable\n",
        "def analyze_sentiment_langsmith(text: str):\n",
        "    \"\"\"Analyzes sentiment of the given text using LangChain and is traced by LangSmith.\"\"\"\n",
        "    prompt = f\"Analyze the sentiment of the following text: '{text}'. Is it positive, negative, or neutral? Explain briefly.\"\n",
        "    response = llm_langchain.invoke(prompt)\n",
        "    return response\n",
        "\n",
        "complex_text_langsmith = \"I loved the movie! The acting was superb, but the ending was a bit disappointing.\"\n",
        "langsmith_complex_response = analyze_sentiment_langsmith(complex_text_langsmith)\n",
        "print(f\"Analyzing text: '{complex_text_langsmith}'\")\n",
        "print(f\"Sentiment Analysis Result: {langsmith_complex_response.content}\")\n",
        "print(\"Check your LangSmith project for the trace of this call.\")\n",
        "\n",
        "\n",
        "# Example with LangFuse and a chained call or sequence\n",
        "print(\"\\n--- LangFuse Example with Chained Calls ---\")\n",
        "\n",
        "def generate_and_summarize_langfuse(topic: str):\n",
        "    \"\"\"Generates text about a topic and then summarizes it, traced by LangFuse.\"\"\"\n",
        "    # Manually create a parent span for the entire operation\n",
        "    parent_trace = langfuse.trace(\n",
        "        name=\"generate-and-summarize\",\n",
        "        input=topic,\n",
        "    )\n",
        "\n",
        "    # First call: generate text\n",
        "    with parent_trace.span(name=\"generate-text\") as generate_span:\n",
        "        generate_prompt = f\"Write a paragraph about {topic}.\"\n",
        "        generate_span.update(input=generate_prompt)\n",
        "        generated_text_response = llm_langchain.invoke(generate_prompt)\n",
        "        generated_text = generated_text_response.content\n",
        "        generate_span.update(output=generated_text)\n",
        "\n",
        "    print(f\"Generated text about {topic}:\\n{generated_text}\\n\")\n",
        "\n",
        "    # Second call: summarize text\n",
        "    with parent_trace.span(name=\"summarize-text\") as summarize_span:\n",
        "        summarize_prompt = f\"Summarize the following text:\\n{generated_text}\"\n",
        "        summarize_span.update(input=summarize_prompt)\n",
        "        summarized_text_response = llm_langchain.invoke(summarize_prompt)\n",
        "        summarized_text = summarized_text_response.content\n",
        "        summarize_span.update(output=summarized_text)\n",
        "\n",
        "    print(f\"Summarized text:\\n{summarized_text}\\n\")\n",
        "\n",
        "    parent_trace.update(output=summarized_text)\n",
        "    parent_trace.end() # End the parent trace\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "topic_langfuse = \"the benefits of machine learning\"\n",
        "langfuse_chained_response = generate_and_summarize_langfuse(topic_langfuse)\n",
        "print(f\"Result of Generate and Summarize for '{topic_langfuse}':\\n{langfuse_chained_response}\")\n",
        "print(\"Check your LangFuse project for traces, including the parent trace and its nested spans.\")\n",
        "\n",
        "# Flush Langfuse data again after the chained example\n",
        "langfuse.flush()\n",
        "\n",
        "\n",
        "# Example with OpenTelemetry and a more complex interaction\n",
        "print(\"\\n--- OpenTelemetry Example with Multiple Calls ---\")\n",
        "\n",
        "# The OpenAIInstrumentor will capture all chat.completions.create calls\n",
        "# on the instrumented client (llm_openai).\n",
        "# You can manually create custom spans around groups of calls if needed,\n",
        "# but the instrumentor handles the individual API calls.\n",
        "\n",
        "def process_query_otel(query: str):\n",
        "    \"\"\"Processes a query using multiple OpenAI calls and is traced by OpenTelemetry.\"\"\"\n",
        "    # Manual span to group the entire process\n",
        "    with trace.get_tracer(__name__).start_as_current_span(\"process-query\") as span:\n",
        "        span.set_attribute(\"query.input\", query)\n",
        "\n",
        "        # Call 1: Classify the query\n",
        "        classify_prompt = f\"Classify the following query: '{query}'. Is it about programming, cooking, or history?\"\n",
        "        span.add_event(\"calling_openai_classify\", attributes={\"prompt\": classify_prompt})\n",
        "        classification_response = llm_openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": classify_prompt}]\n",
        "        )\n",
        "        classification = classification_response.choices[0].message.content\n",
        "        span.set_attribute(\"query.classification\", classification)\n",
        "        span.add_event(\"received_classification\", attributes={\"classification\": classification})\n",
        "\n",
        "        print(f\"Query Classification: {classification}\")\n",
        "\n",
        "        # Call 2: Generate a response based on classification\n",
        "        if \"programming\" in classification.lower():\n",
        "            response_prompt = f\"Provide a brief programming tip related to: {query}\"\n",
        "        elif \"cooking\" in classification.lower():\n",
        "            response_prompt = f\"Give a quick cooking tip related to: {query}\"\n",
        "        elif \"history\" in classification.lower():\n",
        "            response_prompt = f\"Mention a historical fact related to: {query}\"\n",
        "        else:\n",
        "            response_prompt = f\"Provide a general response to the query: {query}\"\n",
        "\n",
        "        span.add_event(\"calling_openai_response\", attributes={\"prompt\": response_prompt})\n",
        "        response_response = llm_openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": response_prompt}]\n",
        "        )\n",
        "        final_response = response_response.choices[0].message.content\n",
        "        span.set_attribute(\"query.output\", final_response)\n",
        "        span.add_event(\"received_response\", attributes={\"response\": final_response})\n",
        "\n",
        "        print(f\"Generated Response:\\n{final_response}\\n\")\n",
        "\n",
        "    return final_response\n",
        "\n",
        "query_otel = \"How do I sort a list in Python?\"\n",
        "print(f\"Processing query via OpenTelemetry: '{query_otel}'\")\n",
        "otel_process_response = process_query_otel(query_otel)\n",
        "print(f\"Final processed response: {otel_process_response}\")\n",
        "print(\"Check the console output or your OpenTelemetry collector for spans, including the 'process-query' span and nested OpenAI call spans.\")\n",
        "\n",
        "# Force export again after the OpenTelemetry example\n",
        "provider.force_flush()\n",
        "\n",
        "print(\"--- Examples Complete ---\")\n",
        "print(\"Remember to replace placeholder API keys and project names.\")\n",
        "print(\"Refer to the documentation for LangSmith, LangFuse, and OpenTelemetry for advanced configurations and export options.\")\n"
      ],
      "metadata": {
        "id": "1RVXD-Gd_W5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. adding another LLM call, 2. integrating a different tool, 3. visualizing data, 4. handling errors\n",
        "\n",
        "# Add these imports at the beginning of your code\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json # To potentially handle tool output if it's JSON\n",
        "from langchain.agents import tool, initialize_agent, AgentType\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "# --- 1. Adding another LLM Call ---\n",
        "# You already have multiple LLM calls shown in the LangFuse and OpenTelemetry examples.\n",
        "# Let's add another simple one demonstrating a different task, still using LangChain for ease of tracing integration.\n",
        "\n",
        "@traceable\n",
        "def translate_text_langsmith(text: str, target_language: str):\n",
        "    \"\"\"Translates the given text to the target language using LangChain and is traced by LangSmith.\"\"\"\n",
        "    prompt = f\"Translate the following text to {target_language}:\\n'{text}'\"\n",
        "    response = llm_langchain.invoke(prompt)\n",
        "    return response\n",
        "\n",
        "print(\"\\n--- Adding Another LLM Call: Translation ---\")\n",
        "text_to_translate = \"Hello, how are you?\"\n",
        "target_lang = \"French\"\n",
        "translation_response = translate_text_langsmith(text_to_translate, target_lang)\n",
        "print(f\"Original text: '{text_to_translate}'\")\n",
        "print(f\"Translated to {target_lang}: {translation_response.content}\")\n",
        "print(\"Check your LangSmith project for the translation trace.\")\n",
        "\n",
        "\n",
        "# --- 2. Integrating a Different Tool ---\n",
        "# We can use LangChain's agent capabilities to integrate a tool.\n",
        "# Let's create a simple tool that looks up a definition (as an example).\n",
        "\n",
        "class DefinitionSearchInput(BaseModel):\n",
        "    query: str = Field(description=\"The term to search the definition for\")\n",
        "\n",
        "@tool(\"definition-search\", args_schema=DefinitionSearchInput)\n",
        "def definition_search_tool(query: str) -> str:\n",
        "    \"\"\"Searches for the definition of a given term.\"\"\"\n",
        "    # This is a mock tool. In a real scenario, you'd integrate with a dictionary API or database.\n",
        "    definitions = {\n",
        "        \"python\": \"An interpreted, object-oriented, high-level programming language with dynamic semantics.\",\n",
        "        \"colaboratory\": \"A Google product that allows anyone to write and execute arbitrary python code through the browser.\",\n",
        "        \"llm\": \"Large Language Model: A type of artificial intelligence algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content.\",\n",
        "        \"observability\": \"The ability to understand the internal state of a system based on external data.\",\n",
        "    }\n",
        "    definition = definitions.get(query.lower(), f\"Definition for '{query}' not found in our mock database.\")\n",
        "    return definition\n",
        "\n",
        "# Initialize an agent that can use the tool and the LLM\n",
        "# The agent's actions and thought process will be traced by LangSmith/LangFuse\n",
        "# if the environment variables are set.\n",
        "agent = initialize_agent(\n",
        "    [definition_search_tool],\n",
        "    llm_langchain,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True, # Set to True to see the agent's thought process\n",
        "    handle_parsing_errors=True # Basic error handling for parsing agent output\n",
        ")\n",
        "\n",
        "print(\"\\n--- Integrating a Tool (Definition Search) ---\")\n",
        "try:\n",
        "    tool_query = \"What is a LLM?\"\n",
        "    print(f\"Using agent to answer: '{tool_query}'\")\n",
        "    tool_response = agent.run(tool_query)\n",
        "    print(f\"Agent's response: {tool_response}\")\n",
        "    print(\"Check your LangSmith/LangFuse project for the agent trace, including tool calls.\")\n",
        "\n",
        "    tool_query_unknown = \"Define 'quux'.\"\n",
        "    print(f\"\\nUsing agent to answer: '{tool_query_unknown}' (unknown term)\")\n",
        "    tool_response_unknown = agent.run(tool_query_unknown)\n",
        "    print(f\"Agent's response: {tool_response_unknown}\")\n",
        "    print(\"Check your LangSmith/LangFuse project for the trace.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during agent execution: {e}\")\n",
        "    # Tracing systems might capture this error, depending on configuration.\n",
        "\n",
        "\n",
        "# --- 3. Visualizing Data ---\n",
        "# Let's create some dummy data that we might get from processing LLM outputs or other sources\n",
        "# and visualize it using matplotlib.\n",
        "\n",
        "print(\"\\n--- Visualizing Data ---\")\n",
        "\n",
        "# Example: Sentiment distribution from hypothetical LLM calls\n",
        "sentiment_data = {'Positive': 15, 'Negative': 7, 'Neutral': 10}\n",
        "sentiments = list(sentiment_data.keys())\n",
        "counts = list(sentiment_data.values())\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(sentiments, counts, color=['green', 'red', 'grey'])\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Sentiments (Example Data)\")\n",
        "plt.show()\n",
        "\n",
        "# Example: Hypothetical latency of different LLM calls (milliseconds)\n",
        "latency_data = {'Call 1': 350, 'Call 2': 420, 'Call 3': 380, 'Call 4': 510, 'Call 5': 450}\n",
        "calls = list(latency_data.keys())\n",
        "latencies = list(latency_data.values())\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(calls, latencies, marker='o', linestyle='-')\n",
        "plt.xlabel(\"LLM Call\")\n",
        "plt.ylabel(\"Latency (ms)\")\n",
        "plt.title(\"Hypothetical LLM Call Latency Over Time/Calls\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization plots displayed above.\")\n",
        "\n",
        "\n",
        "# --- 4. Handling Errors ---\n",
        "# Errors can occur at various stages: API calls, tool execution, parsing outputs.\n",
        "# Implement basic try-except blocks. Observability tools help you see *where* errors happen.\n",
        "\n",
        "print(\"\\n--- Handling Errors ---\")\n",
        "\n",
        "def safe_llm_call(prompt: str, trace_name: str):\n",
        "    \"\"\"Safely calls the LLM and handles potential exceptions.\"\"\"\n",
        "    try:\n",
        "        # Use LangFuse tracing manually around the potentially failing call\n",
        "        trace = langfuse.trace(\n",
        "            name=trace_name,\n",
        "            input=prompt,\n",
        "        )\n",
        "        print(f\"Attempting LLM call with prompt: '{prompt[:50]}...'\")\n",
        "        response = llm_langchain.invoke(prompt)\n",
        "        trace.update(output=response.content)\n",
        "        trace.end(status=\"SUCCESS\")\n",
        "        print(\"LLM call successful.\")\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the LLM call: {e}\")\n",
        "        # Log the error to LangFuse/LangSmith/OpenTelemetry\n",
        "        # LangFuse example: Add error details to the trace\n",
        "        if 'trace' in locals() and trace:\n",
        "             trace.end(status=\"ERROR\", status_message=str(e))\n",
        "             print(\"Error logged to LangFuse trace.\")\n",
        "        # OpenTelemetry example: In a manual span, you'd add the exception as an event or status\n",
        "        # In the instrumented client case, the instrumentor might capture it.\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Example of a potentially problematic call (e.g., extremely long prompt, invalid parameters if not using LangChain helper)\n",
        "# We'll simulate an error by passing a very large string that might exceed context limits or cause issues.\n",
        "# Note: This specific prompt might not *always* fail depending on the model/API,\n",
        "# but it serves as an example structure for error handling.\n",
        "\n",
        "long_prompt = \"Explain the entire history of the universe in one paragraph. \" * 1000 # Very long prompt\n",
        "\n",
        "error_prone_response = safe_llm_call(long_prompt, \"potential-error-call\")\n",
        "print(f\"Result of error-prone call: {error_prone_response[:100]}...\") # Print only a snippet\n",
        "\n",
        "# Example of handling errors from tool execution (already partially covered by agent's handle_parsing_errors)\n",
        "# If you called the tool directly:\n",
        "try:\n",
        "    print(\"\\nAttempting a tool call that might fail...\")\n",
        "    # Simulate a tool call that raises an error\n",
        "    # For demonstration, let's just raise an exception here\n",
        "    # definition_search_tool(\"this should cause an error\") # This tool won't error, adding a manual raise\n",
        "    raise ValueError(\"Simulated error in tool execution\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Caught an error during tool execution: {e}\")\n",
        "    # Log this error using your chosen observability tool\n",
        "\n",
        "# Flush Langfuse data one last time\n",
        "langfuse.flush()\n",
        "\n",
        "print(\"\\n--- Error Handling Demonstration Complete ---\")\n",
        "print(\"Check your observability dashboards (LangSmith, LangFuse, OpenTelemetry collector) to see how these errors were captured.\")\n"
      ],
      "metadata": {
        "id": "wY0ns7k8_y1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}